(window.webpackJsonp=window.webpackJsonp||[]).push([[0],{121:function(e,t,a){e.exports=a.p+"static/media/LOC_mricrogl.028dad47.png"},122:function(e,t,a){e.exports=a.p+"static/media/Margalit_et_al_2017_Fig3.91f0ab2e.png"},123:function(e,t,a){e.exports=a.p+"static/media/Margalit_et_al_VSS_2018.7cec7499.png"},124:function(e,t,a){e.exports=a.p+"static/media/dissim_histogram.81d96289.png"},125:function(e,t,a){e.exports=a.p+"static/media/dp_gui_screenshot.03e9c89f.png"},126:function(e,t,a){e.exports=a.p+"static/media/gabor_grab.95cec5fe.PNG"},127:function(e,t,a){e.exports=a.p+"static/media/lateral_vtc_rdm.85ca8aa4.png"},128:function(e,t,a){e.exports=a.p+"static/media/spacenet.4f23cab6.png"},129:function(e,t,a){e.exports=a.p+"static/media/vtc_faces.eae46f35.png"},132:function(e,t,a){},133:function(e,t,a){},134:function(e,t,a){var i={"./":37,"./blank_template":135,"./blank_template.json":135,"./index":37,"./index.js":37,"./review_jsons/achille_critical_period_dnn":136,"./review_jsons/achille_critical_period_dnn.json":136,"./review_jsons/ackman_retinal_waves":137,"./review_jsons/ackman_retinal_waves.json":137,"./review_jsons/afraz_inactivation_face_gender":138,"./review_jsons/afraz_inactivation_face_gender.json":138,"./review_jsons/albert_retinal_waves":139,"./review_jsons/albert_retinal_waves.json":139,"./review_jsons/arcaro_body_protomap":140,"./review_jsons/arcaro_body_protomap.json":140,"./review_jsons/arcaro_universal_mechanisms":141,"./review_jsons/arcaro_universal_mechanisms.json":141,"./review_jsons/bartoldson_pruning":142,"./review_jsons/bartoldson_pruning.json":142,"./review_jsons/bracci_ventral_shape":143,"./review_jsons/bracci_ventral_shape.json":143,"./review_jsons/chapman_orientation_development":144,"./review_jsons/chapman_orientation_development.json":144,"./review_jsons/chen_simclr":145,"./review_jsons/chen_simclr.json":145,"./review_jsons/crair_binocular_deprivation":146,"./review_jsons/crair_binocular_deprivation.json":146,"./review_jsons/crair_pinwheel_od":147,"./review_jsons/crair_pinwheel_od.json":147,"./review_jsons/dahne_retinal_wave_sfa":148,"./review_jsons/dahne_retinal_wave_sfa.json":148,"./review_jsons/datta_computational_neuroethology":149,"./review_jsons/datta_computational_neuroethology.json":149,"./review_jsons/daw_critical_periods":150,"./review_jsons/daw_critical_periods.json":150,"./review_jsons/devalois_direction_selectivity":151,"./review_jsons/devalois_direction_selectivity.json":151,"./review_jsons/devalois_spatial_frequency":152,"./review_jsons/devalois_spatial_frequency.json":152,"./review_jsons/dicarlo_untangling":153,"./review_jsons/dicarlo_untangling.json":153,"./review_jsons/durbin_mitchison_som":154,"./review_jsons/durbin_mitchison_som.json":154,"./review_jsons/fahey_mouse_global_orientation":155,"./review_jsons/fahey_mouse_global_orientation.json":155,"./review_jsons/garg_color_orientation":156,"./review_jsons/garg_color_orientation.json":156,"./review_jsons/gribizis_retinal_waves":157,"./review_jsons/gribizis_retinal_waves.json":157,"./review_jsons/grill_spector_functional_architecture":158,"./review_jsons/grill_spector_functional_architecture.json":158,"./review_jsons/gunthner_divisive_normalization":159,"./review_jsons/gunthner_divisive_normalization.json":159,"./review_jsons/haxby_hyperalignment":160,"./review_jsons/haxby_hyperalignment.json":160,"./review_jsons/hu_curvature":161,"./review_jsons/hu_curvature.json":161,"./review_jsons/hubel_wiesel_v2v3":162,"./review_jsons/hubel_wiesel_v2v3.json":162,"./review_jsons/jacobs_jordan_short_connections":163,"./review_jsons/jacobs_jordan_short_connections.json":163,"./review_jsons/jang_orientation_classification":164,"./review_jsons/jang_orientation_classification.json":164,"./review_jsons/khalig-razavi_dnn":165,"./review_jsons/khalig-razavi_dnn.json":165,"./review_jsons/kietzmann_recurrence_rda":166,"./review_jsons/kietzmann_recurrence_rda.json":166,"./review_jsons/kim_pasupathy_texture_shape_v4":167,"./review_jsons/kim_pasupathy_texture_shape_v4.json":167,"./review_jsons/koch_pinwheels":168,"./review_jsons/koch_pinwheels.json":168,"./review_jsons/koulakov_chklovskii_wiring_cost":169,"./review_jsons/koulakov_chklovskii_wiring_cost.json":169,"./review_jsons/kriegeskorte_monkey_human":170,"./review_jsons/kriegeskorte_monkey_human.json":170,"./review_jsons/kunin_local_rules":171,"./review_jsons/kunin_local_rules.json":171,"./review_jsons/linsker_spatial_opponent":172,"./review_jsons/linsker_spatial_opponent.json":172,"./review_jsons/livingstone_hubel_color":173,"./review_jsons/livingstone_hubel_color.json":173,"./review_jsons/murty_haptic_blind":174,"./review_jsons/murty_haptic_blind.json":174,"./review_jsons/nauhaus_efficient_tiling":175,"./review_jsons/nauhaus_efficient_tiling.json":175,"./review_jsons/nauhaus_orthogonality":176,"./review_jsons/nauhaus_orthogonality.json":176,"./review_jsons/ohki_pinwheel_order":177,"./review_jsons/ohki_pinwheel_order.json":177,"./review_jsons/olshausen_field_sparse_coding":178,"./review_jsons/olshausen_field_sparse_coding.json":178,"./review_jsons/op_de_beeck_factors":179,"./review_jsons/op_de_beeck_factors.json":179,"./review_jsons/otoole_face_dcnn":180,"./review_jsons/otoole_face_dcnn.json":180,"./review_jsons/ponce_livingstone_super_stimuli":181,"./review_jsons/ponce_livingstone_super_stimuli.json":181,"./review_jsons/rajalingham_inactivation":182,"./review_jsons/rajalingham_inactivation.json":182,"./review_jsons/ringach_orientation_selectivity":183,"./review_jsons/ringach_orientation_selectivity.json":183,"./review_jsons/ringach_tuning_dynamics":184,"./review_jsons/ringach_tuning_dynamics.json":184,"./review_jsons/sato_mosaic":185,"./review_jsons/sato_mosaic.json":185,"./review_jsons/sharma_ferret_rewiring":186,"./review_jsons/sharma_ferret_rewiring.json":186,"./review_jsons/smith_v1_correlations":187,"./review_jsons/smith_v1_correlations.json":187,"./review_jsons/vanBergen_recurrence":188,"./review_jsons/vanBergen_recurrence.json":188,"./review_jsons/vogelsang_acuity":189,"./review_jsons/vogelsang_acuity.json":189,"./review_jsons/weliky_katz_disruption":190,"./review_jsons/weliky_katz_disruption.json":190,"./review_jsons/white_fitzpatrick_map_development":191,"./review_jsons/white_fitzpatrick_map_development.json":191,"./review_jsons/yacoub_orientation_columns":192,"./review_jsons/yacoub_orientation_columns.json":192,"./review_jsons/zhuang_local_aggregation":193,"./review_jsons/zhuang_local_aggregation.json":193};function n(e){var t=r(e);return a(t)}function r(e){var t=i[e];if(!(t+1)){var a=new Error("Cannot find module '"+e+"'");throw a.code="MODULE_NOT_FOUND",a}return t}n.keys=function(){return Object.keys(i)},n.resolve=r,e.exports=n,n.id=134},135:function(e){e.exports={metadata:{title:"",authors:["Doe, JJ","Joe, DD"],institutions:[],date:"YYYY-MM",journal:"",doi:"",url:"",keywords:[],review_date:"YYYY-MM-DD",one_sentence:""},review:{summary:[],background:[],approach:[],results:[],conclusions:[],other:[]}}},136:function(e){e.exports={review:{summary:["Deep neural networks trained on object recognition exhibit critical periods during which some stimulus perturbations yield irrecoverable deficits in performance","The critical period aligns well with a time of dramatic change in the task-information carried in the weights, which itself is transient"],background:["Critical periods (CPs) have been demonstrated in the visual cortex of many mammals, including cats and macaques. CPs are time windows in which perturbation to typical input lead to changes in the system that cannot be recovered by removing the perturbation, e.g., removing cataracts late in life does not yield fully restored vision."],approach:["Train an all-convolutional neural network on CIFAR-10, but introduce blur (by downscaling 4x and interpolating back to full-resolution) starting at a given epoch and extending for a variable amount of time. As a control, also perform other perturbations, e.g., up-down flips with the same time-window approach. For any perturbation, the network is given enough time afterward to see as many normal images as in control cases where no perturbation is not performed.",'Use Fisher Information matrices (FIMs) to approximate how much the change in a given weight is expect to change network output. The authors note that the FIM is also an approximation of the Hessian of the loss with respect to the weights. Because the FIM is hard to compute, the authors instead compute its trace, which is the sum of the "strengths" of synapses in a given layer.'],results:["Test accuracy decreases smoothly as blur is kept on longer and longer, suggesting that the network struggles to recover more when blur is kept around longer in initial training (Figure 1). ","When using a fixed window of 40 epochs of blur, it has the greatest impact on test performance between roughly 20 and 40 epochs into training, but has a lower impact when applied later or earlier (Figure 1)","Blurring is somewhat unique, as other perturbations such as up-down flips allow the network to recover to high performance (Figure 2)","Critical periods are observed in a variety of contexts, including in fully-connected networks, deeper networks, e.g., ResNet-18, with fixed (as opposed to annealed) learning rates, and with different weight regularization (Figure 3)",'Fisher information initially steeply increases, then briefly plateaus, then comes back down (even as performance increases monotonically). This later phase is akin to "forgetting" information that is task-irrelevant and may be useful in preventing overfitting (Figure 4)',"Fisher information is carried by intermediate layers when there is no deficit, and gets pushed later in the network when there is a deficit. If the deficit is removed early enough, however, the early-intermediate layers start to become more informative once again (Figure 5)"],conclusions:["We are able to (and should) consider the developmental trajectories of NNs instead of only their asymptotic behavior","The extent to which a deficit can be corrected is informative of the perturbation type; i.e., the fact that blur is hard to recover from whereas vertical flips are not suggests that within the somewhat-crystalized weights, only a subset of deficits require a total reconfiguration of those weights.","The authors suggest that it would be of interest to approximate a measure of information plasticity in biological systems as well, and I agree!"],other:[]},metadata:{authors:["Achille, A","Rovere, M","Soatto, S"],institutions:["University of California, Los Angeles","Brigham and Women's Hospital","Harvard Medical School"],keywords:["deep learning","critical period","perturbation","blur"],title:"Critical Learning Periods in Deep Networks",journal:"International Conference on Learning Representations (ICLR)",doi:"",url:"https://openreview.net/pdf?id=BkeStsCcKQ",date:"2019-05",review_date:"2020-01-15",one_sentence:'Achille et al. show that, as in biological systems, deep neural networks undergo an early "information plasticity" time frame in which perturbation leads to irrecoverable deficits in performance'}}},137:function(e){e.exports={metadata:{title:"Retinal waves coordinate patterned activity throughout the developing visual system",authors:["Ackman, JB","Burbridge, TJ","Crair, MC"],institutions:["Yale University"],date:"2012-10",journal:"Nature",doi:"doi:10.1038/nature11529",url:"",keywords:["retinal waves","mouse","superior colliculus","V1"],review_date:"2019-05-22",one_sentence:"Retinal waves are measured in vivo for the first time, demonstrating that they propagate to the superior colliculus, V1, and extrastriate visual areas in neonatal mice"},review:{summary:["Retinal waves propagate in vivo in neonatal mice, travel from the retina to V1 and beyond, and depend on cholinergic transmission","The correspondence between in vitro and in vivo values suggests that the in vitro measurements are useful tools, but this approach led to novel findings, such as a preferred wave-initiation site within the retina and coordination of waves between the retina of each eye"],background:["Spontaneous retinal waves had been demonstrated in vitro, and their blockade had been associated with poor refinement of visual circuits in the dorsal LGN and visual cortex. Until this paper, they had never been observed in vivo",""],approach:["Calcium indicator CaGr-Dx was injected into the retina of P3-P9 mice, and wide field imaging was performed over the superior colliculus to identify the retinal terminals","To measure cell activity in the SC, bulk load calcium indicator OGB1- AM. Repeat this procedure in visual cortex to measure responses there too."],results:["Retinal waves spread throughout the most superficial layer of the superior colliculus at speeds consistent with in vitro measurements. Their propagation can be blocked with TTX application the contralateral eye (Figure 1)","Waves propagate within SC cells themselves at the same position, spread, and velocity as the incoming waves measured at RGC terminals, suggesting a direct link between RGC waves and the induced waves in the SC (Figure 2)","Waves preferentially begin in the ventrotemporal retina, and suprisingly, are sometimes coordinated between eyes (Figure 3)","Waves also appear in V1, and their direction matches that of SC waves after correcting for the mirroring and rotation of the retinotopic maps between the two regions (Figure 4)","Waves can be observed in extrastriate areas, but they appear to be more independent, and abolishing waves has less of an impact on their activity (Figure 5)","The mechanism of wave transmission is likely cholinergic, as blocking nicotinic acetylcholine receptors abolishes waves throughout the system (Figure 6)"],conclusions:["In vitro measurements are supported and reinforced by these in vivo results, which demonstrate retinal wave propagation throughout the visual system for at least a week (P3 - P9)","Retinal waves, beginning before birth, may very well play a role in the patterned development of the entire visual system (although these results suggest that past V1, their influence is lower)"],other:["I really like this paper as a clearly-written and communicated tour de force, and it has nice movies you can download too!"]}}},138:function(e){e.exports={metadata:{title:"Optogenetic and pharmacological suppression of spatial clusters of face neurons reveal their causal role in face gender discrimination",authors:["Afraz, A","Boyden, ES","DiCarlo, JJ"],institutions:["MIT"],date:"2015-03",journal:"PNAS",doi:"10.1073/pnas.1423328112",url:"",keywords:["macaque","face","inactivation","gender"],review_date:"2019-05-30",one_sentence:"Inactivation of regions enriched for face-detecting neurons leads to a deficit in face-gender discrimination, both with optogenetic and pharmacological perturbations."},review:{summary:["Optogenetic and pharmacological inactivation of face patches leads to deficits in discrimination of facial gender"],background:["Face cells have been observed for decades, yet no causal evidence exists to support their role in visually-guided behaviors relating to faces","Are the neurons in face-selective clusters necessary to discriminate the gender of a face, or can the distributed IT population handle it alone?"],approach:["Identify the middle face patch in macaque IT of two monkeys, then inactivate a 1mm chunk of face-selective cortex (or nearby nonselective cortex) with either optogenetic tools or pharmacological methods. Note that both methods are reversible and do not require lesions."],results:["The optogenetic manipulation effectively 'deletes' a median of 33% of visually-evoked spikes (Figure 2)","Inactivation reduces face-gender discrimination performance by 2% when faces are in the contralateral visual field, but no significant decrease in the ipsilateral field. The degree of behavioral deficit correlated with the degree of face selectivity for that site (Figure 3)","The face-gender discrimination deficits was not observed for stimulation of nearby sites that were not enriched with face-selective neurons (Figure 3)","The better a site is at separating face gender, the more the monkey is impacted by its inactivation (Figure 4)","Optogenetic effects are consistent with effects from well-established muscimol microinjection. Notably optogenetic perturbation has much higher spatial and temporal resolution than muscimol (Figure 5)"],conclusions:["At least for this one face task, it appears that the monkey relies on activity of face-preferring IT sites more than nearby nonselective sites","It is possible that as a general pattern, regions that are best at some task are used for that task. This is a more general statement that should encompass results beyond the face patch system. And, importantly, it should not hold for regions from which behavior is not linearly decodable, e.g., V4","The contra vs. ipsi results here cast doubt on the idea that receptive fields in IT are so large that laterality is irrelevant"],other:["See Rangalijam et al., 2018 for more on reversible inactivation and task deficits in IT","This paper makes the interesting prediction that tasks that see little effect of, e.g., cortical cooling, may not be choosing tasks that appropriately require the ventral stream (e.g., invariance)"]}}},139:function(e){e.exports={review:{summary:["Performing either ICA or sparse coding on static images of retinal waves is sufficient to generate oriented Gabor-like receptive fields.","The nature of the receptive fields can change with the statistics of the spontaneous activity, potential explaining how spontaneous activity may play different roles in different regions of the brain."],background:["While parts of the visual system obviously need patterned visual input to correctly mature, retinal waves have been shown to play an important role in the development of early visual cortex despite their 1) simplicity and 2) apparent differences from natural images.","Spontaneous activity has been observed in the retina, in the LGN, in V1, in auditory cortex, and to some degree in the hippocampus and spinal cord.","The function of the handoff between spontaneous activity and visual experience (and whether they act on similar mechanisms in early visual cortex) is largely unknown.","The spontaneous activity in V1 and LGN do not overlap in time with retinal waves and may have different spatiotemporal statistics (need to look into this more...)"],approach:['Generate input patterns from general "threshold site percolation" patterns, of which some retinal waves in some species are a clear member. Basically, have activity spread across nodes.',"Perform 1) independent components analysis (ICA) on the images of wave-like patterns and 2) the sparse-coding procedure described in Olshausen and Field's earlier work for both retinal-wave like patterns and more general patterns of the threshold site-percolation family."],results:["ICA and sparse-coding both yield Gabor-like filters for retinal-wave-like images and for generic wavefront-like patterns (Figure 2).","The optimal filters differed depending on the pattern-generation parameters, with a primary difference being the orientation bandwidth of the fit Gabor functions. No strong claim about which parameters yield the most macaque-like V1 cells is made (Figure 3).",'For reasonable filters to form with PCA, the input data must have pairwise correlations but does not require higher-order structure; whitened inputs do not yield "good" filters (Figure 4B).',"For reasonable filters to form with ICA, the input data must be natural or patterned (nonrandom), but can be whitened or correlated (Figure 4C)."],conclusions:["Efficient coding (whether sparse or by ICA) can produce structured receptive fields both for natural images (as previously shown) and for wave-like images (shown in this work)","While molecular guidance cues certainly play a role in establishing the system, spontaneous activity may be important for refinement of receptive fields on the basis of statistical structure that will be shared between spontaneous activity and natural images."],other:["This work can be compared with, e.g., Dahne and Wiskott, 2014, who leverage not just the structure of retinal wave images, but their temporal progressions (they apply slow feature analysis; SFA)"]},metadata:{authors:["Albert, MV","Schnabel, A","Field, DJ"],institutions:["Cornell University","Massachusetts Institute of Technology"],keywords:["retinal waves","ica","sparse coding","gabor"],title:"Innate Visual Learning through Spontaneous Activity Patterns",journal:"PLOS Computational Biology",doi:"10.1371/journal.pcbi.1000137",url:"https://core.ac.uk/display/87506040",date:"2008-07",review_date:"2020-02-20",one_sentence:"Performing either ICA or sparse coding on static images of retinal waves is sufficient to generate oriented Gabor-like receptive fields."}}},140:function(e){e.exports={metadata:{authors:["Arcaro, MJ","Schade, PF","Livingstone, MS"],institutions:["Harvard University"],keywords:["macaque","somatosensory","topography","fmri"],title:"Body map proto-organization in newborn macaques",journal:"PNAS",doi:"10.1073/pnas.1912636116",url:"www.pnas.org/cgi/doi/10.1073/pnas.1912636116",date:"2019-11",review_date:"2019-11-21",one_sentence:"Arcaro et al. demonstrate the somatosensory topography is present at birth in macaques and is robust to dramatic differences in early experience (e.g., visual deprivation)"},review:{summary:["Large-scale somatosensory topography is indistinguishable between monkeys 2 weeks and 2 years of age, whereas fine-scale topography continues to develop with experience","Visual deprivation does not alter the large scale topographic structure"],background:["Topographic maps are present throughout the sensory brain, including retinotopy for the visual system, tonotopy for the auditory system, and somatosensory maps for touch","Prior work has suggested that the somatosensory system is immature at birth, as neurons there do not respond strongly to touch at a young age"],approach:["fMRI of 9 monkeys (11 - 981 days of age) during somatotopic mapping, i.e., touching or stroking different parts of the body while the monkey is awake and alert. The primary body regions studied were the face, hands, and feet","ICA was used to identify response patterns that correspond to stimulation of a given region"],results:["The central sulcus exhibits clear topography (dorsal = hands, ventral = faces) in monkeys of all ages (Figures 1 and 2), including the youngest monkey at 11 days old. The maps showed little development in terms of cortical area devoted to each region (Figure 4)","Representations of individual fingers (digits) were not apparent in young monkeys, but were moderately strong in 2-7 m.o. monkeys and very clear in older monkeys (> 1 year) (Figure 6)","Monkeys raised in the absence of any visual input navigate their world primarily by touch, yet show no clear reorganization of these topographic patterns (Figure 7)"],conclusions:["With this piece of the cortical puzzle missing, it now appears that virtually all of cortex is laid out topographically at birth (Figure 8). The natural question for me is \"why\"? Surely there's some tradeoff between locking in some structure and being flexible to change, and in this case it seems that the maps don't reorganize under even the most dramatic differences in early experience.","The authors argue that these proto-maps evident at the large scale are useful for scaffolding the emergence of finer scale representations. What would some possible mechanisms for that actually look like, here or in the visual system?"],other:[]}}},141:function(e){e.exports={metadata:{title:"Universal Mechanisms and the Development of the Face Network: What You See Is What You Get",authors:["Arcaro, MJ","Schade, PF","Livingstone, MS"],institutions:["Harvard University"],date:"2019-06",journal:"Annual Review of Vision Science",doi:"10.1146/annurev-vision-091718-014917",url:"https://www.annualreviews.org/doi/abs/10.1146/annurev-vision-091718-014917",keywords:["review","macaque","IT","faces","development"],review_date:"2019-07-30",one_sentence:"Arcaro et al. argue against a 'face-specific developmental program', and propose instead that there exists some set of developmental rules that applies equally throughout higher visual cortex"},review:{summary:["The face-processing network in primates can be understood by appealing to simple rules like the dominance of retinotopic maps, plasticity and development rules in other cortical areas, and visual experience. This perspective opposes the alternative view that faces are 'special' and have unique hardware or associated learning rules"],background:["Faces are of obvious ecological relevance, and primates spend an extraordinary amount of time looking at faces. Regions selectively responding to faces have been localized to stereoptypes locations in humans and non-human primates alike (along with regions selective for other categories, e.g., body parts, places), as supported by reports of acquired prosopagnosia","The belief that the face processing system is 'special' depends on the assumption that either 1) the neural hardware involved in face processing is unique or 2) the developmental program acting on that hardware is unique"],approach:["This is a review paper which primarily draws on studies using fMRI and electrophysiological recording in humans and macaques"],results:["Visual Hierarchy: Expansion of cortical territory by the formation of multiple mirrored maps appears to be a universal principle in brain development. Through this lens, the multiplicity of face regions at different stages is unsurprising (Figure 2; see also Conway 2018)","Developmental Rules: In early visual cortex, the patterns of orientation tuning, ocular dominance, etc, can be understood as self-organizing, activity-dependent mechanisms. I'm not sure I agree that this means that V1 doesn't 'serve a particular function but rather arises as a consequence of more general wiring rules', though. The authors propose that activity-dependent self-organization continues into all extrastriate regions, allowing each stage to represent some features across all of visual space","Developmental Rules: The fact that face-selective domains span multiple retinotopic maps instead of being isolated to a single retinotopic map suggests either 1) a break in the expected hierarchy, or 2) a lack of sufficiently high-resolution and high-coverage recording techniques.","How it Gets its Spots: The kinds of push-pull models that succeed in explaining early visual cortical phenomenology, e.g., ocular dominance stripes, might be useful in understanding how higher visual areas get parcelled out for different categories (see Plaut/Behrmann 2011)","How it Gets its Spots: The correlation between semantic category, curvature selectivty, and eccentricity makes it almost impossible to figure out in adults, and figuring out which feature set explains emergence first (in developmental time) has eluded current methods. The authors' stance is that in any case, regions of IT are effectively not specialized until later in life, although retinotopy appears present at birth.","Just Look for the Bare Necessities: Converging lines of evidence suggest that innate preference for face stimuli may result from a combination of an upper-visual-field bias and the saliency of small, round, dark things (i.e., eyes)","The final two sections of the paper discuss how 1) face regions continue to develop in adulthood and 2) how recent results from cross-modal stimulation suggesting that face regions always process face-like-stimuli may be explained by general connectivity patterns from IT cortex to other sensory regions in the brain."],conclusions:["Parsimonious explanations are both desirable and available for understanding the face processing system. The authors argue that the face-processing system emerges from simple rules that apply generally throughout cortex","They interpret this parsimony to mean that instead of asking what a brain region does, we should ask how universal rules act on environmental reinforcement. I mostly agree with that point, although I think the difference is rather semantic at that point. Isn't understanding how a brain region allows complex behavior via enviornmental reinforcement the same as asking what it does, or what it's good for? Ultimately, it may be that theories about how the system actually develops will separate these two apparently similar positions"],other:["This reads in many ways like a rebuttal to other recent reviews worth looking into, including Powell et al., 2019 and Op de Beeck et al., 2019"]}}},142:function(e){e.exports={metadata:{authors:["Bartoldson, BR","Morcos, AS","Barbu, A","Erlebacher, G"],institutions:["Florida State University","Facebook AI Research"],keywords:["neural network","pruning","generalization","deep learning"],title:"The Generalization-Stability Tradeoff in Neural Network Pruning",journal:"arXiv",doi:"",url:"https://arxiv.org/abs/1906.03728",date:"2019-09",review_date:"2019-10-25",one_sentence:"Bartoldson et al. demonstrate that pruning many large weights from a network can actually improve generalization, likely because it creates instability in much the same way that noise would"},review:{summary:["Pruning -- the act of removing neural network parameters -- not only compresses the model description, but also improves generalization on held-out data","Pruning many large weights leads to better generalization gains and can be understood as creating large instability in model performance"],background:["Neural network models are likely vastly overparameterized for the tasks they perform, leading many to seek   lighter models that don't sacrifice task performance. One approach has been pruning small weights, or those that are unlikely to affect task performance","A counter-intuitive finding has been that in addition to reducing the number of weights needed, pruning can actually improve test-set generalization in some cases. However, because different pruning approaches are employed in the literature, it's unclear why it only sometimes improves generalization"],approach:["A few shallow models trained on CIFAR10 are hit with different types of pruning during training: pruning 1% of small weights, 13% of small weights, or 13% of big weights","Different styles of pruning are employed, ranging from pruning those with large L2-norms to E[BN] pruning, which allows feature strength to be evaluated even with batch normalization","To compare to the effects of pruning, two kinds of noise are added: zeroing noise, which holds certain weights at 0 for a fixed number of batches, and gaussian noise added for a fixed number of batches."],results:["For two networks, pruning many large weights led to 1) bigger immediate dips in performance on the next batch (instability), but 2) even higher performance once the network recovers (generalization). This correlation in general is fairly strong (Figure 1)","For both L2-norm and E[BN] pruning, pruning large weights leads to higher instability as pruning percentage increases, but in E[BN] pruning, you can hit higher instability while removing fewer units (Figure 4)","As long as noise (both zeroing and Gaussian) is applied for long enough, the same kinds of instability and then generalization boosts can be observed"],conclusions:["Instability in network parameters appears to be key in creating generalization boosts, and a fast/efficient way to achieve this instability is by pruning large weights according to the E[BN] pruning rule"],other:["These networks are fairly small and trained on a small dataset (CIFAR-10). I'd be curious to see how these results hold up with larger networks trained on ImageNet"]}}},143:function(e){e.exports={metadata:{title:"Dissociations and Associations between Shape and Category Representations in the Two Visual Pathways",authors:["Bracci, S","Op de Beeck, H"],institutions:["KU Leuven"],date:"2016-01",journal:"J. Neurosci.",doi:"10.1523/JNEUROSCI.2314-15.2016",url:"http://www.jneurosci.org/cgi/pmidlookup?view=long&pmid=26758835",keywords:["shape","category","fMRI","human","RSA"],review_date:"2019-06-12",one_sentence:"Bracci and Op de Beeck create a clever stimulus set to demonstrate that ventral and dorsal regions represent object category even when accounting for perceived object shape"},review:{summary:["Higher visual cortical regions in both the ventral and dorsal streams represent object shape as well as category","The emergence of category information from posterior to anterior areas (even when explicitly controlling for object shape) supports models of visual cortex function that posit a disentangling of category information along each stream"],background:["Representations of objects in higher visual areas are thought to represent object category as well as visual features. For example, we can describe regions as being 'face-selective' or 'curvature-selective' depending on the specific question.","Most studies do not account for correlation between category and object shape (although many do account for correlations between, e.g., Fourier decompositions of stimuli."],approach:["Create a stimulus set in which shape and category are orthogonal: each object in a given category has a distinct shape, and for each shape, every category has an exemplar of that shape","Show the stimulus set to subjects to 1) collect responses in an event-related fMRI design and 2) judge behavioral similarity by the 'multiarrangement method'. Note that the subject pools for (1) and (2) are independent","Formalize three models: similarity in representations based on silhouettes, on shapes, or on categories, and identify regions whose activity is consistent with the predictions of those models","Use localizer and anatomical atlas to define ROIs in early visual cortex, the ventral stream, and the dorsal stream in each subject"],results:["The placement of objects in 2D when subjects were instructed to organize based on shape was indpendent of the placement when organizing by category (Figure 2), supporting the validity of the stimulus set","The activity of many ROIs correlates significantly with the shape-based model, whereas a smaller subset correlates with the category-based model (Figure 4). To me, this is the core result: that even when controlling for shape explicitly, traditionally category-selective regions still show substantial category-level information","Similarity in responses across ROIs, across subjects, suggests organization primarily along the posterior-to-anterior axis and secondarily along a ventral-to-dorsal axis (Figure 4)","Although both have category-level information, dorsal and ventral regions cluster differently, suggesting different types of category information in each subset of ROIs. Ventral ROIs are more consistent with an animacy model, whereas dorsal ROIs were more consistent with an action vs. non-action model (Figure 5)."],conclusions:["Many visual areas contain information about both shape and category, but this sitmulus set allows us to conclude that there is category-information above and beyond shape.","Category-information appears to proceed primarily along an A-P axis, consistent with existing theories of the emergence of category information outside of primary visual cortex (see DiCarlo et al., 2012)"],other:["The main result is interesting, but I don't know what to make of the post-hoc analyses. The stimulus set was designed to test a clear hypothesis, but the additional models (animacy, action, etc) used to explain the dissociation in activity between clusters were not explicitly controlled in the same way."]}}},144:function(e){e.exports={metadata:{title:"Development of orientation preference maps in ferret primary visual cortex",authors:["Chapman, B","Stryker, MP","Bonhoeffer, T"],institutions:[" Max Planck Society"," University of California San Francisco"],date:"1996-10",journal:"The Journal of Neuroscience",doi:"10.1523/JNEUROSCI.16-20-06443.1996",url:"http://www.jneurosci.org/content/16/20/6443.full.pdf",keywords:["ferret","orientation","development","V1","optical imaging"],review_date:"2019-08-22",one_sentence:"Chapman and colleagues show that while orientation tuning maps in ferrets begin emerging at different times, they are remarkably stable over the course of maturation"},review:{summary:["Ferret orientation maps appear between postnatal day 31 and 36, then mature with remarkable stability of iso-orientation domain position, size, etc","The emergence of orientation maps does not appear to be dependent on visual experience"],background:["Electrophysiological studies indicate that orientation maps in cats, monkeys, and other animals are mature very early in life, but the development of these maps in individual animals has not been observed","Ferrets are a desirable model organism as their gestation time is much shorter than the cat, although the development of the visual system is about equal in the two species. Thus, ferrets are physiologically more robust to do experiments on even as visual neuron selectivity is developing"],approach:["Intrinsic optical imaging of ferret V1 was conducted between P31 and P55 in eight animals","Animals were shown images of gratings at four orientations (horizontal, vertical, two obliques)","Orientation tuning was evaluated at each of multiple timepoints, then compared across timepoints or animals with a cross-correlation-inspired approach"],results:["V1 starts untuned to orientations, but develops orientation tuning between day 31 and 36, although this start is different in each animal (compare Figures 1, 2)","During maturation, tuning strength increases, although the location and size of iso-orientation domains is remarkably constant (Figures 1-3)",'Selectivity emerged first for vertical and horizontal orientations in "some, but not all, animals"','The pattern of orientation tuning along the cortical surface varied from animal to animal: in some it was spotty, in others stripy, and in at least one, not organized at all (Figure 5). This seems to conflict with the "common design" proposed by Wolf, Kaschube, and colleagues? In animals that have stripy patterns, those strips mostly run perpendicular to the V1/V2 boundary (Figure 6)',"Cells appear to acquire orientation tuning two days earlier when assessed with electrophysiology than when assessed with intrinsic imaging (Figure 7B)"],conclusions:["The stability of the maps, despite other changes thought to occur in V1 due to visual experience, points to the putative importance of orientation maps for... something","That orientation preference is detected earlier with electrophysiology suggests that either 1) something is insensitive about the optical imaging technique or 2) that cells with different tuning are more spatially mixed up early on (I don't think there's much support for this idea, in this paper or elsewhere)"],other:["The stability of maps aside, it's interesting how variable the onset of maturity and pattern of tuning on the cortical surface is across animals. In fact, it leads me to question how readily these results should be ported over to our thinking about other species, including humans"]}}},145:function(e){e.exports={review:{summary:["Complicated methods for unsupervised learning via exemplar clustering/aggregation aren't required for high-performance, a simple contrastive algorithm can achieve state of the art given sufficient batch-size and data augmentation."],background:["A long-standing goal of building powerful learning systems is to break the dependence on labeled data and supervised learning. The most promising approaches (in terms of how readily their learned representations transfer to ImageNet performance) are variations of clustering methods that aim to learn similar representations for transformations of the same exemplar.",'Many successful approaches rely on (1) data augmentation and (2) a "memory bank" that is continuously updated to provide "negative examples": images that are not of the same identity as the image being presented.'],approach:["Instead of using a memory bank, generate a positive pair by taking an example image and applying some combination of color jitter, Gaussian blur, and random crop. All other images in the batch are the negative examples.",'Use ResNet to get an encoding of the images, then learn a separate "projection head" MLP to convert the ResNet representations into low-D vectors to be aggregated/clustered/etc',"Take advantage of massive computational resources to fit huge batch sizes, and use the LARS optimizer to effectively train the network despite batch size increases."],results:["To identify which augmentations might be useful, try all combinations of two sequential preprocessing steps. First cropping, then applying color jitter leads to the best top-1 ImageNet performance in the supervised context (Figure 5). Color distortion helps distinguish images on the basis of their pixels, which probably helps a lot! (Figure 6)","The augmentations that help supervised ImageNet don't necessarily align with those that lead to best unsupervised performance (Table 1)","Adding a non-linear projection head leads to a reliable improvement over a linear alternative (Figure 8)","Unsurprisingly, big batch sizes help an enormous amount! (Figure 9)","This method, with its large batch size and simple (no memory bank) approach achieves state of the art transfer to ImageNet top-1 (Table 6)"],conclusions:["The complexity in other methods is not necessary to achieve high performance. Eshed's note: it's hard to say if this method is truly simpler, since it also requires learning a new non-linear head off of the NN representation.","The authors attribute a large part of the success of this method to the data augmentation process used here."],other:[]},metadata:{authors:["Chen, T","Kornblith, S","Norouzi, M","Hinton, G"],institutions:["Google Research, Brain Team"],keywords:["unsupervised learning","augmentation","machine learning"],title:"A Simple Framework for Contrastive Learning of Visual Representations",journal:"arXiv",doi:"",url:"https://arxiv.org/abs/2002.05709",date:"2020-02",review_date:"2020-02-28",one_sentence:"SOTA on unsupervised representation learning can be achieved with contrastive losses computed within a batch (as opposed to requiring a memory bank)"}}},146:function(e){e.exports={review:{summary:["This paper investigates the role of patterned vision (inputs while eyes are open) on the formation of ocular dominance and orientation maps in cat V1. They show that while initial map formation proceeds independent of patterned vision, experience is necessary to sharpen and retain such maps."],background:["Cells in cat primary visual cortex are tuned to (among other things) stimulus orientation and are sensitive to which eye the input was presented to. Cells responding more strongly to stimuli in a given eye cluster in ocular dominance columns, while cells with similar preferred orientations cluster into stereotypical maps of orientation preference.","The formation of sharp maps could be attributed to 1) genetic wiring programs, 2) retinal waves, or 3) visual experience. The importance of each had not been determined prior to this work."],approach:["Evaluate orientation maps and ocular dominance maps in cat primary visual cortex using intrinsic optical imaging and electrophysiological recording","In a subset of cats, suture both eyelids shut (binocular deprivation; BD) prior to the time of natural eye opening, then remove sutures prior to assessment of map structure (so that you can actually show stimuli to the cat)"],results:["Stimulation of the contralateral eye drives stronger responses (and cleaner orientation maps) than stimulation of the ipsilateral map (Figure 1A) for the first three weeks of life. After that, the relative response depending on eye remains asymmetrical for BD cats but becomes more equal in normal cats (equal meaning well-matched between contralateral and ipsilateral eyes; Figure 3)","For the first three weeks of life, maps of orientation preference from stimulation of either eye become increasingly similar, regardless of whether the eyes were sutured shut or not. However, from week 3 onward, the maps obtained from either eye plateau in similarity for normal cats and deteriorate in deprived cats (Figures 1, 2)","Orientation selectivity increases up until about week 4 in both BD and normal animals, then declines in BD animals after that (Figure 3). At about 4 weeks, the ipsilateral eye will also catch up to the contralateral eye with respect to selectivity in normal animals (Figure 3B) but not in BD animals.","Ocular dominance maps are evident as early as P14 (Figure 4), much earlier than previously thought."],conclusions:["Because ocular dominance maps are apparent so early, the formation of ocular dominance maps must coincide with a time point where responses are much higher in one eye than other. This more or less rules out strictly Hebbian mechanisms for ocular dominance map formation, as they would favor a takeover from the more dominant eye.","Orientation maps may be established by the dominant contralateral eye, which then dictates the form of the map to be assumed by input to the weaker ipsilateral eye, instead of a push-pull competition that results from both eyes fighting to establish the pattern","Pattern vision appears to be totally irrelevant during the third week of life, where BD cats and normal cats look identical. EM note: is this necessarily true? Are there differences invisible to these techniques that lead to different phenomena later on?",'The period in which patterned vision does seem to matter is during the "critical period" in which molecular mechanisms permit or forbid substantial changes to synaptic weights'],other:["This paper is very elegant and presents a fascinating view of the specific role of patterned vision in early development. My only gripe is that it's fairly short on methods -- I couldn't find, for example, when exactly the sutures were removed or how orientation tuning was established, except that it refers to prior work."]},metadata:{authors:["Crair, MC","Gillespie, DC","Stryker, MP"],institutions:["UCSF"],keywords:["cat","v1","critical period","orientation","deprivation"],title:"The Role of Visual Experience in the Development of Columns in Cat Visual Cortex",journal:"Science",doi:"10.1126/science.279.5350.566",url:"http://science.sciencemag.org/content/sci/279/5350/566.full.pdf",date:"1998-01",review_date:"2020-01-24",one_sentence:"Orientation maps in cat primary visual cortex are innately laid out, but require continued visual input to sharpen and avoid degradation"}}},147:function(e){e.exports={metadata:{title:"Ocular Dominance Peaks at Pinwheel Center Singularities of the Orientation Map in Cat Visual Cortex",authors:["Crair, MC","Ruthazer, ES","Gillespie, DC","Stryker, MP"],institutions:["UCSF"],date:"1997-05",journal:"Journal of Neurophysiology",doi:"10.1152/jn.1997.77.6.3381",url:"http://dx.doi.org/10.1152/jn.1997.77.6.3381",keywords:["cat","V1","orientation","ocular dominance"],review_date:"2019-12-31",one_sentence:"Crair and colleagues demonstrate that in cat V1, pinwheel centers nearly perfectly coincide with peaks in the ocular dominance map"},review:{summary:["Whereas prior work had only shown that pinwheel centers tend to lie on the center lines of ocular dominance bands, this study shows a much tighter relationship: that ocular dominance peaks coincide with pinwheel centers"],background:["Individual neurons in primary visual cortex (V1) are tuned to a variety of parameters, including the orientation of a stimulus and the eye (left or right) that a stimulus is presented to. Previous studies in cats had reported a coarse-grained relationship between ocular dominance maps and orientation maps, whereby pinwheel centers lie along the center lines of ocular dominance bands"],approach:["Intrinsic optical imaging was used to measure activity in cat primary visual cortex (ages: 22 to 56 days) during presentation of oriented gratings. Stimuli were shown to the two eyes separately to infer an ocular dominance ratio map.","Orientation maps were sometimes smoothed, then pinwheel centers were identified and compared with the center lines of ocular dominance columns in the same regions"],results:["As reported previously, orientation maps constructed from stimulation to either eye were more or less identical (Figure 1J, 1L)","On average, ocular dominance peaks and pinwheel centers were only 138 microns apart, which is much smaller than expected if one of the maps is shuffled (Figure 2)","The relationship between OD peaks and pinwheel centers is more robust than the more general observation that pinwheels lie along ocular dominance center lines (as reported previously)"],conclusions:["Hypothesis 1 for co-incidence of OD peaks and pinwheel centers: there is a common molecular process that creates peaks in monocular responsiveness and pinwheel centers, e.g., koniocellular projections from the LGN to CO blobs in V1. Eshed's note: this is interesting, as it suggests that most map structure for these two parameters (orientation and eye preference) is driven by a single seed phenomenon","Hypothesis 2: there is a common online learning program that is satisfied simultaneously by OD peaks and pinwheels, e.g., pressure for local correlations. Pinwheel centers achieve this by having weak orientation tuning and thus responding similarly to many inputs, and OD peaks achieve this by having, e.g., spontaneous activity that comes from a single eye (retinal waves)","Hypothesis 3: (this one was the least clearly stated in my opinion) that maps for multiple modalities are vying to reduce total variability in represented parameters at a given cortical location."],other:["One thing I personally struggle with is in understanding how much ocular dominance maps really matter from a functional perspective. Is a model of visual cortex functional architecture incomplete without dealing with OD maps, or are they a strange artifact of the particular sensors we're dealing with? This paper shows a tight relationship between topography of ocularity and orientation preference, but what does that mean for neural function?","While the small median distance between pinwheel centers and OD peaks is indeed small, the figures aren't all that striking. The two maps don't appear especially aligned to my untrained eye."]}}},148:function(e){e.exports={review:{summary:["Slow feature analysis (SFA) on retinal-wave-like inputs results in functions consistent with measured complex cells in mammalian V1, suggesting slowness in temporal coding as an objective of the early visual system","The success of this unsupervised method suggests that the emergence of complex cells in utero is plausible, i.e., environmental feedback is not needed to get complex cells"],background:["The emergence of simple cells has been explained by sparse-coding of natural scenes (Olshausen and Field), but 1) receptive fields are formed prior to natural image inputs and 2) other unsupervised objectives are plausible","Slow feature analysis is an unsupervised feature learning method that, given a temporally-varying input, identifies functions that lead to slowly-varying and decorrelated scalar outputs","In the developing visual system, the time-varying input of interest is retinal waves: sweeps of endogenous and spontaneous activity that propagate from the retina to the midbrain and to visual cortex"],approach:['Simulate retinal-waves using a biologically-plausible model introduced in Godfrey and Swindale, 2007. Collect 289 "movies", each consisting of 3600 frames of size 16 x 16 pixels. Reduce the dimensionality of the input to 50 via PCA.',"Perform SFA on the inputs and evaluate a number of characteristics, including orientation tuning and modulation ratio (simple vs. complex) by presenting a set of sine gratings at different orientations, spatial frequencies, and phases. Additionally, synthesize the optimal and least-optimal inputs for each learned SFA unit for visual inspection."],results:["Many optimal stimuli resemble oriented Gabors, as found in mammalian V1 (Figure 2). Note: many of these also don't look like \"textbook\" V1-cells, but that doesn't mean they're biologically incorrect... More on that later.",'Many cells appear "complex" in that they respond similarly to gratings at all phases, and there is a spread of orientation selectivity indices (OSIs) that appear to loosely match biological distributions from postnatal week 6 in ferrets (Figures 3, 5).',"The optimal inputs to phase-invariant SFA units resemble quadrature pairs, mirroring the mechanism proposed in studies of mammalian visual cortex (Figure 6)."],conclusions:["Slowness is a biologically-plausible unsupervised objective that may explain the presence of complex cells at birth. However, no claim is made that the algorithm used here is biologically valid.","This work makes the intriguing prediction that disruption of retinal waves' temporal structure while preserving aspects of the spatial structure should prevent emergence of complex cells at birth. To my knowledge, this has not been tested (and how could it be?)."],other:["The core claim of this paper rests on a series of ultimately qualitative comparisons to existing data. While the authors do a good job of showing a range of phenomena, they face two challenges. First, these results span multiple species and recording techniques due to lack of a unified measured dataset. Second, there is no quantitative fit to any such dataset! We simple don't know if this model is a better predictor of V1 firing rates than deep nets (Cadena et al., 2019), wavelet banks, ICA, sparse-coding, or any number of alternative explanations of V1 receptive field structure."]},metadata:{authors:["D\xe4hne, S","Wilbert, N","Wiskott, L"],institutions:["Humboldt University of Berlin","Ruhr University Bochum"],keywords:["v1","complex cell","slow feature analysis","simulation","retinal waves"],title:"Slow Feature Analysis on Retinal Waves Leads to V1 Complex Cells",journal:"PLOS Computational Biology",doi:"10.1371/journal.pcbi.1003564",url:"https://core.ac.uk/display/87613734",date:"2014-05",review_date:"2019-11-27",one_sentence:"D\xe4hne, Wilbert, and Wiskott show that given retinal-wave-like input, functions optimized to maximize temporal 'slowness' of outputs resemble V1 complex cells"}}},149:function(e){e.exports={metadata:{title:"Computational Neuroethology: A Call to Action",authors:["Sandeep Robert Datta","David J. Anderson","Kristin Branson","Pietro Perona","Andrew Leifer"],institutions:["Caltech","Janelia","Harvard","Princeton"],journal:"Neuron",doi:"10.1016/j.neuron.2019.09.038",url:"https://doi.org/10.1016/j.neuron.2019.09.038",date:"2019-10",review_date:"2019-10-17",one_sentence:"Datta and colleagues argue that now is the time for neuroscientists to push toward rich measurements of complex, naturalistic behaviors",keywords:["ethology","motion tracking","behavior"]},review:{summary:["Recent advances in hardware and software have made it possible to record the pose of an animal performing 'naturalistic' behaviors and to automatically segment and label such recordings","Because neural activity is high-dimensional, we may need high-dimensional measurements of brain output (behavior) to make sense of it"],background:["Ethology has traditionally been concerned with the construction of state transition probabilities, where states are animal behaviors that an ethologist manually identifies and labels","Until recently, marker-assisted tracking of animals was the best we could do. Instead, many trained animals to do stereotyped tasks to ask questions about how brain activity relates to behavior","In parallel, brain measurements are becoming increasingly high-dimensional and comprehensive, with high density, high coverage optical and electrophysiological systems in place for worms, flies, and mice"],approach:["This is a review paper that discusses a number of supervised and unsupervised approaches to labeling behavior from camera footage. The main techniques described are DeepLabCut, JABAA, CADABRA, MoSeq, and MotionMapper"],results:["The authors outline recent progress in tracking mice, zebrafish, and worms with the techniques described.","The bulk of the paper acknowledges that the really hard problems still need to be solved: at which spatiotemporal scale should behavior and neural activity be quantified? Should we want interpretable or predictive models, especially when we can't have both? How naturalistic of behaviors can or should be measured?"],conclusions:['While the core questions of the field remain open, the tools are arriving to take a meaningful bite out of the array of question marks. The "call to action" in the paper title is indeed for neuroscientists to begin employing these methods and to get serious about quantifying behavior.'],other:['The authors seem to flip-flop a bit on whether models that are highly predictive but uninterpretable are better than models that are somewhat predictive and highly interpretable. This parallels a general tension in machine-learning-for-neuroscience that I\'ve seen, and relates strongly to the "Learning Divisive Normalization..." paper that I wrote a review on recently']}}},150:function(e){e.exports={review:{summary:["'Critical period' is an overloaded term that can refer to many overlapping time periods in the development of a system","In general, critical periods last from eye opening to puberty and differ across brain regions and functions"],background:["Critical periods (and/or 'sensitive' periods) have primarily been studied in cats in the context of copmetition of ocular dominance columns"],approach:["This is a book chapter that discusses a number of methods, including:","Monocular deprivation, in which one eye is sutured shut for a period of time","Reverse suturing, in which one eye is sutured shut for a period of time, then the other eye is sutured shut","Controlled rearing, in which the visual statistics of an animal's environment are tightly controlled"],results:["As a rule of thumb, critical periods begin around the time of eye opening and close at puberty, although this is variable, e.g., macaque critical period closes by 1 year, but puberty does not begin until 3-3.5 years.","In general, the magnitude of the effect scales with the length and severity of the deficit, and is thus largest during the peak sensitivity of the critical period and for extreme manipulations","Later stages of the visual system have later and longer-lasting critical periods, e.g., monocular deprivation doesn't appear to affect the retina and LGN, but does affect V1. IT seems to remain plastic for longer (what is the evidence here?) and the hippocampus may be indefinitely plastic to support memory formation","Critical period timing, even within a given region, varies depending on the property assessed. For example, the critical period for orientation and direction tuning closes before that for ocular dominance in cat V1. Daw argues this has functional advantages, as one would want cells nicely tuned to direction and orientation before stereo vision (a computation requiring both eyes) can be refined","The critical period is delayed in dark-reared animals, suggesting that the opening and closing of the window can be disocciated from animal age even within the same species"],conclusions:["One must distinguish between the critical period for initial development of function, the critical period for disruption of function, and the critical period for recovery of function, as they may all differ"],other:["Nearly half of this chapter is devoted to detailing assays of critical periods in human subjects, often relating to visual acuity. Given the relative mystery regarding mechanism that these data offer, I don't review it here."]},metadata:{authors:["Daw, NW"],institutions:["Yale"],keywords:["critical period","deprivation","review"],title:"Critical Periods",journal:"Visual Development (Book)",doi:"",url:"https://www.springer.com/gp/book/9781461490586",date:"2014-01",review_date:"2020-02-06",one_sentence:"Daw reviews critical period basics in both human and non-human model systems"}}},151:function(e){e.exports={metadata:{title:"Inputs to directionally selective simple cells in macaque striate cortex",authors:["De Valois, RL","Cottaris, NP"],institutions:["UC Berkeley"],date:"1998-11",journal:"PNAS",doi:"10.1073/pnas.95.24.14488",url:"https://www.pnas.org/content/95/24/14488.short",keywords:["macaque","V1","direction","magnocellular","parvocellular","biphasic"],review_date:"2019-05-16",one_sentence:"De Valois and Cottaris demonstrate that two classes of direction-nonselective simple cells, likely inheriting responses from distinct LGN pathways, drive direction-selective V1 cell activity"},review:{summary:["Spatiotemporal receptive fields of V1 cells in macaque can be strongly biphasic, and many are direction selective","The activity of direction-selective cells can be explained by two principal components, a biphasic and a monophasic component","These components match the timecourses of two classes of V1 simple cells without direction selectivity nearly perfectly"],background:["Theories had suggested that the inputs to direction-selective cells might be matched in preference for orientation and spatial frequency, but differ in phase. No experimental evidence had existed to support that claim","Existing motion models posit that the inputs to these cells would both have biphasic temporal response profiles"],approach:["Single cells in the central 5 degrees of macaque V1 were recorded, and selectivity for spatial frequency, orientation, and phase was mapped with drifting gratings (for determining preference) and black and white bars (for determing STRFs)","For each cell, a direction-selectivity index and a biphasic index were computed. Cells with a biphasic index less than 0.3 were considered weakly biphasic (except in Figure 3, for which the threshold switches to 0.4)","SVD was used to determine the first two principal components of the STRFs of direction-selective V1 cells"],results:["Many V1 cells have receptive fields that vary in time (Figure 1A, C), although some have monophasic and unchanging responses (Figure 1B)","Many V1 cells are biphasic in their temporal responses, although the majority are not (Figure 2)","The timecourses of non-directional-selective simple cells match the first two principal components of the direction-selective cells' receptive field (Figure 3)","Alternative models which posit that all inputs to direction-selective cells are biphasic do not fit the empirical data as well (Figure 4)","Most of the strongly biphasic cells preferred black and white to chromatic stimuli, while most of the weakly biphasic cells exhibited spectral opponency."],conclusions:["The inputs to direction-selective V1 cells are likely arriving from simple cells who, in turn, receive inputs from magnocellular and parvocellular LGN neurons. This conclusions is supported by measurements of repsonse latency and spectral opponency."],other:["It would have been great to see any attempt at quantification for either 1) how well the simple cell timecourses actually match the derived principal components and 2) how well the models described fit the data recorded."]}}},152:function(e){e.exports={metadata:{title:"Spatial Frequency Selectivity of Cells in Macaque Visual Cortex",authors:["De Valois, RL","Albrecht, DG","Thorell, LG"],institutions:["UC Berkeley"],journal:"Vision Research",doi:"10.1016/0042-6989(82)90113-4",url:"https://www.sciencedirect.com/science/article/pii/0042698982901134",date:"1982-09",review_date:"2019-11-04",one_sentence:"De Valois et al. comprehensively measure the spatial frequency tuning of simple and complex cells in macaque V1 and find a range of preferences and tuning widths",keywords:["macaque","v1","spatial frequency","simple","complex"]},review:{summary:["Cells in macaque V1 are tuned to a range of spatial frequencies, and the bandwidth of this tuning varies considerably. ","Sharp tuning for orientations and spatial frequencies can be considered a signature of a role for V1 in performing a Fourier decomposition of visual inputs that either arises de-novo in V1 or is carried by parallel pathways that originate in the retina"],background:["At the time of this paper, a framework for the early visual system as a Fourier analyzer had been proposed, but cells with sharp tuning to particular spatial frequencies had not been conclusively identified in macaque","Psychophysical studies (without measurements of neural activity) had suggested that channels selective to specific spatial frequencies should exist"],approach:["Present sine wave gratings in the receptive field of a cell being recorded. Each grating was presented at multiple contrast levels and spatial frequencies to map the contrast sensitivity function of the cell. Orientation and temporal frequency were fixed at peak values for each cell while measuring spatial frequency tuning.","Cells recording varied from foveal to five degrees periphery, most cells were parafoveal (0 to 1.5 degrees)"],results:["Both simple and complex cells were identified by the spatial frequency mapping procedure. Simple cells were characterized by a peak response that started at the basal firing rate, peaked at some point, and came back down. Complex cells, on the other hand, had a baseline shift in firing rate but not much modulation in response to the grating (Figure 1)",'Consistent with Hubel and Wiesel\'s statement that simple cells sum excitatory and inhibitory fields, simple cells in this sample that are evaluated with a counterphase flickering grading show stimulus positions at which excitation and inhibition balance to produce no response. This \'nullification\' is not observed for complex cells. The authors note that these characteristics map onto retinal "X" and "Y" cells respectively',"The range of tuning bandwidths among cells in the population is quite large (many narrowly tuned cells, many broadly-tuned cells), and the simple/complex or foveal/parafoveal distinctions are not strong predictors of a cells tuning bandwidth (Figures 4, 5).","Neurons were tuned to spatial frequencies between 0.5 and 15 cycles per degree of visual angle. Cells that are neighboring often have dramatically different spatial frequency tuning (Figure 6)."],conclusions:["As Hubel and Wiesel reported, simple and complex cells are easily differentiable and detectable in V1, although more simple cells were reported here than in H/W's work.","Because cells that are sharply orientation tuned are also sharply spatial-frequency tuned, one can consider these cells the basis of a Fourier-analysis role for V1. "],other:['The authors do some interesting math regarding what they call a "cortical integration region (CIR)" which is roughly a complete orientation and ocular dominance hypercolumn. This space would be roughly 1mm square in macaque, with about 100k neurons. Given the tuning distribution provided in Fig 6 of this paper, we expect that roughly 32k cells would be sharply tuned, leaving 1600 cells per orientation (assuming 20 distinct orientation)']}}},153:function(e){e.exports={metadata:{title:"Untangling invariant object recognition",authors:["DiCarlo, JJ","Cox, DD"],institutions:["MIT"],date:"2007-07",journal:"Trends Cogn. Sci.",doi:"10.1016/j.tics.2007.06.010",url:"http://dx.doi.org/10.1016/j.tics.2007.06.010",keywords:["IT","object recognition","manifold","hierarchy"],review_date:"2019-04-18",one_sentence:"DiCarlo and Cox argue that the role of the primate ventral stream functions to untangle retinal representations into representations from which useful information can be linearly decoded"},review:{summary:["Linear decoding of information such as facial identity is more difficult from the representation of a group of retinal ganglion cells than it is from a group of IT neurons","The role of the primate visual system may be to progressively untangle the retinal representation","The untangling perspective largely ignores biological features such as recurrence, noting that the timecourse of feedforward decodability precludes feedback as a necessity"],background:["The ability of primates to rapidly and reliably recognize visual objects under a wide range of lighting/viewpoint/pose conditions is attributed to a series of cortical areas termed the 'ventral visual stream'","The last stage of the ventral visual stream in macaque, inferotemporal (IT) cortex contains neurons from which object identity can be easily, i.e., linearly, decoded","One important point is that all of the information available to IT comes from the retina! It's just that the information in the retina is not formatted properly for efficient readout"],approach:["Theoretical arguments supported by some simulations of what manifolds for different objects in different representational spaces might look like"],results:["Pixel-like (maybe retina-like?) representations of two facial identities at different viewpoints are deeply tangled such that linear decoding would fail (Figure 1)","A V1-like simulation shows some disentangling of the retinal representation, but the IT-like representation is quite good at separating identities while maintaining some sensitivity to variables that may matter, such as viewpoint and pose. The authors not this differs from the 'classical' model of IT, in which information about those variables is discarded to maximize sensitivity to identity (Figure 2)"],conclusions:["Untangling of representations moves us away from thinking about single neuron response properties, and instead emphasizes where representations lie in a space spanned by a population of neurons","The task of the ventral stream is to separate identities of objects, but to simultaneously balance tolerance and generalizability to identity-preserving transformations of that object","Local and iterative 'flattening' of manifolds may be the implementational strategy of the ventral stream"],other:[]}}},154:function(e){e.exports={metadata:{authors:["Durbin, R","Mitchison, G"],institutions:["King's College","Stanford University"],keywords:["self-organizing map","orientation","wiring"],title:"A dimension reduction framework for understanding cortical maps",journal:"Nature",doi:"10.1038/343644a0",url:"https://www.nature.com/articles/343644a0",date:"1990-01",one_sentence:"Durbin and Mitchison argue that maps in early visual cortex can be understood as the solution to a self-organizing map problem that minimizes wiring length and reduces dimensionality",review_date:"2019-09-10"},review:{summary:["A self-organizing map approach to modeling the development of cortical maps is wiring-efficient, fast, and biologically plausible via Hebbian learning","Application of the self-organizing algorithm to stimuli defined by orientations and retinal position yield maps that are qualitatively similar to orientation maps observed in cat visual cortex"],background:["Retinotopy, ocular dominance, and orientation are all mapped relatively smoothly in the primary visual cortex of cats and macaques, which can be considered effectively 2D due to the columnar nature of V1 functional architecture","Because cortex is 2D and there are more than 2 stimulus dimensions to map (x, y, orientation, ocular dominance), the mapping from 'parameter space' to the cortical surface must contain imperfections","Prior work from these authors demonstrated that mapping a 2D parameter space onto a theoretical 1D cortex yields relatively smooth parameter maps when using a biological cost function that favors many short connections and allows relatively few long-range connections","Searching the space of all layouts becomes computationally difficult (and probably biologically implausible) in the mapping of 4+ dimensions to a 2D cortex, so we turn instead to self-organizing algorithms, e.g., Kohonen maps"],approach:["Create a Kohonen map in which each unit responds to some stimulus defined by (x, y, orientation angle, orientation tuning strength), and the update rule is based on a Gaussian neighborhood function","Compare the resulting map to orientation maps measured in cat primary visual cortex"],results:["For a simplified 2D parameter space -> 1D cortex mapping, applying a self-organizing algorithm yields results that are very similar in wiring cost and structure to the optimal layout (Figure 1c, d)","Mapping a 4D parameter space (x, y, orientation angle, orientation tuning) to a 2D space yields a mostly retinotopic map with singularities for orientation, as shown in cat (Figure 2). Of note is that the retinotopic map is warped near pinwheel centers in this model"],conclusions:["The self-organizing problem is both more biologically plausible and more efficient than direct minimization of wiring cost, yet it yields maps that are score very well on wire length minimization metrics","Brain evolution may have favored short connections and self-organizing maps as a way to quickly and robustly (although not perfectly) bootstrap the layout of stimulus features on the cortical surface"],other:["The self-organizing map algorithms require inputs that are fully formed stimuli, i.e., in this case they are points in a 4D space defining an orientation at some position in space. However, the maps are fully formed at birth. So, what are the stimuli/inputs to the model in utero? The authors cite Linsker '86 and Miller '89 for evidence that oriented, local inputs could drive development before eye opening."]}}},155:function(e){e.exports={metadata:{title:"A global map of orientation tuning in mouse visual cortex",authors:["Fahey, PG","Muhammad, T","Smith, C","Froudarakis, E","Cobos, E","Fu, J","Walker, EY","Yatsenko, D","Sinz, FH","Reimer, J","Tolias, AS"],institutions:["Baylor College of Medicine","University of Tubingen","Rice University"],doi:"10.1101/745323",date:"2019-08",journal:"bioRxiv",keywords:["mouse","V1","orientation","pinwheel","calcium imaging"],review_date:"2019-08-26",one_sentence:"Fahey and colleagues demonstrate that although small regions of mouse V1 may be salt-and-pepper (or micro-clustered) w.r.t. orientation tuning, there is a global orientation map that centers around one central pinwheel"},review:{summary:["Wide-field calcium imaging in eight mice indicates a large, area-wide gradient of orientation tuning that converges at a central pinwheel","The function and origins of this map remain unknown, but its existence suggests 1) a revision of how we think about orientation maps in mice and rats and 2) a role for emerging systems neuroscience techniques to identify novel phenomena in a field that's been studied to death"],background:['Maps of orientation tuning in primary visual cortex have been identified and rigorously quantified in primates and most carnivores (e.g., cats), but the story in mice and rats has always been that they exhibit "salt and pepper" tuning, or, at best, micro-columns of small degrees of clustering by orientation preference',"Cardinal bias (the over-representation of cells preferring vertical and horizontal orientations) has been previously reported in mice, and to a lesser degree, in primates. Already, this bias suggests that the orientation map in mice is not entirely random"],approach:["Measure calcium activity from L2/3 - L4 neurons in primary visual cortex in eight mice. Total imaging volume: 1800 x 1800 x 250-350 microns (very large!)","Play dynamic pink noise stimuli with coherent orientation and direction, then fit neuron responses with a two-peak von Mises function (similar to a circular Gaussian) for neurons that had more than 2.5% of variance explained. Total cells: 17k - 29k / animal"],results:["Many cells are strongly orientation tuned in V1, with a very large cardinal bias (Figure 1)","Collapsing neurons across depths makes it clear that there is a global orientation map, and projecting these points into stimulus-monitor space shows a clear pinwheel around the center of the screen (and thus, the center of retinotopic space; Figures 2, 3)","The pinwheel-like nature of the maps is relatively consistent across cortical depths (Figure 4)","The global pattern is obvious in individual animals, but the degree to which it's clearly pinwheel-like is less convincing (Extended Data Figures 3, 5)","The orientation bias cannot be easily explained by border effects, as it persists when stimuli are restricted away from the edges of the monitor (Extended Data Figure 7)"],conclusions:["The unprecedented scale of this recording was critical to discovering this large map that spans all of V1 and extends into neighboring areas -- if only focusing on small imaging regions, there is a lot of heterogeneity in tuning. Eshed's note: how do we reconcile this with the extreme precision of pinwheels in cats (Ohki et al., 2006)? What's different about the development of mice?","Large field-of-view imaging may be a useful tool in continuing to 'revisit' principles of cortical organization"],other:["I think this paper will be very impactful, as it disrupts a difficult-to-reconcile notion that despite the presence of orientation tuned cells, their position is totally irrelevant. In my mind, it's easier to think about these maps, both within and across species, as points on a continuum between extremely precise organization and more lax organization","What could be the computational or metabolic advantages of having one mega-pinwheel instead of regularly tiled pinwheels?"]}}},156:function(e){e.exports={metadata:{title:"Color and orientation are jointly coded and spatially organized in primate primary visual cortex",authors:["Garg, AK*","Li, P*","Rashid, MS","Callaway, EM"],institutions:["Salk Institute","UCSD"],date:"2019-06",journal:"Science",doi:"10.1126/science.aaw5868",url:"https://science.sciencemag.org/content/364/6447/1275",keywords:["macaque","color","orientation","CO"],review_date:"2019-07-01",one_sentence:"Garg, Li, et al. find that many orientation-tuned macaque V1 cells are also tuned to specific hues (and organized with respect to CO blobs), challenging standing models of V1 functional architecture that posit spatial and functional separation between color and oriention tuning"},review:{summary:["Instead of being strictly segregated, many V1 neurons jointly code stimulus color and orientation","Neurons were clustered with respect to hue preference and generally (but not completely) aligned with CO-dense regions"],background:["The canonical model of V1 is that there are orientation-selective cells outside of CO blobs, and these cells are generally not color-selctive","Eshed's note: when training deep convolutional neural networks to do object recognition tasks, many of the early filters violate this separate and show both chromatic and orientation tuning in the same units"],approach:["GCaMP6f imaging in macaque of 4,531 cells in superficial layers of V1 from four animals and fourteen imaging sites","Responses collected in response to square-wave gratings (eight orientations, four spatial frequencies) that were either achromatic or one of 12 equally spaced colors in CIE space","CO staining after euthanasia at imaging sites to determine correspondence between color/orientation tuning and CO","Measurements at multiple depths to determine columnar structure","Only consider responses from visually-responsive neurons"],results:["Over 46% of cells preferred chromatic stimuli to achromatic stimuli, and 11% of all cells were both strongly orientation selective and strongly color selective (Figure 3A)","Orientation selectivity generally decreased as CO intensity increased, while color preference increased with CO intensity, consistent with prior findings (Figure 3B, 3C)","97% of color preferring neurons were also tuned to at least one specific hue, with the majority of hue-selective neurons tuned to blue and red (likely due to cone contrast in the retina)","Neurons generally clustered in the xy-plane on the basis of hue preference and were organized similarly at nearby cortical depths (Figure 4A)"],conclusions:["Shape and color may be jointly processed by a population of cells outside of CO blobs that likely project to V2 pale stripes","These recordings were made in superficial layers, but deeper layers (closer to LGN input in layer IV) may show different functional signatures"],other:["Was there any tuning with respect to spatial frequency? If not, how were those data used?","Looking at the supplement, it's clear that there is clustering with respect to orientation and color tuning, but the typical patterns (e.g., pinwheels) are hard to see in these maps. I imagine they would look cleaner in ISI maps than these fine-grained 2P imaging maps"]}}},157:function(e){e.exports={review:{summary:["While stage 2 retinal waves (first postnatal week) and stage 3 retinal waves (second postnatal week) both propagate reliably from retina -> LGN/SC, stage 3 waves have a much lesser influence on V1 activity","This paper introduces in-vivo simultaneous imaging of stage 3 retinal wave propagation for the first time, opening the door for more studies of early visual system development in rodents"],background:["Retinal waves are traveling waves of spontaneous activity in the developing visual system that have been implicated in retinotopic map refinement and feature selectivity (e.g., orientation tuning -- Weliky/Katz '97)","Retainl waves proceed in 3 stages: gap-junction mediated stage-1 waves in late gestation, acetylcholine mediated stage-2 waves in the first postnatal week, and glutamate mediated stage-3 waves in the second postnatal week -- up until eye opening","To date, stage 3 waves had not been measured and compared to stage 2 waves in vivo throughout the developing visual system"],approach:["Label pre- and post-synaptic populations (sometimes retina/SC, sometimes retina/LGN, etc) with genetically-encoded RCaMP and GCaMP, then perform simultaneous imaging sensitive to each fluorophore independently. This allows the experimenters to image different populations in vivo in the same field of view.",'Measure spontaneous activity and describe the "transfer" of activity between regions using a regression model. The model simply predicts activity in the postsynaptic region as a weighted sum of the current post-synaptic activity and the incoming presynaptic activity',"The authors use both genetic knockouts and injection of activity-blocking molecules to determine the necessity of different receptor types for the described transfer functions"],results:["Activity in the superior colliculus can be predicted from retinal activity in both stage 2 and stage 3 waves, unless waves in the retina are blocked by knockout of the nicotinic acetylcholine receptor (nAChR) (Figure 2)","Stage 3 waves measured in the superior colliculus are shorter and more punctate than stage 2 waves, but occur with higher frequency. Stage 3 waves also preferentially travel in the rostral-to-caudal direction (Figure 3)","The contribution of pre-synaptic (LGN) activity to post-synaptic (V1) activity is higher in stage 2 waves than in stage 3 waves (Figure 4)","While transfer from retina -> LGN was fairly robust throughout the first two postnatal weeks, the transfer from retina to V1 was more apparent in stage 2 than stage 3 (Figure 5)"],conclusions:["Stage 3 waves differ from stage 2 waves in vivo (in mice), leading V1 to become relatively independent of retinal activity in the second postnatal week. This independence may reflect the emergence of competing intracortical activity in V1 that overrides the retinal signals.","The methods introduced here can be broadly applicable to simultaneous imaging of two spatially-intermingled neuronal populations in-vivo"],other:["This was a very enjoyable read and the methods introduced here are awesome! One thing that always strikes me about retinal wave research is how intricate the progression of waves (stages 1 -> 2 -> 3) are, yet how simple the supposed results are (slightly lower retinotopic precision for an animal with already-poor visual acuity). I wonder if these new methods can somehow be used to make predictions about either V1 activity or animal behavior following this early developmental period. Can we predict those outcomes based on either retinal wave activity or the inferred transfer functions between regions? What about under all of these knockout and inhibition conditions?"]},metadata:{authors:[" Gribizis, A"," Ge, X"," Daigle, TL"," Ackman, JB"," Zeng, H"," Lee, D"," Crair, MC"],institutions:["Yale University","Allen Institute For Brain Science","University of California, Santa Cruz"],keywords:["mouse","v1","retinal waves","calcium imaging"],title:"Visual Cortex Gains Independence from Peripheral Drive before Eye Opening",journal:"Neuron",doi:"10.1016/j.neuron.2019.08.015",url:"https://www.cell.com/neuron/fulltext/S0896-6273(19)30699-3",date:"2019-08",review_date:"2019-12-07",one_sentence:"Gribizis and colleagues develop a novel technique to simultaneously image pre- and post-synaptic contributions to retinal wave propagation and find that stage 3 retinal waves differ substantially from stage 2 waves in vivo"}}},158:function(e){e.exports={metadata:{title:"The functional architecture of the ventral temporal cortex and its role in categorization",authors:["Grill-Spector, K","Weiner, KS"],institutions:["Stanford University"],date:"2014-08",journal:"Nat. Rev. Neurosci.",doi:"10.1038/nrn3747",url:"https://www.nature.com/articles/nrn3747",keywords:["VTC","human","higher visual cortex","functional architecture"],review_date:"2019-05-10",one_sentence:"Grill-Spector and Weiner review evidence for the central role of VTC in object categorization"},review:{summary:["VTC is organized in a nested spatial hierarchy, such that coarse object distinctions are represented at coarse spatial scales and fine-scale distinctions are represented at fine spatial scales."],background:["The ventral visual stream (V1, V2, V3, V4, IT) is involved in visual perception and the recognition of objects","Neurons are VTC are not arranged randomly, but instead organize into reliable topologies"],approach:["Most results reported use either fMRI (human and non-primate) or electrophysiology (non-human primate)"],results:["VTC is bounded by a set of anatomical landmarks: from the ptCoS to the anterior tip of the MFS along the A-P axis, and by the OTS and CoS on either side (Box 1)","VTC representations support categorization, but with some degree of tolerance for identity-preserving transformations. That is, generalization with specificity (Figures 1, 2)","Neurons selective for different ecological object categories cluster in higher visual cortex (Figure 3)","The MFS marks transitions in anatomical and function properties. Within each half of VTC (split at the MFS) there is a nested spatial hierarchy of object information (Figures 4, 5)"],conclusions:["The functional architecture of VTC, and in particular, the superimposition of representations at different semantic scales, allows information to both converge and diverge through different routes","This flexibility allows rapid and reliable categorization, but the exact implementation details of how representations are superimposed remains an open question"],other:[]}}},159:function(e){e.exports={metadata:{title:"Learning Divisive Normalization in Primary Visual Cortex",authors:["Gunthner, MF","Cadena, SA","Denfield, GH","Walker, EY","Tolias, AS","Bethge, M","Ecker, AS"],institutions:["University of Tubingen","Bernstein Center for Computational Neuroscience","Baylor College of Medicine","Rice University"],journal:"bioRxiv",doi:"10.1101/767285",url:"http://dx.doi.org/10.1101/767285",date:"2019-09",review_date:"2019-10-02",one_sentence:"Gunthner and colleagues demonstrate that adding learned divisive normalization to a model of primate primary visual cortex offers a parameter-efficient improvement over basic linear-nonlinear models, but still falls short of 'black box' deep neural networks",keywords:["macaque","v1","divisive normalization","deep neural network","orientation"]},review:{summary:["Adding divisive normalization to a neural network model of V1 firing rates in response to natural images closes most of the gap between a linear-nonlinear subunit model and a black-box CNN without relying on too many additional parameters.","The learned normalization weights suggest that normalization may be strongest between units with similar orientation tuning preferences."],background:['Divisive normalization (DN) is a nonlinear computation thought to be performed in primary visual cortex, whereby a neuron\'s output is normalized (likely by lateral inhibitory connections) by the output magnitude of its neighbors in cortex. Traditionally, DN has been thought to be non-specific; that is, neurons can be normalized by a "pool" of units that are selected without consideration of their response patterns/stimulus selectivity/etc',"Recently, a deep neural network model has been introduced as the state-of-the-art model of macaque V1 spiking rates (Cadena et al., 2019), but that model does not include any computation resembling DN"],approach:['Build a neural-network model in which input images are passed through a divisive normalization "core" that consists of learned filters and learned normalization weights. The output of this model is then subjected to a linear readout mechanism which has been discussed at length in another publication (Klindt et al., 2017), but which boils down to learning weights on model output channels and receptive field positions separately.',"Present black and white natural images spanning 2 degrees of visual input and record firing rates from macaque V1 neurons with silicon probes spanning multiple cortical layers. Use firing rates to train model weights. EM note: is it possible that one would learn different schemes of divisive normalization for A) different images or B) different recording locations?"],results:["A model with nonspecific divisive normalization (same normalization weights on all channels) significantly outperforms a simple subunit model. Eshed's note: the difference in terms of fraction of explainable variance (FEV) is actually quite small! But it's inflated by reporting performance relative to a black-box CNN (Figure 3)","A model with specific DN (learnable normalization weights per channel) outperforms the non-specific DN model by 0.6% FEV. (Figure 3)","The divisive normalization model has way fewer parameters than the full CNN: 6.5k parameters vs. 24k parameters (Table 1). EM note: Given that the relationship between number of parameters and FEV is non-linear, is it possible that a parameter-matched black-box CNN could be just as good as the DN model? I doubt it, but it would be a nice control to see.","When learning specific DN, it seems that features with similar orientation tuning have stronger weights on each other than features with dissimilar orientation tuning (Figures 5 and 6). Not sure what happens here with the filters learned that appear to have no orientation preference... are they ignored in this analysis?","As control analyses, the authors show that readout weights are not concentrated on only a handful of channels (Figure 7) and that effects of the size of the normalization pool support using a pool that is roughly matched to receptive field sizes (Figure 8). They note that a limitation of the dataset is that images were only shown to macaques in an area spanning twice the size of the estimated receptive field."],conclusions:["Divisive normalization is a compact (with respect to number of parameters) computation that helps fit neural data with learned convolutional features. ","This model suggests that divisive normalization may be stronger for units with similar orientation tuning, an idea that has had weak experimental support to date."],other:["The big question here lies in the tradeoff between parameters and FEV. How impressed are we by the parameter savings with respect to the hit in the model's ability to explain brain data?","How does this framework extend beyond V1? If we use a more general notion of tuning similarity (correlations of unit outputs instead of orientation preference estimates from the learned filters, for example), can we extend this mechanism to multiple stages in a deeper model that fits later brain areas well?"]}}},160:function(e){e.exports={metadata:{title:"A common, high-dimensional model of the representational space in human ventral temporal cortex",authors:["Haxby, JV","Guntupalli, JS","Connolly, AC","Halchenko, YO","Conroy, BR","Gobbini, MI","Hanke, M","Ramadge, PJ"],institutions:["Dartmouth College","Universita degli studi di Trento","Princeton University","Universita di Bologna"],date:"2011-10",journal:"Neuron",doi:"10.1016/j.neuron.2011.08.026",url:"https://www.ncbi.nlm.nih.gov/pubmed/22017997",keywords:["human","fMRI","MVPA","hyperalignment","VTC"],review_date:"2019-07-16",one_sentence:"Haxby and colleagues introduce a method for aligning brain data between subjects that outperforms anatomical alignment in between-subjects classification"},review:{summary:["Individual brain data can be mapped into a high-dimensional space that is common to all subjects. This space can be used to achieve very high between-subject's classification that does not require anatomical alignment"],background:["Multi-variate pattern analyses are widespread in human neuroimaging. In many studies, classifiers are built to predict a variable of interest from the distributed pattern of activity in a single brain area. But because each brain is different, this requires a new representational space (defined by a set of voxels) in each subject, making intersubject comparison tricky","Getting around this problem has been approach in a couple of ways: 1) the anatomy is abstracted away in approaches like representational similarity analysis (RSA), or 2) individual anatomies are warped into a space common to all subjects"],approach:["3 experiments: In E1, 21 subject watched Raiders of the Los Ark. In E2, 10 of those 21 also viewed images of object categories, and in E3 11 subjects viewed images of animal species.","Hyperaligment in E1 begins with 42 matrices (21 subjects, left and right VTC). Each matrix is 500 voxels x 2205 time points. A procedure based on iterative Procrustean transformation (rigid transforms) are performed to bring every subject into a common 1000-D space (2 hemispheres x 500 dimensions x 2205 time points). Finally, those 1000-D vectors are reduced to 35 dimensions via PCA","Perform between-subject classification to determine the validity of the common space. Predict one subject's brain activity from the other 20. The between-subject performance is then compared to both within-subject performance and Talairach-space-aligned brains"],results:["Hyperaligned between-subjects classification of which portion (18 seconds) of the movie was being seen at a given time was at 70.6%, far higher than the anatomically-aligned classifier at 32.0%. (Figure 2)","In E2, hyperalignment performs at 63.9%, which was significantly higher than the 44.6% afforded by anatomical alignment. For comparison, within-subjects classification also achieves 63% (Figure 2) In E3, hyperalignment also performs much better than anatomical alignment (Figure 2)","Classification accuracy improves up to about 20 PCs (from the original 1000), but somewhat plateaus above that. It only takes about 12 PCs to accurately decode the faces and objects, and only 9 to decode the animal species (Figure 3)","Building the common space with complex stimuli (the movie) is generally effective regardless of the 'test' data, but creating the common space based on simple or narrow datasets does not generalize well to complex stimuli (Figure 4)","The first few principal components of the 35D space don't map onto hand-drawn ROIs (e.g., FFA; Figure 5) very well, but specifically searching for dimensions that have a high faces vs. places contrast aligns very well with hand-drawn ROIs (Figure 6)"],conclusions:["Hyperalignment is useful for estimating the dimensionality of human VTC as well as predicting brain activity in held out subjects. The validity of this common space depends on acquiring rich, complex data"],other:["See Guntupalli et al., 2016 (Cereb. Cortex) for an application and extension of hyperalignment","I haven't seen this method get used as often as anatomical alignment techniques. Is that because acquiring a rich enough dataset is difficult?"]}}},161:function(e){e.exports={review:{summary:["Curvature domains exist in macaque V4, are tuned to specific orientations, are cue-invariant, and are distinct from color and spatial frequency domains"],background:['In the early visual system in macaques, there are orientation domains in V1 and "higher order orientation domains" in V2: regions that preferentially respond to orientations irrespective of the type of cue (luminance, depth, etc)',"In V4, neurons have been shown to be selective for curvature, degree of curvature, and orientation of a curve. However, it is unknown how these responses are spatially organized."],approach:["Intrinsic optical imaging in macaque V1, V2, and V4. Map orientation, spatial frequency, and color tuning with a set of gratings, then evaluate curvature sensitivity with a set of gratings curved either up, down, left, or right.","Functional domains are identified by computing pixel-by-pixel t values.","In some experiments, different kinds of curved stimuli were used: gratings vs. straight lines, and finer gradataions of orientation (8 instead of 4)."],results:["Curvature-preferring domains were observed in V4; they are non-overlapping with color-preferring domains and spatial-frequency-tuned domains, but overlap substantially with regions selective for straight gratings. (Figure 1)",'Preference for curves was "cue-invariant": regions that preferred curved gratings to straight gratings also preferred curved single lines to straight single lines (Figure 1)',"Response maps to similar degrees of curvature were correlated (Figure 2), and curvature domains exhibit subdomains tuned to higher or lower curvature (suggests smooth mapping on the cortical surface of the degree of curvature; Figure 3)","There are systematic spatial shifts from domains preferring straight lines to increasingly high curvature preference (Figure 6). The preferred orientation of the curved stimuli forms a pinwheel reminscent of those observed in V1 for straight orientations (Figure 6). Eshed's note: not sure how convinced I am of the pinwheel claim, these look about as noisy as when you average a bunch of noise with small biases together."],conclusions:["Curvature domains exist in macaque V4 and are distinct from high-spatial-frequency domains, indicating a novel component to the functional architecture of this region",'The existence of curvature domains in V4 points to a "third stage" of the shape processing system, following that of orientation representations in V2 and V4'],other:['Beyond the demonstration of curvature domains, this study is also a nice overview of our existing knowledge of V4 functional architecture. It\'s also interesting (for me, anyway) to consider the implications of these findings with respect to the search for general principles of functional organization that might extend from early visual cortex to "intermediate" visual areas and beyond']},metadata:{authors:["Hu, J","Song, XM","Wang, Q","Roe, AW"],institutions:["Zhejiang University","Oregon Health Sciences University"],keywords:["macaque","v4","orientation","curvature","pinwheel","maps"],title:"Curvature domains in V4 of Macaque Monkey",journal:"bioRxiv",doi:"",url:"https://www.biorxiv.org/content/10.1101/2020.02.28.969329v1.full",date:"2020-02",review_date:"2020-03-11",one_sentence:"Curvature domains exist in macaque V4 and form curvature-direction pinwheels",updatedAt:{$date:{$numberLong:"1583954740475"}}}}},162:function(e){e.exports={metadata:{title:"Receptive Fields and Functional Architecture in Two Nonstriate Visual Areas (18 and 19) of the Cat",authors:["Hubel, DH","Wiesel, TN"],institutions:["Harvard"],date:"1965-03",journal:"J. Neurophysiol.",doi:"10.1152/jn.1965.28.2.229",url:"http://dx.doi.org/10.1152/jn.1965.28.2.229",keywords:["cat","V2","V3","complex","hypercomplex"],review_date:"2019-04-11",one_sentence:"Hubel and Wiesel's discovery of hypercomplex cells in cat V2 and V3 fuels a tradition of hand-picking stimuli that might cause cells to spike"},review:{summary:["V2 and V3 are topographically arranged in the cat brain and exhibit columnar organization","Hypercomplex cells, which are typically end-stopped, compose 10% of cells in V2 and about 50% in V3","V2 and V3 align with regions defined by microscopic anatomy and receive projections from V1"],background:["In H&W's 1962 exploration of V1, simple cells were established by their sensitivity to bar orientation, spatial frequency, and position. Complex cells additionally have position invariance and are found co-columnar with simple cells of the same orientation and spatial frequency preference","Cytoarchitectonically distinct regions 18 and 19 had been established in the cat visual cortex, but the functional nature of these cells was unkonwn"],approach:["Microelectrode penetrations in 17 cats in and around areas 17, 18, and 19","Pick some stimuli to show to the cell, try to identify its preferred stimulus","Histology to see where the cells were in the cortical sheet"],results:["V2 and V3 have a topographic representation of visual space (Figures 1-3)","Transitions between areas identified functionally coincide with boundaries seen in Nissl and myelin stains (very unclear to the untrained eye; Figure 3)","Both V2 and V3 have hypercomplex cells, which prefer bars or slits of light (or dark) within the 'activating region' of the receptive field, but decrease activity if the light (or dark) extends into the 'antagonistic region' (Figures 8-22)","Some hypercomplex cells are invariant to 90 degree rotations, and both orientations are found in the preferences of cells in the same column (Figure 30). These cells are given the designation 'higher-order'","Orientation tuning changes gradually parallel to the cortical surface and is stable perpendicular to the cortical surface in both V2 and V3 (Figure 32)","Puncturing V1 with a 25 gauge needle and tracing degenerating axons shows that V1 projects to discrete parts of V2, V3, and the suprasylvian gyrus"],conclusions:["Much as simple cells are thought to form from LGN projections into layer 4C of V1 and complex cells are thought to form from simple cell projections within a V1 column, hypercomplex cells might form from a combination of excitatory and inhibitory projections onto cells in V2 and V3","Higher-order hypercomplex cells are a neat example of a transformation that both increases generalization and specificity","These experiments are a far way off from explaining perception, but they are a start"],other:["On the term 'hypercomplex', they write 'The word hypercomplex, while ugly and  unwieldy, and perhaps also unwise in view of the  possibility of finding even more complex cells,  is  at least relatively neutral.'","While the results have been undoubtedly impactful, I have two main criticisms of this work, although both may be artifacts of their time. First, there is tremendous bias in the neurons chosen to record from and the images selected by the authors to present to the animals. Second, these results have not propagated very far in the modern visual neuroscience discourse -- the term hypercomplex is rarely used and no longer taught in many introductory courses. Do characterizations of this nature still have value?"]}}},163:function(e){e.exports={metadata:{title:" Computational Consequences of a Bias toward Short Connections",authors:["Jacobs, RA","Jordan, MI"],institutions:["MIT"],date:"1992-09",journal:"Journal of Cognitive Neuroscience",doi:"10.1162/jocn.1992.4.4.323 ",url:"https://www.mitpressjournals.org/doi/abs/10.1162/jocn.1992.4.4.323",keywords:["neural network","wiring cost","spatial"],review_date:"2019-06-19",one_sentence:"Jacobs and Jordan show that simple neural networks optimized for task and a 'short connection bias' allows the network to learn multiple tasks in a spatially-efficient way"},review:{summary:["Short connections obtained via pruning are sufficient to explain modularity in a variety of computational settings","The emergence of topographic maps may be explained by a realtively simple supervised cost that favors short connections"],background:["Whether the apparent modularity of the brain is genetic or learned is a matter of debate among neuroscientists","Modularity is efficient from a space, energy, and multi-task learning perspective; that is, it is one way to avoid 'catastrophic forgetting'","Much work on modularity occurs in an unsupervised context, e.g., Kohonen maps that represent inputs to the system topographically. Supervised approaches to understanding modularity are more rare","Modularity could arise either by selective addition of connections, or selective pruning of connections"],approach:["Assign neural network units positions in a 3D space, such that the distance between any pair of units can be evaluated","Train relatively simple (my modern standards) neural networks to perform tasks while adhering to a spatial constraint. The network can learn to set some weights to zero, akin to the pruning hypothesis for modularity","The spatial constraint used is for a pair of units is d * w ** 2 / (1 + w ** 2), where d is the distance between them and w is the weight between them","Evaluate the 'spatial' structure of network weights following optimization"],results:["For a network trained on a simulataneous 'what/where' task (identifying or locating a pattern), half the weights are used for the first task and half are used for the second task (Figures 5, 6). If the output nodes for the two tasks are brought closer together, there is more spatial continuity to the structure of the non-zero weights (Figure 7)","In a temporal shift detection task, units in adjacent layers form short local loops (as found in cortex; Figure 9)","In a dimensionality reduction task, hidden layers units form a topographic representation of the input, serving as a supervised analogue to the unsupervised methods of Kohonen or Durbin/Mitchison"],conclusions:["This work supports the framework that cortex is initially homogenous, and that a learning rule that favors short local connections can explain the emergence of modularity. Of course, that hypothesis neglects the tremendous genetic variability in the developing brain, but is nevertheless useful in thinking about neurodevelopment","That all connections are initialized as non-zero here, and some are learned to be zero, is taken as evidence that elimination of synapses may be the dominant mechanism in pruning that contributes to modularity","Whether systems start with long connections and prune down or have many short connections that are pruned reamins an open question"],other:["'If biological neural systems are biased to use short connections, then it is important to know how neurons determine their relative positions.' Very true! This is likely genetic, but may also be instructed in some way by spontaneous activity preceding eye opening","How would the supervised loss (applied via backprop) actually be biologicall instantiated?"]}}},164:function(e){e.exports={metadata:{title:"Classification of columnar and salt-and-pepper organization in mammalian visual cortex",authors:["Jang, J","Song, M","Paik, S-B"],institutions:["Korea Advanced Institute of Science and Technology"],date:"2019-07",journal:"biorXiv",doi:"10.1101/698043",url:"https://www.biorxiv.org/content/10.1101/698043v1",keywords:["orientation","retina","Moire","V1","Nyquist","pinwheel"],review_date:"2019-08-10",one_sentence:"Jang et al., demonstrate that whether an animal's V1 is organized in a columnar or salt-and-pepper pattern can be predicted empirically by the ratio of retinal neurons to V1 neurons and theoretically by the Nyquist sampling ratio"},review:{summary:["While the presence of orientation columns or salt-and-pepper tuning cannot be predicted from a single parameter, e.g., V1 size, it can be predicted by the combination of V1 size and retina size","The retina/V1 size ratio that separates columnar from salt-and-pepper species is predicted by the Nyquist sampling theorem"],background:["While neurons in V1 are tuned to specific orientations in many species, the organization of such orientation-selective neurons varies both parallel and perpendicular to the cortical sheet. The organization parallel to the cortical sheet has been the subject of intense study, as in some (but not all) mammals the neurons are organized into columns that change smoothly in their orientation preference","A somewhat common viewpoint is that the difference between species emerges not from species-specific developmental mechanisms, but from a universal developmental mechanism that acts on differences in particular parameters between species","To date, V1 size, retina size, and orientation tuning spatial structure have been thoroughly described in eight mammalian species: Tree shrew [C(olumnar)], Ferret [C], Cat [C], Macaque [C], Mouse [S(alt and ) P(epper)], Rat [SP], Squirrel [SP], and Rabbit [SP]"],approach:["Modeling of retinal ganglion cell (RGC) connectivity to V1 by assuming Gaussian receptive fields, then computing orientation tuning of model V1 neurons (Extended Data Figure 2)","The specific model used here is that of retinal mosaics in a Moire interference pattern (see Paik/Ringach '11 for details). This model uses the observed offsets in positions of ON/OFF RGC feedforward afferent pairs to V1 to predict the resulting orientation tuning in the retinotopically matched V1 region to which they project.","Comparison to 1) measured V1 size, retinal area, and number of RGCs in each animal species and 2) orientation tuning patterns observed (or not observed) in each animal"],results:["The presence of columnar organization cannot be well predicted by V1 acuity, body weight, retina size, or V1 size (Eshed's note: you can come very close to a clean separation with V1 size alone, suggesting that projecting onto an additional dimension that is even slightly independent will allow perfect classification. This seems like an area where more data from other species would be very helpful; Figure 1)","Classifying species in a 2-D space defined jointly by V1 and retina size allows for perfect separation, whether going by surface area or by estimated number of neurons (Figure 1)","Assuming the same exact retinal mosaic and varying only the ratio of V1 to RGC cells, the resulting orientation maps can shift from columnar to salt-and-pepper (Figure 2). Values near those recorded for the cat (V1:RGC = 2.5) yield columnar structure, while those recorded for the mouse (V1:RGC = 0.1) yield salt-and-pepper structure. Eshed's note: kind of? The salt-and-pepper map is so sparse that even if it were columnar it would be hard to tell.","Because a V1:RGC ratio of less than 2 might be susceptible to sample aliasing by the Nyquist Sampling Theorem, i.e., the sampling of orientations from the full spectrum might appear lower frequency than it really is, the authors predict and show that there is a phase transition between columnar and salt-and-pepper maps at a sampling ratio of 2. Indeed, this value separates the 6 species for which the data are fully available into their correct pattern groups (columnar or not). (Figure 2)"],conclusions:["Columnar and salt-and-pepper organizations, while obviously different, might share a common developmental mechanism and act on differences in the V1:RGC sampling ratio","There are many differences between, e.g., mice and cats, including RGC types and convergence ratios from RGCs to the LGN neurons. However, the authors here argue that those results are dwarfed by the similarities in ON/OFF afferents across species.","Eshed's note: in general I support the idea that common developmental mechanism act on different substrates, and I think the appeal to alias sampling theory is elegant here. However, it appears that a more rigorous match to specific patterns, e.g., pinwheel density, is lacking."],other:["This paper is the latest in an installment of arguments between two groups: the Paik/Ringach group (this paper) and the Wolf/Kaschube group, who argue that this retinal mosaic model fails to capture the correct statistics of V1 in four divergent species and ignores large-scale cortical organization principles in favor of randomness. See Kaschube et al., 2010, Schottdorf et al., 2014, 2015 for the Wolf/Kaschube perspective, and this paper + Paik & Ringach, 2011, for more."]}}},165:function(e){e.exports={metadata:{title:"Deep supervised, but not unsupervised, models may explain IT cortical representation",authors:["Khaligh-Razavi S-M","Kriegeskorte N"],institutions:["Medical Research Council, Cognition and Brain Sciences Unit"],journal:"PLOS Computational Biology",doi:"10.1371/journal.pcbi.1003915",url:"http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003915",date:"2014-11",review_date:"2019-09-25",one_sentence:"Khaligh-Razavi and Kriegeskorte compare 37 computer vision models to the representational geometry of human and macaque IT, and find that only features from an ImageNet trained neural network (with recombination of features) was able to explain the data",keywords:["macaque","human","rsa","deep neural network","computer vision"]},review:{summary:["Not-strongly-supervised models (those with no or relatively little supervision on object tasks) do not explain IT geometry even after feature reweighting and remixing","Features from a deep convolutional neural network are able to explain the representational geometry, but only after reweighting and remixing of features"],background:["Yamins et al., 2014 demonstrated that learning a linear mapping from object-recognition-trained neural networks to IT neurons beats a linear mapping from any other computer vision model. However, this approach requires a large amount of high-quality data","Representational similarity analysis (RSA) allows an alternative method to matching feature spaces to brain data in which the representational geometry of a stimulus set can be compared","There is an open question as to whether the response patterns in IT can be explained by an intelligently-selected feature space, or if supervised learning of massive neural networks is the only approach that yields a good fit to biological measurements"],approach:["Datasets include monkey electrophysiology from Kiani et al., 2007 and human fMRI data Kriegeskorte et al., 2008. In both cases, RDMs are constructed","37 candidate computer vision models are tested, ranging from V1-like models, to HMAX, to deep convnets. A variety of approaches to re-weighting the feature spaces produced by these models (stretching the feature space toward some end) or remixing the feature spaces (finding new linear combinations, i.e., affine transformations of the space) are performed","Kendall's Tau is used to compare feature spaces to biology. Furthermore, an interesting approach is introduced in which model RDMs are weighted in a least-squares regression to explain the observed RDM",'In this context, "remixing" features means using SVM decision boundaries on three tasks: body vs nonbody, face vs. nonface, and animate vs. inanimate as new features and constructing RDMs from those. One expects that if the representation is able to perform these three tasks well, then these remixed features will match the IT representation (which we know is strongly categorical; Figure 4)'],results:['Of the not-strongly-supervised models (those without many labeled images used while learning the representation), most have only a small amount of similarity to human and macaque IT and do not show much categorality. A combination via feature concatenation of all of these models, termed "combi27", is the closest match to biological data but still leaves a substantial gap to be explained (Figures 1, 2, 3, 4)',"Remixing or reweighting of the not-strongly-supervised models does not close the gap to biological data, presumably because their representations are not categorical in the way IT is (Figure 5)","Alexnet (trained on Imagenet) matches IT much better than the not-strongly-supervised models, but still requires feature remixing and reweighting to completely close the gap (Figures 6, 7, 8, 9, 10)"],conclusions:["Strong supervision is required to build a representation that matches that in human and macaque IT. However, strong supervision may not be enough: the categorical boundaries that are important to IT, e.g., animate vs. inanimate, need to be weighted more heavily than other boundaries.",'This work raises an interesting question regarding the definition of visual object "categories", especially given that low-level differences explicitly available in, e.g., the retina, often correlate with semantic labels. They write that "a representation is \'categorical\' when it affords better category discriminability than any feature set that can be learned without category supervision, i.e., when it is designed to emphasize category divisions." This is an interesting, but perhaps nonstandard definition in my opinion.'],other:["Compared to direct linear fits, the RSA approach seems to be less computationally intensive, but also requires complex post-hoc manipulation of features, e.g. constructing a new, warped feature space from SVMs fit on separate tasks. In practice I assume that the choice of methods (RSA vs. direct fits) likely doesn't matter all that much"]}}},166:function(e){e.exports={metadata:{title:"Recurrence is required to capture the representational dynamics of the human visual system",authors:["Kietzmann, TC","Spoerer, CJ","S\xf6rensen, LKA","Cichy, RM","Hauk, O","Kriegeskorte, N"],institutions:["MRC Cognition and Brain Sciences Unit, University of Cambridge","Donders Institute for Brain, Cognition, and Behavior, Radboud University","University of Amsterdam","Freie Universitat Berlin","Columbia University"],journal:"PNAS",doi:"10.1073/pnas.1905544116",url:"https://www.pnas.org/cgi/doi/10.1073/pnas.1905544116",date:"2019-10",review_date:"2019-10-09",one_sentence:"Kietzmann et al. use source-localized MEG to argue that recurrent connections (and recurrent neural network models) are needed to explain the dynamics of object representations in human visual cortex",keywords:["human","recurrence","feedback","RSA","deep neural network"]},review:{summary:["The temporal dynamics of object representations throughout the ventral visual stream suggest that solely feedforward processing is unlikely","Deep neural networks trained to predict time-varying brain data do a better job when recurrence is built into the architecture, even when matching for overall number of parameters"],background:["Visual cortical regions are densely interconnected, yet most research treats the system in feedforward. In large part that's because much of 'core object recognition' can be explained that way, but vision scientists have suspected that recurrence and feedback are important for predicting brain measurements.","Most neural network models used to explain brain data are strictly feedforward, lacking recurrence. Note, however, recent work from the DiCarlo and Yamins groups demonstrating the importance of recurrence in explaining neuronal dynamics."],approach:["Record MEG data from 15 participants as they view many repetitions of 92 object stimuli","Create model RDMs (e.g., human faces on, everything else off) for those stimuli that make predictions about what structure may be there. Note: this procedure seems fairly important but is somewhat glossed over-- the choice of which model RDMs one uses aren't just important to the analysis, but to the entire presentation of results! In the methods, they clarify that '4 main and 10 additional control predictors were investigated', but an explanation for how those 14 are chosen and which are 'main' and which are 'control' is not provided. In a longer publication I would love to see more discussion of how these results compare to other reasonable choices of model RDM components","Evaluate contribution of each RDM component over time in each of three ROIs: early visual cortex, V4/LO, and IT/PHC. Another note: how much do these results depend on this grouping? What if V4 goes with V1-V3, and PHC isn't included with IT? Of note here is that the authors justify this selection by attempting to pick spatially distinct ROIs (to limit crosstalk) that are large (to maximize SNR)","Use Granger causality to infer directions of feedforward and feedback projections","Train two deep neural network classes to predict the MEG data directly: feedforward models with temporal ramping, and recurrent models, then test on same images humans saw and evaluate quality of fit"],results:["RDMs vary across ROIs and timepoints, suggesting that there are, in fact, dynamics to explain (Figure 1)","Early visual features explain the brain data at early time points, animacy has peaks in different ROIs at different times, face clustering has a well-defined peak, and real-world size has some interesting dynamical structure (Figure 1)","Causality analysis suggests that feedback from IT to V4 peaks at two distinct later time points, whereas feedforward contributions are early. Early visual cortex to V4 connections (feedforward and feedback) seem pretty well-aligned in time, with feedforward leading feedback by a bit (Figure 2)","Deep neural networks with recurrence exhibit temporal dynamics in how well they fit the data, but the peaks appear fairly consistent regardless of ROI (about 0.15s after stimulus presentation; Figure 3)","Recurrent DNNs outperform feedforwawrd alternatives in all ROIs (Figure 3)","Virtual 'cooling' (increasing dropout probability) suggests that the lateral and top-down connections in the recurrent models are actually important for explaining data in V4/LO and IT/PHC, but not in V1-V3 (Figure 4)"],conclusions:["Human ventral stream dyanmics likely arise from recurrence, which may be important for recognizing objects under difficult conditions (like occlusion)","We should probably use recurrent DNNs to explain brain data"],other:["The data appear very high-quality and the modeling results are clear, but I'm left wondering to what degree this is a fishing expedition. For instance, reasonable people may disagree or craft different stories about what they're seeing in the dynamics, and the choice of model RDM components matters tremendously to how that story looks! Nevertheless, I don't have specific gripes with the choices here. They seem reasonable and are well-grounded in prior work (e.g., animacy and human-face representations are RDM components most people in this field would likely choose)","The thrust of the argument here appears to be that the dynamics are difficult to explain without recurrence between the regions specified here. I think I agree, but it'd be interesting to hear if there are other strong alternative models that could explain what's happening. Are there candidate regions that could influence timecourses? To what extent does adaptation or decay explain some bit of these results? It would be unreasonable for the authors to completely explore that space, but if the data are made available it'd be fun to play with different architectures and alternative models","The use of an objective weight schedule here (Figure 3A, inset) is pretty interesting, I'd personally want to know why this one works, i.e., how alternative obejctive schedules fail. All that's provided here is that the value of 10:1 (category: neural fit) was chosen arbitrarily, which seem splausible"]}}},167:function(e){e.exports={metadata:{title:"Neural Coding for Shape and Texture in Macaque Area V4",authors:["Kim, T","Blair, W","Pasupathy, A"],institutions:["University of Washington"],date:"2019-06",journal:"J. Neurosci",doi:"10.1523/JNEUROSCI.3073-18.2019",url:"http://www.jneurosci.org/content/jneuro/39/24/4760.full.pdf",keywords:["macaque","V4","shape","texture"],review_date:"2019-07-08",one_sentence:"Kim et al. demonstrate that V4 neurons encode both an object's shape and its texture, with stronger selectivity and faster responses on average for shape stimuli"},review:{summary:["Individual V4 neurons lie on a continuum between strongly shape-selective and strongly texture-selective","The populations are thus 'partially overlapping'. Indeed, individual neurons that are strongly selective for shape can be observed"],background:["V4 neurons have been found to be selective for shape and surface properties such as color and brightness, but less attention has been given to texture","Adelson, Movshon, Simoncelli, and others argue that early/intermediate visual areas are responsible for the representation of 'stuff' instead of 'things'","It is unknown how V4 accomplishes the joint representation of shape, color, texture, and more. It could be performed by groups of neurons, each tackling part of the problem, or it could be the case that individual neurons jointly encode multiple dimensions of stimuli"],approach:["Single unit recordings (N=101) from V4 in two macaques. Grayscale stimuli presented at two sizes: entirely within unit's RF or at twice the RF diameter","Stimuli were drawn from 225 shapes (from prior work) or 168 textures defined by three axes: directional/nondirectional, regular/irregular, and coarse/fine"],results:["52 of the 101 neurons had a higher peak response for shapes than peak response for texture (after controlling for number of stimuli in each set), whereas only 19 neurons had a stronger peak response to texture (Figure 2)","HMax can predict neuron responses to shape stimuli, but not texture stimuli (Figure 3)","Tuning for shape and texture in individual neurons (note, this is a subset of the original 101 neurons) is largely independent (Figure 7)","The onset of selectivity was significantly later for texture stimuli than shape stimuli (Figure 8)"],conclusions:["Given that shape generally yielded stronger and faster selectivity than texture is consistent with frameworks suggesting that it is an object's outline that determines our percept (see the work of I. Biederman), instead of texture","Notably, recent work (Geirhos et al., 2018) has shown that leading neural network models default to using texture information, indicating a disconnect between artificial and biological vision systems","We shouldn't completely ignore texture, since a substantial portion of V4 neurons do encode texture along seemingly relevant dimensions"],other:["Given my personal interest in functional architecture, it would have been great to see where these units were and how their responses correlated with their cortical positions. Are functionally overlapping populations also spatially overlapping?"]}}},168:function(e){e.exports={metadata:{title:"Functional implications of orientation maps in primary visual cortex",authors:["Koch, E","Jin, J","Alonso, JM","Zaidi, Q"],institutions:["State University of New York"],date:"2016-11",journal:"Nature Communications",doi:"10.1038/ncomms13529",url:"https://www.ncbi.nlm.nih.gov/pubmed/27876796",keywords:["pinwheel","orientation","V1","cat","BA17","contrast"],review_date:"2019-02-28",one_sentence:"This work establishes the functional properties of pinwheel-center neurons in cat V1 and proposes that these neurons may be useful for recognition of multi-orientation textures"},review:{summary:["Neurons in orientation pinwheels are more broadly tuned to orientations and do not saturate as quickly with increasing contrast","Neurons in pinwheels are not as strongly suppressed by cross-orientation plaid gratings","Normalizing responses by local divisive normalization cannot account for empirical results, but normalizing by response of local interneurons can","Argues for functional implications of having pinwheels: having neurons be broadly tuned without suppression saturation is useful for recognizing textures composed of multiple orientations"],background:["Pinwheels in orientation-tuning maps are common in many animals but lacking in others. There is no consensus on whether there is functional utility to having orientation maps","Neurons in pinwheel centers are generally less orientation-selective, which may help with processing stimuli composed of many orientations. But! V1 neurons generally exhibit cross-orientation suppression, which would greatly weaken those responses."],approach:["Record from cat V1 using horizontal linear track electrodes separated by 100 microns.","Evaluate neuronal response characteristics for cells in iso-orientation domains and pinwheel centers.","Develop an image-computable model of V1 neurons that can explain the empirical data"],results:["Neurons in iso-orientation domains are strongly orientation selective (Figure 1a), exhibit strong response saturation (Figure 1c; 1g), and have strong cross-orientation suppression (Figures 1-3)","Model of thalamocortical projections and cortical normalization can reproduce empirical results if an excitation-normalization (EN) model (normalization by non-oriented nearby neurons) is used (Figure 4). The model predicts responses in an experiment with novel stimuli and 3 new subjects (cats), Figure 5)","Divisive normalization would yield results opposite the empirical findings unless unbiological assumptions are made re: the origin of inhibitory projections (Figure 6)","Using a horizontal linear track to record (as opposed to a 2D method) leads to similar conclusions re: whether you're in a pinhweel or iso-orientation domain"],conclusions:["Neurons in pinwheels respond differently to inputs and experience different normalization from surround cells. This allows them to represent multi-orientation patterns without suffering from suppression and saturation","Animals without pinwheels (or any orientation map) may sacrifice pattern recognition for robust edge detection","Local divisive normalization does not account for the empirical results, as it would have the effect of broadening tuning curves in iso-orientation domains and narrowing them at pinwheel centers."],other:["Follow-up reading: Levy, M., Lu, Z., Dion, G. & Kara, P. The shape of dendritic arbors in different functional domains of the cortical orientation map. J. Neurosci. 34, 3231\u20133236 (2014)."]}}},169:function(e){e.exports={metadata:{title:"Orientation Preference Patterns in Mammalian Visual Cortex: A Wire Length Minimization Approach",authors:["Koulakov, KA","Chklovskii, DB"],institutions:["Salk Institute","Cold Spring Harbor Laboratory"],date:"2001-02",journal:"Neuron",doi:"",url:"",keywords:["pinwheel","wiring cost","icecube","orientation","V1"],review_date:"2019-03-07",one_sentence:"A simple wiring-cost minimzation model is suggested as the underlying explanation for the appearance of orientation-tuning maps in primary visual cortex"},review:{summary:["Purpose of pinwheels in orientation tuning maps is unknown -- which cost function (task-based, physiological, etc) do they help minimize?"],background:["Self-organizing map models, e.g., Durbin and Mitchison, suggest that pinwheels are a developmental irregularity that occurs if what you're _actually_ trying to do is end up with an Icecube-like structure (repeating bands), see Wolf and Geisel 1998","60% of cortical volume is axons and dendrites (Brait- enberg and Sch\xfcz, 1998), so minimizing neurite length is important for keeping brains small","Most inputs to cortical cells are cortical as opposed to thalamic, so minimizing intracortical wiring length was likely selected for over evolutionary time"],approach:["Model orientation tuning maps by minimizing wiring-cost functions of different forms. Specifically, compute wiring-length after deciding on a neuron-connection function (which neurons connect with which other neurons, given their orientation preference?)","Test which functions give maps consistent with biological findings"],results:["If each neuron connects to an equal number of neurons of all preferred orientations, i.e., connection does not depend on orientation preference, the optimal layout is a salt and pepper pattern as observed in rats (Figure 2a-b)","A wide Gaussian connection function, i.e., generally connected to neurons of like orientations, yields the Icecube layout (Figure 2c-d)","A narrow Gaussian function yields pinwheel-like structures, as neurons need to connect both to like-orientation neurons and cross-orientation neurons (Figure 3)","Periodicity of icecube and pinwheel maps depends on the number of connections each neuron is thought to make-- more connections means a higher frequency pattern","Wire-length minimization applied to direction-preference maps are consistent with empirical findings, suggesting it may apply in that context as well (Figure 5)","Irregularities in maps in other animals can be attributed, in part, to incomplete development due to biological noise, blood vessels, anchoring to other maps, etc, see Figure 6."],conclusions:["Emergence of pinwheels can be explained by a simple wiring-cost minimization paradigm. This same rule can explain the emergence of linear zones (as on the V1/V2 boundary) if one assumes a different connection function for those neurons -- this seems reasonable to me","Lack of structure, as in rat, may also be due to a different connection function for V1 neurons","Coupling with other maps is likely weak, given that the same orientation map is seen in many species, including those without maps like ocular dominance (EM: but lots of evidence for alignment between maps nonetheless)","Continuity-based approaches like self-organizing maps are implicitly enforcing a wiring-cost minimization"],other:["Koulakov and Chklovskii write 'we postpone the treatment of the interaction between different cortical maps until more experimental evidence is available.' -- is there sufficient evidence now?","The authors find these maps by updating one neuron's orientation preference, recomputing connections, then recomputing wiring, which can be really slow-- they write that it takes almost 20 days to optimize a grid of 3600 neurons!"]}}},170:function(e){e.exports={metadata:{title:"Matching categorical object representations in inferior temporal cortex of man and monkey",authors:["Kriegeskorte, N","Mur, M","Ruff, DA","Kiani, R","Bodurka, J","Esteky, H","Tanaka, K","Bandettini, PA"],institutions:["NIH","Mastricht University","University of Washington","Shaheed Beheshti University","Institute for Studies in Theoretical Physics and Mathematics","RIKEN Brain Science Institute"],date:"2008-12",journal:"Neuron",doi:"10.1016/j.neuron.2008.10.043",url:"http://www.sciencedirect.com/science/article/pii/S0896627308009434",keywords:["macaque","human","RSA","MDS"],review_date:"2019-04-30",one_sentence:"Monkey and human representations of objects are strikingly similar despite differences in species and acquisition techniques"},review:{summary:["Images of 92 objects were presented to both human and macaque subjects, and representational distance matrices (RDM) were compared","Both RDMs and multi-dimensional scaling point to striking similarity between representations in the two species"],background:["Macaque is one of the most often used model organisms for human vision, yet direct comparisons of the visual system are mired by difference in neuroanatomy and recording techniques","Some progress has been made by using fMRI in both species, but this doesn't solve the issue of mapping parts of the monkey brain to parts of the human brain"],approach:["In two macaques, record 674 single neurons in anterior IT","In four humans, run a rapid event-related design at a spatial resolution of 1.95 x 1.95 x 2 mm, then define human-IT anatomically","Construct RDMs and MDS embeddings in each species, then compare","Also compare early visual cortex responses in humans to human-IT responses"],results:["RDMs, MDS embeddings, and hierarchical clustering trees look qualitatively similar between monkeys and humans (Figures 1, 2, and 4)","Quantitatively, the similarity of the dissimmilarity vectors across species is fairly high for many different kinds of stimuli. One place this interspecies similarity breaks down is within images of human faces or within images of macaque faces (Figure 3)","Category boundaries are strong in macaque IT and human-IT, but weak in early visual cortex in humans. Still more category structure than expected by chance in V1, but much noisier than in IT. The dissimilarity in early visual cortex is predictive of dissimilarity in higher visual cortex (Figures 5 and 6)"],conclusions:["'[...] even if a fine-grained representation is unique in each individual, like a fingerprint, the region containing the representation may be homologous, like a finger\u2014serving the same function in both species' (p. 1126)","fMRI and single-cell physiology may not be that different after all, at least not at this level of analysis","Exclusion of category-selective regions doesn't really change the representational structure, suggesting that the strong modularity hypothesis is not totally correct","Monkeys are probably a pretty good model system for human object recognition, keep plugging away!"],other:["The fiber-flow diagram in Figure 2 is really cool, I'd be curious to find out how they made it although I'm not entirely convinced it's an intuitive representation of interspecies similarity"]}}},171:function(e){e.exports={review:{summary:["For biologically plausible local learning rules to be competitive with abiological backpropagation, implicit alignment of feedforward and feedback weights is required.","The authors provide a regularization framework that explicitly decouples forward and backward weights, allowing many possible local learning rules to be proposed and tested on performance and generalization."],background:['Modern deep learning relies on backpropagation (BP) as the only learning algorithm that scales to large networks and complex tasks. However, neuroscientists are quick to criticize BP as abiological, as it assumes that the "backward weights" which determine how the error signal should flow through a model unit be the exact transpose of the "forward weights" which determine its response.','Recent work proposed "feedback alignment" (FA) and "weight mirror" (WM), two methods that can be computed locally, i.e., do not require weights to be identical in the forward and backward passes. However, these methods are either fragile or don\'t scale to large-scale tasks such as ImageNet.',"Kunin et al., 2019 demonstrate that L2-regularization of linear autoencoders boils down to enforcing symmetry between forward and backward weights (a way around strict weight transport), indicating a promising direction for regularization-based methods in more complex model architectures and tasks."],approach:['Formulate network training as a regularization problem where 1) there are distinct forward and backward weights for each node, and 2) loss on the task is a function of the "forward weights" and the regularization is computed as a function of the "backward weights".','Build the regularization on the backward weights from a set of primitives that can be sorted into "local" (those which act only on their inputs, e.g., a norm-penalty on the weights, and "non-local" (those which depend on both their inputs and some other set of weights), e.g., alignment of forward and backward weights directly. Previous work can be described with this set of primitives (cool!)',"For a given learning algorithm, evaluate 1) task performance, i.e., ImageNet performance, and 2) Metaparameter robustness, i.e., how well one can use metaparams such as learning rate across different architectures"],results:["The authors are unable to replicate the weight mirror results suggesting competitive performance with backpropagation, even after attempting over 800 combinations of metaparameters to identify the best ones. This approach is also very sensitive to metaparameter choice, as choosing them on one architecture does not transfer to new architectures (Figure 3). A dynamical systems analysis suggests that this sensitivity follows from instability inherent to the WM algorithm, such that metaparameters have to be very specifically selected (Figure 2).","Information Alignment (IA) is a modification to weight mirror that effectively penalizes the sparsity of activations in a layer. This addition prevents the forward and backward weights from diverging in magnitude during training. The authors demonstrate that IA is stable and performant, making it the best known local learning rule (Figure 3). However, it is not quite as good or robust as BP.","Symmetric Alignment and Activation Alignment are non-local learning rules (they do not require weight symmetry, but do violate strict locality of plasticity and computations). They effectively work by encouraging approximate symmetry of forward and backward weights.","The non-local weight estimation methods, e.g., SA are more robust to noisy updates than BP (Figure 5), suggesting that a spike-dependent mechanism previously proposed to allow some form of weight estimation might be plausible at-scale.","Except for feedback alignment, all of the methods described here predict firing rates in macaque V4 and IT equally well, suggesting that there is no signature of the learning rule employed by the brain in the final pattern of activations we can presently measure in experiments."],conclusions:['Strictly local learning rules can be very effective with proper regularization, but do not match BP. Non-local learning rules that still separate forward and backward weights, i.e., those employing "weight estimation" are competitive with BP and have more biologically-plausible interpretations.'],other:[]},metadata:{authors:["Kunin, D","Nayebi, A","Sagastuy-Brena, J","Ganguli, S","Bloom, J","Yamins, D"],institutions:["Stanford University","Broad Institute"],keywords:["neural networks","plasticity","learning rules","deep learning"],title:"Two Routes to Scalable Credit Assignment without Weight Symmetry",journal:"arXiv",doi:"",url:"https://arxiv.org/abs/2003.01513",date:"2020-03",review_date:"2020-03-04",one_sentence:"Kunin, Nayebi, and Sagastuy-Brena et al. show that learning rules separating forward and backward weights can be competitive with backprop on ImageNet"}}},172:function(e){e.exports={metadata:{authors:["Linsker, R"],institutions:["IBM"],keywords:["hierarchical","unsupervised","neural network","Hebbian"],title:"From basic network principles to neural architecture: emergence of spatial-opponent cells",journal:"PNAS",doi:"10.1073/pnas.83.19.7508",url:"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC386748/",date:"1986-09",review_date:"2019-09-18",one_sentence:"In the first of a 3-paper series, Linsker demonstrates how center-surround receptive fields might form in a modular, Hebbian network with random initial activity"},review:{summary:["A three layer neural network, in which random activity patterns are generated in the first layer, gives rise to center-surround receptive fields in the third layer when trained with a  Hebbian learning rule and spatially localized synaptic projections from layer to layer","This architecture will serve as the foundation for the emergence of orientation-selective cells, and eventually columns in later papers in this series (Linsker 1986b,c)","Linsker emphasizes that the results here are not restricted to the visual system, but are easily understood through that lens"],background:["A highly reduced description of the early visual system includes center-surround cells in the retina and LGN, followed by orientation selectivity in primary visual cortex. Most of this structure is apparent at birth, ruling out strict forms of supervised learning (as performed in leading machine-learning frameworks today)","Spontaneous activity is known to be important in the early visual system (mostly in the form of retinal waves in mammals), although this paper takes as a starting point a theoretical system that has spatially random activity patterns"],approach:["Cells are laid out in a series of two-dimensional sheets; in this case there are three layers. The first layer is the input layer, which generates random activity patterns at some resolution (imagine a grid of some density, with each spot in the grid either being on or off at random)","Connect units in layer i to units in layer i + 1 such that corresponding points in each 2D sheet are more densely innervated. In practice this means that the connection strengths are initialized according to a Gaussian centered at the same location in the next sheet",'Use a Hebbian update rule to learn layer 2 given a series of "presentations" in layer 1, then after those weights have stabilized, learn layer 3 from the activity of layer 2 (which in turn is a processed form of the input form layer 1)',"Evaluate the mature weights of the system and compare to known biology"],results:["The weights in this system are unstable until they reach some boundary condition, e.g., +/- 1. That is, for a given postsynaptic cell, all (or all except one) synaptic weights onto that cell will be at extreme values.","There is a range of learning hyperparameters for which layer 2 develops all-excitatory, all-inhibitory, or mixed-sign responses. The author analyzes the all-excitatory case. For this case, layer 2 exhibits spatially local correlations.","Layer 3 emerges as a sheet of cells with center-surround tuning, as found in the retina and LGN of primates. Sometimes orientation-selective cells here, but the conditions under which that occurs are not described in detail."],conclusions:["A multilayer, unsupervised feedforward neural network with a Hebb-type learning rule yields spatial-opponent (center surround) cells in the final layer. A similar result could likely be obtained with other learning rules, although that isn't explored in this paper.","Linsker makes a point to say that this is not meant to be a model of the retina or its development, but is instead a proof of principle that a simple architecture with a simple learning rule and reasonable assumptions can give rise to a fairly complicated motif of the functional architecture of the visual system. I personally find that approach very inspiring, and believe there's a lot of work in this vein yet to be done!"],other:["This paper uses a TON of variable names and symbols. At some point it'd be great to see a simulation or even just a table of parameter names and their meaning..."]}}},173:function(e){e.exports={metadata:{title:"Anatomy and Physiology of a Color System in the Primate Visual Cortex",authors:["Livingstone, MS","Hubel, DH"],institutions:["Harvard Medical School"],date:"1984-01",journal:"J. Neurosci",doi:"10.1523/JNEUROSCI.04-01-00309.1984",url:"http://www.jneurosci.org/content/4/1/309.long",keywords:["color","CO","V1","V2","macaque","blob"],review_date:"2019-03-21",one_sentence:"A seminal paper in the field of color vision, this work establishes the functional properties of cytochrome oxidase blobs in V1 and their relationship to stripes in V2"},review:{summary:["A blob thus seems to be made up of a core of unoriented cells surrounded by a shell of poorly oriented cells (p. 315).","Blobs are color-selective","Blobs project to stripes in V2"],background:["V1 has modular organization defined by functional characteristics such as retinotopic position, ocular dominance, and orientation. There are also blobs that can be identified with cytochrome oxidase (CO) staining. The blobs are found along the tangential pattern of ocular dominance columns in animals that have ocular dominance columns.","Blobs probably consume more energy than non-blobs, as indicated by 1) their labeling with CO, a mitochondrial enzyme, and 2) their uptake of 2-deoxyglucose. That means that there's either different neuronal machinery in blobs, are that they're less selective to simuli and thus fire more often in general","Unknown at the time how these neurons may differ functionally from non-blobs neighbors and if they have different pattersn of projection to downstream areas"],approach:["Present stimuli to anesthetized macaques and squirrel monkeys while making single-unit recordings. Electrode penetrations were about 4-5mm each, with responses recorded every 50 microns on the track","After stimulus presentation and recording, perfuse the animal and stain for CO"],results:["Upon approaching a blob in a penetration, orientation selectivity fell off dramatically and background activity increased. See track reconstructions in Figure 2. Lack of orientation found in 63/72 recorded cells in macaque and 8/8 in squirrel monkey","In macaque, CO blobs are more monocular than surrounding cells. Not so in squirrel monkey, where there is no registration of CO blobs with ocularity maps","Blobs interrupt orientaiton selectivity, but do not perturb it: the smooth varying pattern is usually preserved on the other side of the blob (Figure 5)","Of unoriented cells, the vast majority were double-opponent (center-surround with flipped color preference in center and surround), followed by broad-band (generally center-surround without strong color tuning) -- see Table 1 on p. 322","Staining of V1/V2 demonstrate that blobs project to thin stripes in V2, and interblobs project to interstripe regions in V2"],conclusions:["CO blobs in primary visual cortex are functionally and physiologically distinct from inter-blob regions, but do not disrupt existing patterns (e.g., of orientation tuning) and are in register with other maps (e.g., of ocular dominance)","CO blobs are color selective and project to stripes in area V2"],other:["Interesting self-reference: The historically minded reader may have wondered how so prominent a group of cells could have been missed by so prominent a pair of investigators (Hubel and Wiesel, 1968,1974a).' They conclude that these unoriented cells were either considered damaged, had no clear spatial pattern, were in layer 4C, or that the impression that they weren't there was 'ill-begotten'"]}}},174:function(e){e.exports={review:{summary:["Face-selective voxels in the congenitally blind localize to the lateral fusiform gyrus (haptic and auditory stimulation), much as they do in sighted subjects (visual/haptic/auditory)",'With face-selectivity shown in the blind in "typical" cortical locations, the authors suggest that connectivity to downstream areas (not visual experience) drive the formation of this famous neural circuit'],background:['In typical, sighted subjects, a series of "face patches" (clusters in higher visual cortex) can be found in stereotyped locations. The innateness of this face-processing system has long been the subject of debate. ','Arcaro and Livingstone reported that monkeys deprived of visual experience with faces do not develop face patches (although "face sounds" and touching faces was possible). Meanwhile van den Hurk and Op de Beeck reported typical VTC organization in congenitally blind subjects listening to auditory stimuli. These two results initially appear to be at odds.'],approach:["3D print exemplars of faces, hands, mazes, and chairs. Allow subjects in the scanner to feel each stimulus on a rotating wheel; stimuli were ordered in blocks. Sighted subjects additionally saw videos of the 3D rendered objects as visual stimuli.",'Identify the top N (usually N=50) voxels in a pre-defined anatomical "parcel" based on part of the data, then evaluate the selectivity of those N voxels on a held out portion of the data. Note that sometimes, the top 50 voxels are _not_ significantly selective on their own, despite being the best available.',"Scan some subjects in a second session with additional stimuli (spheroids, cuboids) to assess reliability and the curvature-organization hypothesis (that VTC is organized on the basis of eccentricity preference).",'For each subject, construct a "connectivity fingerprint": the resting state correlation between the face-selective region defined and every other region in the brain.'],results:["Face-selective regions appear in the lateral fusiform gyrus (and somewhat broadly elsewhere) during haptic stimulation for both blind and subject subjects. (Figure 2)","Responses in haptically-defined face-selective areas to auditory stimuli (confusing, I know) confirm modality-independent face-selectivity. (Figure 3).","Connectivity fingerprints were more similar between sighted face-selective regions and blind face-selective regions than between face- and scene-selective regions in either group.","Predicting face-selectivity of one group from the connectivity fingerprints of another group works better than predicting from the functional activity of another group directly (Figure 4). Note, the predictions aren't that great in general (r < 0.5, R^2 < 0.25), so not sure how much to read into this."],conclusions:["Face-selectivity patterns are innate and are generated from connectivity patterns to other neural parcels"],other:["It's difficult to know to what extent this is a useful general framework for thinking about VTC organization, since I don't think other selectivity patterns were tested, e.g., body-selectivity.","I'd be curious to see how these results compare to methods that strictly require face-selective voxels to define a face-selective region (as opposed to the top N most face selective voxels in a region)"]},metadata:{authors:["Murty, AR","Teng, S","Beeler, D","Mynick, A","Oliva, A","Kanwisher, N"],institutions:["MIT (BCS)","MIT (CSAIL)","Smith-Kettlewell Eye Research Institute"],keywords:["human","fmri","blind","development","faces","vtc"],title:"Visual Experience is not Necessary for the Development of Face Selectivity in the Lateral Fusiform Gyrus",journal:"bioRxiv",doi:"10.1101/2020.02.25.964890",url:"https://doi.org/10.1101/2020.02.25.964890",date:"2020-03",review_date:"2020-03-18",one_sentence:"Murty and colleagues compare face-selectivity using visual, auditory, and haptic stimuli in sighted and congenitally blind participants"}}},175:function(e){e.exports={metadata:{authors:["Nauhaus, I","Nielsen, KJ","Callaway, EM"],institutions:["Salk Institute"],keywords:["macaque","V1","calcium imaging","orientation","spatial frequency","ocular dominance","retinotopy"],title:"Efficient Receptive Field Tiling in Primate V1",journal:"Neuron",doi:"10.1016/j.neuron.2016.07.015",url:"http://dx.doi.org/10.1016/j.neuron.2016.07.015",date:"2016-07",review_date:"2020-02-13",one_sentence:"Retinotopy in V1 is remarkably precise, yielding a small point image on cortex; also, spatial frequency and ocular dominance maps run parallel to each other"},review:{summary:["Ocular dominance and spatial frequency maps 1) run parallel to each other but 2) have different spatial periods, suggesting a strategy for optimizing coverage among feature maps.","Macaque V1 micro-retinotopy is very precise, and retinotopic gradients can be observed in small (200 micron) imaging regions both in iso-orientation domains and in pinwheel centers."],background:["V1 in primates (and other large mammals) contains continuous maps of stimulus variables such as orientation, spatial position (including ocular dominance), and spatial frequency. The prevailing framework has been that 1) a complete set of visual features at a given location in space is coded by a retinotopically precise hypercolumn, and 2) that the feature maps run more or less orthogonal to one another to maximize coding efficiency.","Studies spanning organisms have both provided support for and disputed the notion that retinotopy is precise, leaving open the question of how large the cortical 'point image' is: the region of V1 dedicated to processing a point of visual space."],approach:["Wide-field (> 0.5mm) 2P imaging in macaque V1 to compute orientation maps, ocular dominance maps, and spatial frequency maps.","High spatial resolution (cellular resolution) imaging of a small 200 micron patch of V1 to determine micro-retinotopic 'scatter': the degree to which RF centers systematically shift within the imaging region.","For micro-retinotopy, fit a 2D plane to the X positions and Y positions, respectively, then quantify residual scatter as the deviation of each unit from that plane."],results:["Confirming prior findings, the orientation map is orthogonal to both the ocular dominance map and the spatial frequency map (Figure 2). For the first time, it was also shown that regions of high spatial frequency preference tend to localize to transition zones in the ocular dominance map, where neither eye has dominance (Figure 2). The SF and OD maps generally run in parallel.","The SF maps have a lower period (more rapid cycling across the cortical surface) than OD maps, suggesting that staggering maps with different periods is a way around the constraint that three maps (orientation, OD, SF) cannot all be mutually orthogonal (Figure 2)","Micro-retinotopy shows detectable gradients even in the small imaging windows. This holds both for iso-orientation domains and for pinwheel centers (Figure 4)"],conclusions:["The precision of retinotopy and orthogonality of maps suggests that whatever mechanisms lays out these maps (activity-dependent, genetic, etc) is extremely precise.","The feedforward anatomy from the LGN might explain the registration of SF and OD maps. A growing body of evidence now suggests that there is 'a spatial coincidence among blob centers, the center of OD bands, low-SF zones, direct koniocellular input, and indirect magnocellular input, while the interblob zones demonstrate a similar relationship among binocularity, high SF, and parvocellular input'","It is dramatically less clear how orientation maps, which form de-novo in V1, end up in tight register with all other maps. Some theories posit that orientation maps also have retinal/thalamic origins, but the evidence there is contested","The authors argue that while they show lower RF scatter than in other work, their results are also the best because 1) 2P calcium imaging allows greater statistical power and coverage and 2) their particular stimulus (randomized presentation of oriented bars) is better suited to revealing the true nature of RF scatter."],other:["I think this is a valuable paper in filling the gap in macaque knowledge; this reminds me a lot of the systematic work done in cat by Hubener et al., 1997. I would have loved to see more discussion about whether the authors believe this precision is genetically-encoded or enforced by some activity-dependent mechanism, though!"]}}},176:function(e){e.exports={metadata:{title:"Orthogonal micro-organization of orientation and spatial frequency in primate primary visual cortex",authors:["Nauhaus, I","Nielsen, KJ","Disney, AA","Callaway, EM"],institutions:["Salk Institute"],date:"2012-12",journal:"Nature Neuroscience",doi:"10.1038/nn.3255",url:"https://www.nature.com/articles/nn.3255",keywords:["orientation","spatial frequency","calcium","macaque","V1"],review_date:"2019-03-14",one_sentence:"Ca2+ imaging in macaque V1 reveals ordered tuning to orientation and spatial frequency, as well as an orthogonal relationship between the two maps"},review:{summary:["Orientation and spatial frequency maps are consistent across cortical depth in macaque V1","At both microscopic (200x200 microns) and mesoscopic (800x800 microns) scales, orientation and spatial frequency maps are clustered and run orthogonal to each other","Orientation and spatial frequency maps are surprisingly smooth at the scale of a few tens of microns, suggesting that smoothness in intrinsic imaging maps is not solely an artifact of blur"],background:["Maps of spatial frequency tuning have been in rare in macaque and rarely give complete picture due to methodological limitations (resolution and/or sparsity of sampling)","While interactions between orientation and spatial frequency maps have been reported in cats and ferrets, direct evidence in macaques has been elusive"],approach:["2P Ca2+ imaging of layer 2/3 macaque V1 neurons at two spatial scales and two depths","Compute tuning curves for each pixel in the large scale map and each neuron in the fine-scale map","Compare tuning for orientations and spatial frequency"],results:["Both orientation and spatial frequency maps are clustered (tuning curves more correlated at closer distances), although spatial frequency maps have more scatter (Figure 4)","Large scale (smooth) maps for orientation and spatial frequency intersect each other at 90-degree angles (Figure 5)","Maps are periodic (2D autocorrelations begin to rise again after about 500 microns (Figure 5)","At fine scale, direction of maximum tuning curve change is orthogonal for orientation and spatial frequency maps (Figure 6)","For a pair of cells, the correlation between tuning curves for orientations is anti-correlated with correlation between tuning curves for spatial frequencies at larger distances"],conclusions:["Maps for both parameters are relatively continuous and have lawful relationships to one another","This is the first study to examine both maps with sufficiently high spatial resolution and density of sampling","Fine-scale maps reveal very smoooth progression of peak tuning curve values"],other:["Would be interesting to see color tuning in these imaging regions!"]}}},177:function(e){e.exports={metadata:{title:"Highly ordered arrangement of single neurons in orientation pinwheels",authors:["Ohki, K","Chung, S","Kara, P","H\xfcbener, M","Bonhoeffer, T","Reid, RC"],institutions:["Harvard Medical School","Max Planck Institute of Neurobiology","Korea Institute of Science and Technology","Medical University of South Carolina"],date:"2006-08",journal:"Nature",doi:"10.1038/nature05019",url:"http://ctx.kist.re.kr/Teams/ctx/PDFs/Ohki%20et%20al%20Nature%202006.pdf",keywords:["cat","V1","calcium imaging","pinwheel","orientation"],review_date:"2019-08-16",one_sentence:"Ohki et al. demonstrate that at a single-neuron level, pinwheel structure is held up by sharply tuned neurons that obey the radial structure of the pinwheel"},review:{summary:["Single neurons in orientation pinwheels are tightly arranged, suggesting that the pinwheel center goes down to the single neuron level instead of being an artifact of intrinsic imaging methods","Neurons in pinwheel centers have lower activity levels and are more broadly tuned than neurons peripheral to the pinwheel center"],background:["Prior study of orientation tuning maps have either lacked coverage (electrode/tetrode recording) or spatial specificity (electrophysiology, intrinsic optical imaging), leaving open the question of how pinwheel centers were organized at the single neuron level","Intrinsic imaging has suggested that pinwheel centers are more broadly tuned than iso-orientation domains (see, e.g., Bartfeld and Grinvald '92). Note: this is also observed in subsequent calcium imaging studies, e.g., Nauhaus et al., 2008"],approach:["Label neurons near pinwheel centers in V1 of 28-35 day old cats with a calcium indicator (Oregon Green BAPTA-1)","Perform 2-photon calcium imaging at 9 different cortical depths while presenting bars at orientations separated by 45 degrees to the cat"],results:["Neurons near pinwheel centers are organized with a very low degree of 'scatter', indicating that the pinwheel structure is valid down to individual cell bodies (Figure 1, 3B)","Neuron tuning is columnar, with highly similar orientation preference across depths (Figure 1)","At the pinwheel center, spatially adjacent neurons can have very different tuning (Figure 2)","Neurons in the pinwheel center had lower responses and broader tuning in every experiment performed (Figure 3). The difference in tuning width between pinwheel center and periphery increased even more when bar orientations were separated by smaller steps (22.5 degrees), but they were still fairly sharply tuned (or at least, more than people might have expected?)"],conclusions:["Neurons in pinwheel centers are sharply tuned and don't deviate from the expected radial structure of the maps by much. This suggests a precise developmental mechanism at play, given that these patterns are fairly well-established early in life","One important mechanism to figure out is how dendritic arbors might be A) arranged to sample input differently, or B) might receive inputs of different strengths that drive this sharp tuning"],other:["This paper stands out to me as an example where biology isn't messy... The precision of these maps suggests that pinwheel centers might be important either for performing some task (see my review of Koch et al., 2016), or for reducing some metabolic cost."]}}},178:function(e){e.exports={metadata:{title:"Emergence of simple-cell receptive field properties by learning a sparse code for natural images",authors:["Olshausen, BA","Field, DJ"],institutions:["Cornell"],date:"1996-06",journal:"Nature",doi:"10.1038/381607a0",url:"https://www.nature.com/articles/381607a0",keywords:["V1","Gabor","sparse coding","natural image"],review_date:"2019-04-25",one_sentence:"Olshausen and Field provide a normative argument for V1 receptive fields: they are the optimal basis for sparse reconstruction of patches of natural images"},review:{summary:["Minimizing a combination of image reconstruction loss and a sparsity constraint via gradient descent leads to a Gabor-like filterbank","This coding scheme gives more biological results than PCA, due to the non-Gaussian distribution of image pixel values"],background:["V1 neurons have receptive fields that are tuned to spatial frequency, orientation, and stimulus position","Nearly all natural images exhibit higher-order correlations, making methods relying on pairwise linear correlation, e.g., PCA, unhelpful in recovering the image structure"],approach:["Learn a 192-filter bank of 16x16 black and white filters, trained on small portions of natural images of the American northwest","Define the loss function as a sum of pixel-wise reconstruction loss and a penalty on large filter coefficients","Whiten images, then perform conjugate gradient method (more efficient than steepest descent) for roughly 400,000 image presentations to minimize the desired loss function"],results:["PCA fails to recover biological filters. Instead, it finds unbiological checkerboard-like patterns (Figure 1)","The resulting filters are an overcomplete set of Gabor-like filters selective for different orientations, spatial frequencies, and positions (Figure 4)","The result depends on the use of natural images, as learning sparse decompositions of other types of stimuli is ineffective (Figure 3)"],conclusions:["V1 receptive fields are the way that they are because it's a good way to encode the visual scene under sparsity constraints!"],other:["This is one of my favorite results in visual neuroscience. In a way, these results have been reproduced in the deep-learning era, as networks trained with regularization and a limited number of channels in early convolutional layer learn that Gabor-like filters are a good basis set for representing natural images. Whether or not the basis is as good as the one in this paper is a difficult question to answer, but may be a relevant one in years to come."]}}},179:function(e){e.exports={metadata:{title:"Factors Determining Where Category-Selective Areas Emerge in Visual Cortex",authors:["Op de Beeck, HP","Pillet, I","Ritchie JB"],institutions:["KU Leuven"],date:"2019-07",journal:"TICS",doi:"10.1016/j.tics.2019.06.006",url:"https://www.sciencedirect.com/science/article/abs/pii/S1364661319301585",keywords:["human","VTC","fMRI","category","topography","development"],review_date:"2019-08-04",one_sentence:"Op de Beeck et al. argue that the functional organization of human VTC can be understood by considering three factors: which visual features a region represents, how the visual features area processed, and the cognitive/semantic domain associated with the category"},review:{summary:["The functional architecture of higher visual cortex can be understood by considering the features represented, how those features factor into a hierarchical computational framework, and the cognitive domain they are associated with","Visual features and categories cannot be meaningfully divorced from each other; instead, we should consider pre-existing preference for visual features predictors of where regions for categories consisting of those features will emerge"],background:["Category-selective regions have long been studied in human VTC, but theories predicted their specific spatial organization with respect to anatomy and to each other have been lacking","Whether these regions are strictly experience-dependent, activity-dependent, or innate/genetically-programmed has been a matter of debate"],approach:["This is a review paper which primarily draws on studies using fMRI in humans"],results:["VTC category preference is a lot stickier than people like to make it seem. Regions often have graded selectivity and cluster according to superordinate dimensions such as object size and animacy. 'Metaphorically speaking, a category-selective area is not a lone functional peak in an otherwise flat functional landscape, like a neural Kilimanjaro. Rather, it is one of many hilltops in a Yellowstone-like complex, of which the origin can only be understood by investigating the larger-scale environment'","Categories and category-typical features are tightly correlated, e.g., faces have a typical shape and are viewed in the same position of the visual field. Nevertheless, category-selective areas have been shown to respond to a given category even when controlling for such factors (Box 1)","Within the occiptotemporal cortex (OTC), regions with similar preference connect with each other. Connections reaching outside OTC, however, diverge in their targets. For example, word-form areas preferentially connect to left hemisphere language regions","Congenitally blind individuals provide a unique perspective on the developmental emergence of category-selective regions. The fact that the same general topography emerges in these individuals suggests that visual experience itself is not required for the formation of category-selective regions (but see Arcaro et al., 2017)","The eccentricity bias theory (that VTC layout is determined by typical viewing eccentricity; see Gomez et al., 2019's Pokemon study) has been popular in the field, but studies that support it have insufficiently separated typical viewing eccentricity from the other factors listed here"],conclusions:["While the three factors described (features, computation, and domain/connectivity) are clearly important, none of them is sufficient in isolation to explain the functional organization observed"],other:["Reviewing this paper was inspired by last week's review of Arcaro et al., 2019","A huge missing piece in my reading of this is: 'what about anatomy?! Beyond connectivity, that is.'. We know that different regions of higher visual cortex have microanatomical differences, and presumably these dictate the layout at a much more specific level than what is offered here.","While I agree with virtually everything in this very nice review, I find it a bit unsatisfying that these observations don't bring us closer to answering a questions like 'why is there a face-selective region on the lateral fusiform, and not somewhere else? What determines its size?"]}}},180:function(e){e.exports={metadata:{title:"Face Space Representations in Deep Convolutional Neural Networks",authors:["Alice J. O\u2019Toole","Carlos D. Castillo","Connor J. Parde","Matthew Q. Hill","Rama Chellappa"],institutions:[" University of Texas at Dallas"," University of Maryland College Park"],date:"2018-08",journal:"Trends in Cognitive Sciences",doi:"10.1016/j.tics.2018.06.006",url:"https://www.sciencedirect.com/science/article/pii/S1364661318301463",keywords:["face","DCNN","face space"],review_date:"2019-09-03",one_sentence:"O'Toole et al. summarize recent advances in the use of deep convolutional neural networks for face recognition and as models of biological face processing"},review:{summary:["Deep convolutional neural networks (DCNNs) achieve state-of-the-art on  demanding face recognition tasks",'By considering how individual identities are represented in a high-dimensional "face-space", we see that DCNNs accomplish this task via rich yet compact representations, reminiscent of observations about how objects are represented in ImageNet-trained DCNNs'],background:["Early attempts at modeling the representation of faces relied on an image-based space commonly constructed by performing PCA directly on pixel inputs. This technique isn't terrible, but breaks down quickly when there are changes in viewpoint or luminance","Active appearance models (AAMs) emerged as an alternative to image-based models. They were more flexible in that they represented shape and texture information separately, allowing a generative model of faces at any viewpoint or illumination condition. Notably, though, there's no mechanistic understanding of how this division of labor might be accomplished in the brain, although there is evidence that face-selective neurons care about combinations of these dimensions (see Change and Tsao, 2017)","DCNNs have become clear favorites for object recognition tasks, and under some formulations of the task, one can consider face recognition an extension of object recognition task: critically, both require tolerance to identity-preserving transformations but specificity across identities","An important shortcoming of previous face-recognition models is that they don't account for 1) familiarity/\"training\" with certain identities and 2) similarity-based models don't explain changes in illumination and pose neatly, they only describe how similar two faces may be","As recently as 2012-2013, models trained to recognize faces only reached human performance on frontal views of faces but could not generalize to other viewpoints effectively"],approach:["This is a review paper that mostly cites work using deep convolutional neural networks trained on Labeled Faces in the Wild, IJB-A, and Mega-Face",'The networks are often trained with some combination of good-old softmax loss and "triplet loss", which when given three faces (two of which are the same identity) is penalized for failing to separate the two unique identities',"Networks are often, but not always, pre-trained on an object recognition task, then fine tuned on a face training set."],results:["Face representations in later DCNN layers provide readouts of many relevant variables, including identity, face viewpoint, pose, etc! This is consistent with similar observations for non-face objects reported in Hong et al., 2016 (Nature)",'Some faces end up being coded robustly across many viewpoints, whereas others seem "bound" to a specific viewpoint. Whether this is a shortcoming of the training data or an indicator of properties specific to those identities remains to be seen',"The DCNN face space appears to care quite a bit about image quality: images at low resolution or with a lot of occlusion are represented as more similar to each other than images where the face is clear, high-resolution, and unobstructed. Eshed's note: I doubt this is specific to face-trained networks, but is likely a property of any network in which classification becomes hard under occlusion"],conclusions:["The paper ends with a number of really important and interesting questions. In fact, this was probably my favorite part of the paper!","Q1) What should the training set be? Objects + faces? Objects then faces? Faces alone? How do we think face-recognition emerges in humans? As part of a more general system of networks (this is my camp), or as specialized hardware with a specialized training routine?","Q2) If you take a network and train it for faces or for objects, in which layer do you start to see appreciable differences? My guess is that it would be fairly late.","Q3) How can DCNNs be used to explain activity in face-selective brain areas?","Q4) Do DCNNs represent characteristics of faces such as gender and age?","Q5) How does the quality of the data (specifically, as a function of training stage) affect learned representations?","Q6) How much does the architecture of the DCNN matter?","Q7) How does neuroscience need to change to measure axes of the neural space that map onto semantically meaningful concepts?"],other:["I found this review very well-written and thoughtful, and I learned a lot about the state-of-the-art before the deep learning boom!"]}}},181:function(e){e.exports={metadata:{title:"Evolving super stimuli for real neurons using deep generative networks",authors:["Ponce, CR","Xiao, W","Schade, PF","Hartmann, TS","Kreiman, G","Livingstone, MS"],institutions:["Harvard University"],date:"2019-01",journal:"bioRxiv",doi:"10.1101/516484",url:"https://www.biorxiv.org/content/10.1101/516484v1",keywords:["generative","genetic algorithm","IT","macaque","face"],review_date:"2019-03-28",one_sentence:"A closed loop paradigm to identify the best stimulus to drive a neuron is applied to recordings of macaque IT"},review:{summary:["A combination of a deep generative network and a genetic algorithm synthesizes stimuli that drive neurons better than natural images","Natural images that are similar to synthesized images, given the encoding used, also drive higher firing rates than those dissimilar to the synthesized images"],background:["Identifying the optimal stimulus for a neuron is difficult, and is often approximated with hand-crafted stimuli of a certain form. This approach breaks down further in the ventral visual stream","Deep generative networks have recently been used to produce realistic stimuli that drive neural network units optimally"],approach:["Begin with a group of images, convert them to textures via the Portilla and Simoncelli algorithm, convert those to 4096-dim vectors. Evaluate 'fitness' of each code by using a deep generative network to conver the code to an image and measure the firing rate in response to that image. Finally, use a genetic algorithm to modify the code such that it will lead to higher firing rates in the future. Repeat.","Recordings were made from IT in 5 monkeys and V1 in 1 monkey as a control. Mostly chronic array recordings were used, but one monkey had a recording chamber."],results:["The synthesis procedure gradually produces better and better stimuli, and at the end stage of synthesis, responses to these images are higher than to a large collection of natural images (Figures 4 and 5)","The images are (sort of?) realistic, and produce different categories of objects for different neurons, such as faces for face-selective neurons and scene-like images for scene-selective neurons (Figure 6)","Finding natural images that are close to the synthetic image in the encoded space yields higher firing rates than expected by choosing random images, suggesting that the synthetic images lie near a 'manifold' of natural stimuli that drive high firing rates","Firing rates are more invariant to natural image rotation, scaling, and position shifts than to the synthesized images (Figure S3)"],conclusions:["This generative evolutionary approach is successful in identifying stimuli that drive the target neurons","The synthesized images are abstract and hard to describe, maybe an indicator that IT neurons aren't selective to objects per se, but to abstract axes of objects, as posed by Doris Tsao?","The approach is closed-loop and requires a lot of iterative data acquisition. Furthermore, it's not clear to what degree the choice of initial images constrains the space of images that the synthesizer might find","Most importantly, I was left unsure of why such a complicated approach was needed -- what was the justification for the genetic algorithm? Would gradient ascent not work for these neurons for some reason?"],other:["Related work: Neural Population Control via Deep Image Synthesis by Bashivan, Kar, and DiCarlo. The advantage of this alternative approach is that it is more generally flexible (does not impose priors on what sorts of image can be generated) and only requires data acquisition at two time points (once to establish a linear mapping from a model, e.g., a convolutional neural network, to the neuron of interest, and again to validate the method with the stimuli"]}}},182:function(e){e.exports={metadata:{title:"Reversible inactivation of different millimeter-scale regions of primate IT results in different patterns of core object recognition deficits",authors:["Rajalingham, R","DiCarlo, JJ"],institutions:["MIT"],date:"2019-04",journal:"Neuron",doi:"10.1016/j.neuron.2019.02.001",url:"",keywords:["inactivation","macaque","IT","object recognition"],review_date:"2019-06-06",one_sentence:"Muscimol injections in arbitrarily-selected millimeter scale regions of IT suggests a heterogeneous yet spatially continuous layout of IT with respect to core object recognition"},review:{summary:["Inactivation of small regions of IT negatively affects some object discrimination subtasks, but not others","Nearby injections yield similar patterns of behavioral deficits","Decoding models suggest that local discriminability, not local selectivity, are best predictive of the behavioral effects of inactivation"],background:["A large body of work supports the idea that activity in IT is correlated with behavior. However, in the visual motion literature, recent evidence has suggested that regions with behavior-correlated activity are not necessary to support that behavior, i.e., it is epiphenomenal","Previous attempts at causal manipulations have overwhelming been targeted to face patches and to face-specific tasks"],approach:["Inactivate many regions along the ventral surface of IT with muscimol injections (about 2.5mm spread), evaluate effects on object recognition performance on a set of 6 (or 10, in one experiment) subtasks, where each subtask is a pairwise discrimination such as dog vs. elephant","Collect neuron responses in the same regions that are inactivated during a passive fixation task, then compare decoders that predict how neuronal activity translates to behavioral deficits during inactivation"],results:["Inactivation results are hetereogeneous: inactivation of any given site only affects some subtasks (Figures 2, 4), but no task is affected more than others when averaging across all subtasks (Figure 5)","The strongest effect of inactivation is seen when the object of interest is in the visual field contralateral to the injection site and peaks around a d' difference of 0.6 (Figure 3)","Inactivation of nearby sites yields similar behavioral deficits (Figures 6, 7), with correlations that drop to 0 after about 10mm","Models that predict inactivation from response selectivity perform worse than models that predict inactivation from the accuracy of an SVM trained on responses of those neurons (Figure 8)"],conclusions:["IT is necessary for object discrimination behavior, and different regions support different subtasks. It is not the case that all non-face regions are necessary for all non-face tasks","Local discriminability is a strong predictor of the effects of inactivation, suggesting one possible readout mechanism from IT to downstream regions"],other:["See last week's paper-a-week (Afraz et al., 2015) for more inactivation work from the DiCarlo lab, but targeted specifically to face patches","These effects are small in terms of balanced accuracy, and the monkeys are generally at ceiling on these tasks to begin with. There's a very fair and careful justification of using d' instead of accuracy/choice bias/other, and in general the work is incredible carefully done and impressive"]}}},183:function(e){e.exports={metadata:{title:"Orientation Selectivity in Macaque V1: Diversity and Laminar Dependence",authors:["Ringach, DL","Shapley, RM","Hawken, MJ"],institutions:["UCLA","NYU"],date:"2002-07",journal:"J. Neurosci",doi:"10.1523/JNEUROSCI.22-13-05639.2002",url:"http://www.jneurosci.org/content/22/13/5639.long",keywords:["macaque","V1","orientation","circular variance","bandwidth","laminar"],review_date:"2019-04-04",one_sentence:"Ringach and colleagues establish a variety of orientation selectivity metrics throughout different cortical layers of macaque V1"},review:{summary:["Many V1 cells are poorly tuned for orientation with a median circular variance of 0.6 (0 is perfectly orientation-selective, 1 is perfectly non-selective)","Orientation selectivity correlates weakly with spontaneous activity and more strongly with modulation ratio (a measure of how simple or complex a cell is)","Despite both measuring orientation tuning selectivity, circular variance and bandwidth can give different views on a distribution of cells"],background:["V1 neurons are known to be selective for particular orientations, but the exact distribution of selectivity in different cortical layers in macaque is unknown","Bandwidth of orientation tuning is an often used but potentially inaccurate method, as it does not consider responses distant from the preferred orientation of a cell","Unoriented cells were primarily thought to reside in layers 2/3, where CO blobs are often found (see Livingstone and Hubel, 1984 for more), and to some degree in layer 4C"],approach:["Electrode penetrations from 26 anesthetized macaques","Monocular stimulation with drifting sinusoidal grating in steps of 10, 15, or 20 degrees (depending on experimenter's interpretation of how sharply tuned a cell was)","Compute modulation ratio, circular variance, bandwidth, and spontaneous activity level for all isolated single units. See paper for definitions of these quantities"],results:["V1 cells span the range of circular variances, with a median circular variance of 0.61, and skew to lower bandwidths (median == 23.5 deg) (Figures 1, 2)","Circular variance and bandwidth both vary substantially at all cortical depths (Figures 4, 5)","Bandwidth and circular variance are correlated, but there are many low-bandwidth neurons with high circular variance (Figure 3).","Both metrics are correlated with a third metric: the ratio of response to the anti-preferred (orthogonal) orientation and the preferred orientation. This relationship is tighter for circular variance (Figure 6)","Low spontaneous activity appears to be associated with higher orientation selectivity (Figure 8)","Modulation ratio (low == more like a complex cell, high == more like a simple cell) varies by cortical depth, with highest values in layer 4C and lowest values in layer 3B and 5. Cells with high circular variance typically have low modulation ratios, suggesting that complex cells have lower selectivity than simple cells (Figures 11, 12)","Complex cells have higher spontaneous activity than simple cells (Figure 14)"],conclusions:["V1 has a diversity of functional properties across cortical depth, challenging the idea that there are sharp differences in tuning at layer boundaries. Importantly, cells poorly tuned for orientation are found throughout the cortical depth","Metrics matter! Circular variance appears reliable and is a measure of global tuning curve structure, but in other cases one may genuinely care about other metrics. Whenever possible, comparison to previously reported metrics helps bring many results into alignment, and is a really useful approach"],other:[]}}},184:function(e){e.exports={review:{summary:["In layer 4C (the input layer) cells retain the same orientation tuning over time. However, in other layers, cells have dynamic tuning profiles that can change dramatically over the span of a few tens of milliseconds.","The dynamic nature of the tuning of V1 cells suggest some recurrent mechanisms that are unaccounted for in simple feedforward models."],background:["The canonical model of of V1 tuning is that all cells are orientation tuned, and those orientation preferences are static over the course of stimulus presentation.","The input to V1, LGN cells, are definitely not tuned, prompting the question of how orientation preferences emerges in V1. LGN cells project to a limited laminar range in V1: only to layers 4C-alpha and 4C-beta."],approach:["For each cell, construct a set of sine gratings at an optimal spatial frequency, but varying in orientation and spatial phase. Present random images from this set at a rate of 60fps for about 15 minutes per cell.","Record all spikes in the 15 minute window, then perform the following reverse correlation method. Pick a time-delay, tau, and go back tau ms from the time of each spike. Log the orientation (ignoring phase) of the stimulus at that time delay. Finally, construct and normalize a histogram of orientations at a given tau. Define 'optimal tau' as the tau at which the cell exhibits the sharpest tuning (farthest from uniform across all orientations).","Record from 61 V1 cells whose laminar locations, e.g., layer 4C-alpha are known."],results:["In layer 4C, all 12 cells recorded had unimodal temporal preferences. This means that at very long time delays, their tuning is uniform, then exhibits a single peak, then relaxes back to uniform (Table 1 and Figure 2AB)","Outside of 4C, many cells showed multimodal tuning as a function of time delay. Some had sharp peaks early that switched to orthogonal orientation preference, whereas others had continuously changing orientation preference (Table 1 and Figure 2C-H).","In a small number of recorded cells, the early excitatory component could be seen sharpening over time, casting further doubt on a simplistic feed-forward model (Figure 3). "],conclusions:["The temporal dynamic observed outside layer 4C are difficult to reconcile with a feedforward model, suggesting some sort of recurrence. One potential mechanism involves inhibitory suppression by neighboring neurons, which might explain the biphasic responses in many cells.","Similarly, sharpening of tuning over time might be due to recurrent excitation."],other:["An interesting theoretical question here relates to the putative role of these dynamics. Why should a system want to have these dynamic properties? Or is it the case that the dynamics have no effect on behavior, but are an unintended consequence of wiring that's used for other things?","How do these results, which suggest that most cells are tuned to specific orientations but that the orientation changes, reconcile with Ringach et al., 2002 in which they demonstrate that relatively few cells are strongly tuned?"]},metadata:{authors:[" Ringach, DL"," Hawken, MJ"," Shapley, R"],institutions:["Center For Neural Science"],keywords:["macaque","v1","orientation","dynamics","feedback"],title:"Dynamics of orientation tuning in macaque primary visual cortex",journal:"Nature",doi:"10.1038/387281a0",url:"https://nyuscholars.nyu.edu/en/publications/dynamics-of-orientation-tuning-in-macaque-primary-visual-cortex",date:"1997-05",review_date:"2019-12-24",one_sentence:"Ringach et al. demonstrate that V1 neurons exhibit dynamic orientation tuning over the span of tens of milliseconds, especially in non-input layers"}}},185:function(e){e.exports={metadata:{title:"Object Representation in Inferior Temporal Cortex Is Organized Hierarchically in a Mosaic-Like Structure",authors:["Sato, T","Uchida, G","Lescroart, MD","Kitazono, J","Okada, M","Tanifuji, M"],institutions:["RIKEN","UC Berkeley","University of Tokyo","Waseda University","National Yang-Ming University"],date:"2013-10",journal:"J. Neurosci",doi:"10.1523/JNEUROSCI.5557-12.2013",url:"http://www.jneurosci.org/content/jneuro/33/42/16642.full.pdf",keywords:["macaque","IT","category","topography"],review_date:"2019-07-25",one_sentence:"Sato and colleagues demonstrate that macaque IT is organized hierarchically in its responses: columns form 'domains', and similar domains cluster on the cortical surface"},review:{summary:["Electrophysiological assessment of IT reveals domains on the scale of multiple mm that are tuned to particular categories of objects, e.g., faces and bodies. This supports and explains existing literature using fMRI and intrinsic signal imaging."],background:["Two competing models for IT organization are the modular theory, which posits that computation proceeds from spatially-distinct module to module (e.g., the face patch network), and the distributed theory, which posits that computation proceeds from the assembly of general visual features that represent objects","Much of the support for the two theories comes from fMRI and optical intrinsic signal imaging (OISI), which are indirectly coupled to neuron activity and have poor spatial resolution. So it remains unclear what the fine-scale spatial organization of IT is with respect to rich visual features."],approach:["Extracellular recordings in the right hemisphere of three monkeys (total of 99 sites across monkeys). At each site, record from three electrodes in a triangle pattern and at five cortical depths. These 15 sites (3 electrodes x 5 depths) are averaged to produce the 'local activity' for that site","Present images of 7 categories: faces, scrambled faces, monkey hands, monkey bodies, nonprimate bodies, foods/veggies, and man-made objects","Determine correlation of responses to each image across sites, then perform hierarchical clustering to try and identify structure in the correlations"],results:["Hierarchical clustering suggests that there are between 7-8 distinct response groups, and these groups also tend to be spatially co-localized (Figure 2). The extent of these 'domains' were multiple mm in diameter.","Hierarchical clustering suggests that there are between 7-8 distinct response groups, and these groups also tend to be spatially co-localized (Figure 2). The extent of these 'domains' were multiple mm in diameter. Note: this implies functional domains are larger than single columns!","The identified clusters are robust to variations in stimuli like color or silhouette/full, and are further robust to the choice of clustering algorithm (Figures 3, 4)","Selectivity analysis of the domains indicates that some are face selective, some are body selective, some are anti face-selective, and some appear nonselective. (Eshed's note: this is cool, I've always wondered if there are spatially-contiguous regions that respond especially poorly to faces!) (Figure 5)","Optical intrinsic imaging suggests that the domains are composed of columns (sites on the order of 0.5mm in diameter) (Figure 10)"],conclusions:["IT is organized hierarchically with respect to topography; domains are composed of multiple feature columns, and regions are likely to be contiguous domains","Results indicating that modality of measurement doesn't matter (e.g., Kriegeskorte et al., 2008) may arise from the fact that randomly sampling within a domain is the same as recording aggregate domain activity"],other:["The outstanding question in my mind is why we observe this hierarchial structure. That is both 1) why is it hierarchical, and 2) why this particular structure with respect to faces, bodies, etc","See Rajalingham et al., 2018 for a causal manipulation of arbitrarily selected IT sites. Their results are generally aligned with these; that IT is a heterogeneous landscape with some local spatial dependencies in the response profiles"]}}},186:function(e){e.exports={review:{summary:["When retinal inputs are redirected to auditory cortex in early life, ferrets exhibit orientation-tuned auditory cortex cells that arrange into maps","The apparently instructive activity from the retina appears to change the nature of intracortical horizontal connections, yet doesn't fully recapitulate visual-cortex-like structure"],background:["Many approaches to study whether visual inputs are instructive or permissive of V1 functional architecture use either 1) deprivation, which is not specific enough to get at mechanism, or 2) perturbation later in the animal's development after some orientation tuning is already formed","An open debate (still to this day, I think) pertains to the genetically-endowed rigidity of a region's function. That is, if we know that area 1 has some anatomy and typically supports function 1, and area 2 has different anatomy and supports function 2, is it impossible for area 2 to support function 1 and vice a versa?"],approach:["Deafferent the medial geniculate nucleus (MGN) and ablate the superior colliculus in ferrets immediately after they are born","Perform intrinsic optical imaging while showing sine gratings to evaluate orientation and direction tuning","Use anatomical tracers to determine the anatomical connectivity structure of either V1 or A1","Use single-unit recording to investigate the origins of maps apparent at coarser intrinsic imaging resolution"],results:["Cells in rewired-A1 were visually-responsive, sharply orientation-tuned, and formed coherent maps across the cortical surface (Figure 1)","Map organization was stable across imaging sessions, at least to the degree V1 maps were","The maps formed in rewired A1 featured larger orientation domains that were less periodic across the cortical surface than those in V1 (Figure 1)","Individual cells were sharply tuned both in A1 and in an anterior region typically involved in auditory perception (Figure 2)","Long-range horizontal projections are characteristic of V1 but not A1. In rewired A1, however, longer range horizontal connections were observed (Figure 3). These maps were also more periodic in rewired A1 than typical A1, more closely resembling V1-like maps (Figure 5)"],conclusions:["Finding sharply tuned cells in a not-so-organized map suggests that the two features are not as tightly linked as previously thought. In modern retrospect, this makes sense -- we know for example that V1 has very organized maps with a lot of cells that are not sharply orientation tuned, e.g., Ringach et al., 2002","Whereas previous work had shown that afferent activity is needed to maintain maps, this work shows that it is able to instruct their formation as well. The residual difference between rewired-A1 and normal-V1 maps, however, suggests that instructive activity cannot account for all of the observed structure"],other:["An obvious question in my mind is: how is the activity coming from the retina meaningfully different from that coming from the inner ear? Could we ever understand the requirements for orientation tuning in a mechanistic way, i.e., such that we could play the right set of sounds to induce pinwheeled-maps?","If sharpness of tuning can be dissociated from sharpness of map structure, why is precision of the maps so important (see Ohki et al., 2006)?"]},metadata:{authors:["Sharma, J","Angelucci, A","Sur, M"],institutions:["MIT"],keywords:["ferret","v1","a1","orientation","plasticity","rewiring"],title:"Induction of visual orientation modules in auditory cortex",journal:"Nature",doi:"10.1038/35009043",url:"https://www.nature.com/articles/35009043",date:"2000-04",review_date:"2019-12-16",one_sentence:"Sharma, Angelucci, and Sur rewire retinal inputs to the MGN and onward to A1 and observe the emergence of V1-like structure (orientation tuning in maps) the deviates from normal A1 development"}}},187:function(e){e.exports={metadata:{title:"Distributed network interactions and their emergence in developing neocortex",authors:["Smith, GB","Hein, B","Whitney, DE","Fitzpatrick, D","Kaschube, M"],institutions:["Max Planck Florida Institute for Neuroscience","University of Minnesota","Frankfurt Institute for Advanced Studies","International Max Planck Research School for Neural Circuits","Goethe University"],journal:"Nature Neuroscience",doi:"10.1038/s41593-018-0247-5",url:"https://doi.org/10.1038/s41593-018-0247-5",date:"2018-11",review_date:"2019-11-14",one_sentence:"Smith and colleagues perform longitudinal Ca2+ imaging in ferret V1 and find that modular spontaneous correlation patterns before eye opening predict orientation maps after eye opening",keywords:["ferret","v1","orientation","correlation","development"]},review:{summary:["Before eye opening, activity in V1 is modular and distributed -- relative to some seed point, a patchy network of correlated regions emerges","This correlation structure does not depend on feedforward input from the retina or LGN and predicts orientation structure at about postnatal day 5 (P5)"],background:["Visual cortex is not silent before eye opening, but exhibits spontaneous, endogenous inputs","The capacity of this spontaneous input to produce maps organized at the scale of several millimeters has been unknown, given that intracortical horizontal connections continue to develop after eye opening"],approach:["Imaging of V1 in female ferrets spanning timepoints before and after eye opening",'A model with anisotropic "Mexican-hat" tuning functions is proposed to explain the observed patterns of cortical activity'],results:["Spontaneous activity has local correlations on the scale of a few mm (dies off at about 3mm) and is relatively consistent whether examined at the cellular or wide-field scales, and whether the animal is awake or anesthetized (Figure 1)","The network of correlations observed in spontaneous activity can be used to predict orientation tuning maps, even when the area immediately surrounding a seed point is masked out of the analysis. This suggests that the patterns are robustly organized across many millimeters (Figure 2)",'"Fractures" in the correlation structure, lines along which moving the seed point will lead to dramatically different correlation maps, perfectly coincide with traditionally-defined fractures in orientation maps (lines along with the orientation gradient is high) (Figure 3)',"Measuring spontaneous activity before eye opening provides information to predict orientation tuning as early as 5 days after eye opening, suggesting a role for the spontaneous activity in setting up cortical networks (Figure 4)","After development, reversible inactivation of the retina (abolishing waves with TTX) or LGN (muscimol injection) does not abolish correlation structure, indicating a cortical locus for the correlations. Note: is it possible other thalamic regions are still online in this analysis (e.g., pulvinar?) (Figure 5)"],conclusions:["Visual cortex may have a 'fundamental modular structure' that is established early in development (but by which mechanisms?) and may prime the system for the patterns of, e.g., orientation tuning that are observed later. The authors also note that the spontaneous correlation patterns align somewhat with ocular dominance (OD) columns, but more weakly. How does that work? Given that there's supposed to be a regular orientation-OD relationship (see Hubener et al., 1997), shouldn't it be strongly predictive?","The feedforward inactivation results suggest that somehow, despite short horizontal connections, V1 is propagating these long-range correlation structures. The authors explain this with their anisotropic Mexican-hat tuning model (Figure 6), but are there alternative explanations? Similarly, is there empirical evidence for the kinds of receptive fields described here?"],other:["I really enjoyed reading this paper and thought the results go a long way in binding some confusing results about the timeline and robustness of orientation tuning maps early in development"]}}},188:function(e){e.exports={review:{summary:["Recurrence can be used to understand how both biological and artificial neural networks can accomplish complex tasks under spatial, metabolic, and temporal constraints","Once we overcome some technical hurdles, recurrence will reveal itself to be superior framework for solving visual inference -- a goal computer scientists and neuroscientists can work toward"],background:["The visual system employs recurrent computation that is apparent in its connectivity, temporal response patterns, and timing of behavioral responses. However, it is unknown why recurrence is used in the brain. In fact, feedforward neural networks have enjoyed success as the current state-of-the-art models of both behavior and neural function.","There are three explanations for why recurrence might be used in the brain despite the success of feedforward neural network models (FNNs). 1) the brain operates under space, time, and energy constraints that neural networks do not, 2) that feedforward algorithms are what really matter, but recurrence is useful for corner cases, and 3) that recurrence is fundamentally superior as an algorithm to feedforward computation"],approach:["This is a perspective paper so there aren't methods to discuss per se. The primary tools described are recurrent neural network models, although a distinction is made that any RNN can be unrolled into an FNN, so we need to be careful about designations like that.","Networks unrolled in space feature 1) shared weights, because the same connections are being modeled as different stages in a fully feedforward architecture, and 2) many skip connections."],results:['Instead of thinking of RNNs as a special case of FNNs, we should think of it the other way around. Direct quote: "Thus,  any  finite-time  RNN  can  be  transformed  into  an equivalent  FNN.  But  this  should  not  be  taken  to  mean that  RNNs  are  a  special  case  of  FNNs.In  fact,  FNNs are  a  special  case  of  finite-time  RNNs,  comprising  those which happen to have no cycles." (Figure 1)',"Not all neural networks are realistic, in the sense that some are more readily deployed (either by the biological system, or by the hardware an ANN is deployed on). For example, some finite-time RNNs are realistic whereas their unrolled-in-space feed-forward counterparts are not. (Figure 2)","ResNets (and DenseNets) are examples of networks that resemble spatially unrolled RNNs and perform exceptionally well on task and brain-similarity benchmarks.",'Recurrence has many high-level advantages: it can perform arbitrarily "deep" computations by running longer in time, instead of in space, and it can choose to trade off accuracy for speed by truncating computation temporally (Figure 3)','Recurrence has been implicated in critical functions, including attention, predictive coding, learning temporal context, i.e., stimulus history, estimating dynamics, for "cleaning up" representations of both dynamic and static inputs, and for selecting visual targets in "active vision" (Figure 4)'],conclusions:["While the idea of hierarchical feedforward nets is attractive, we should shift to thinking about the power, simplicity, and flexibility of recurrent alternatives. In a similar vein, we shouldn't assume that recurrence is only there to support the heavy lifting done by feedforward responses","To move beyond the feedforward framework, we also need to find alternative to backprop through time, which is both computationally expensive and reduces recurrent graphs to large feedforward graphs"],other:["I like the framing of these arguments a lot and generally agree with the points made. However, for me to be really convinced that I need to start using RNNs, there needs to be 1) a dramatic improvement in how easy they are to train and implement, and 2) they need to demonstrate superiority on a strongly-constraining task"]},metadata:{authors:["Ruben S. van Bergen","Nikolaus Kriegeskorte"],institutions:["Columbia University"],keywords:["recurrence","neural network"],title:"Going in circles is the way forward: the role of recurrence in visual inference",journal:"arXiv",doi:"",url:"https://arxiv.org/abs/2003.12128",date:"2020-04",review_date:"2020-04-02",one_sentence:"van Bergen and Kriegeskorte argue for recurrent neural networks as a promising path forward in understanding how brains and machines accomplish visual inference"}}},189:function(e){e.exports={review:{summary:["Alexnet trained on face recognition is the most generalizable and performant when trained first with blurred inputs, and then with high resolution inputs"],background:["Recent results suggest that there is a critical period for face learning: 1) monkeys raised without seeing faces do not develop patches of face-selective neurons and 2) children who have surgery to remove cataracts in the first year of life are often prosopagnosic later in life.  A domain-general (i.e., not face specific) account for these observations is lacking.","Newborns have very low visual acuity (20/600) due both to immaturity of the retina and of the visual cortex. In children with cataracts, acuity can improve despite opacity of the eyeball -- in one patient (RK) that had surgery at the age of 4.5, his initial acuity was 20/40","Computational work has demonstrated that local image patches are sufficient to discriminate images (e.g., BagNets); presumably this is not the case with blurry inputs (like those an infant may see). Blurry inputs may thus require circuits that integrate over extended spatial ranges, which sets up the kind of processing necessary for face recognition"],approach:["Train convolutional neural network (Alexnet modified for 100x100 input and layer 1 kernels increased from 11x11 to 22x22) with blurred training images and blurred testing images (varying amount of blur). Inspect performance of model and learned receptive fields. The input images are 50k faces from about 400 identities.","To estimate receptive field size and spatial frequency, fit ellipses to different lobes of the learned Gaussians and quantify size and separation of those ellipses. ",'Train some instances of the model on one of 4 regimens: "blurred-to-high-resolution"(250 epochs of training with blurred images followed by 250 epochs of training with high-resolution images);"high-resolution-to-blurred"(250 epochs of training with high-resolution images followed by 250 epochs of training with blur-red images);"blurred-to-blurred"(500 epochs with exclusively blurred training images); and "high-resolution-to-high-resolution"(500 epochs with exclusively high-resolution training images)."'],results:["When discriminating face images, smaller regions of the input are needed for high-resolution images than blurred images (Figure 1), indicating that blurred inputs require larger regions of spatial integration","Learned filters in the first layer become increasingly coarse-scale as inputs become blurrier (Figure 2) and receptive fields increase in size.","Training at one blur level yields poor generalization at other blur levels (Figure 2C), implying that the low-spatial-frequency info learned when training at extreme blur doesn't help with general face features at higher resolutions. Instead, the network likely shifts learning high SF features to other layers?","If high-res follows blur in the training schedule, RF sizes stay large. However, if blur follows high-res, the RFs switch from small to large (Figure 3).","The best-performing model is the one trained first on blur, then on high-resolution images (as normally developing infants do)."],conclusions:["Low initial visual acuity may benefit a learning visual system by encouraging development of receptive fields that are generally useful and integrate over extended spatial fields.","The training regime proposed here might be a way to make NNs more generalizable in other tasks. I'm a bit skeptical of this claim, as this definition of generalizability isn't one that most computer vision researchers are concerned with (compared to e.g. generalizing to new tasks)"],other:['An assumption is made that 1) the input to the CNN is "akin to the output of the retina" and 2) that the first layer of the CNN is effectively V1-like. In my experience, neither of these claims is necessarily true across architectures.','Of course, small children do not "train" on exclusively face images. How do these results hold up to more diverse training settings, including those with natural images?']},metadata:{authors:["Vogelsang, L","Gilad-Gutnick, A","Ehrenberg, EC","Yonas, A","Diamond, S","Held, RM","Sinha, P"],institutions:["Massachusetts Institute of Technology","University of Zurich","Arizona State University"],keywords:["neural network","acuity","development","blur","faces"],title:"Potential downside of high initial visual acuity",journal:"Proceedings of the National Academy of Sciences of the United States of America",doi:"10.1073/pnas.1800901115",url:"https://www.pnas.org/content/pnas/115/44/11333.full.pdf",date:"2018-10",review_date:"2020-03-25",one_sentence:"Vogelsang et al. use CNN models of face recognition to argue that low initial visual acuity in infants has the advantage of setting up receptive fields to be able to integrate over larger spatial extents"}}},190:function(e){e.exports={metadata:{title:"Disruption of orientation tuning in visual cortex by artificially correlated neuronal activity",authors:["Weliky, M","Katz, LC"],institutions:["HHMI","Duke University"],date:"1997-04",journal:"Nature",doi:"10.1038/386680a0",url:"https://www.ncbi.nlm.nih.gov/pubmed/9109486",keywords:["ferret","V1","orientation","stimulation"],review_date:"2019-06-28",one_sentence:"Weliky and Katz demonstrate that the introduction of artifically correlated activity in the ferret optic nerve weakens orientation selectivity in cortex while leaving orientation maps intact"},review:{summary:["Stimulation of the optic nerve during development reduces orientation tuning and direction selectivity for single cells, but leaves the spatial pattern of orientation tuning across the cortical surface relatively intact"],background:["Orientation selectivity is apparent at birth, but that selectivity is weak relative to adults","Visual deprivation experiments suggest that visual experience is necessary for orientation map development, but it's unclear whether that's because visual experience provides general input (i.e., preserves # of action potentials) or because the input is structured and thus, instructive"],approach:["'Play in' competing patterns of activity to the visual system via artificial stimulation of the optic nerve in ferrets. Current delivered to a cuff was used to drive optic nerve activity","Stimulate between P25 and P35, then allow 7-10 days of natural visual experience (same as for control ferrets)","Extracellular, spike-sorted recordings in visual cortex in response to drifting bars at 18 orientations","Compute the orientation selectivity index (OSI), where higher values indicate more selectivity","Intrinsic optical imaging (ISI) used to evaluate the spatial pattern of orientation tuning"],results:["Compared to control ferrets (60% of cells significantly orientation selective), ferrets receiving cuff stimulation had significantly fewer orientation selective cells (17%). The decreases in orientation selectivity was accompanied by a decrease in direction selective (Figure 2)","The decrease in orientation selectivity was not due simply to decreased firing rate: in both control and stimulated animals, there was no correlation between orientation selectivity and peak firing rate (but could there be a difference in mean firing rate?)","In stimulated animals, no observed effects of cortical depth on orientation selectivity, although along an electrode penetration, OSI varied smoothly. In control animals however, OSI were significantly higher in superficial and deep cortical depths (Figure 3)","ON/OFF LGN responses appeared similar in control and stimulated animals, suggesting a cortical origin in the orientation selectivity deficits (Figure 4)","Despite changes in OSI, the pattern of orientation selectivity across cortex, including size, spacing, and pattern of iso-orientation domains, appears unaffected. The authors note that the signal magnitude in these imaging experiments is markedly reduced in stimulated animals (Figure 5)"],conclusions:["Replacing patterned spontaneous activity from retinal waves with artificially correlated activity disrupts the development of sharp orientation tuning","The persistence of the orientation maps might be due to the earlier origins of those maps, where thalamic projections begin to colonize layer 4 around P10.","One shortcoming of this technique is that it doesn't entirely replace ongoing endogenous activity, it simply adds to it in an artifical way. Still, this proves an effective technique for influencing the developing circuit"],other:["This work finds significantly higher OSI in superficial and deep layers than in middle layers, but Ringach et al., 2002 find uniformity in orientation selectivity across cortical depths. Is this due to species differences, metric differences (OSI vs. CV), stimulus differences, or recording differences?"]}}},191:function(e){e.exports={metadata:{title:"Vision and Cortical Map Development",authors:["White, LE","Fitzpatrick, D"],institutions:["Duke University"],date:"2007-10",journal:"Neuron",doi:"10.1016/j.neuron.2007.10.011",url:"https://www.cell.com/neuron/fulltext/S0896-6273(07)00773-8",keywords:["review","development","maps","critical period","orientation","direction","deprivation"],review_date:"2020-01-31",one_sentence:"This review article argues that visual experience plays a role not only in map maintenance, but in map formation in animals like ferret and cat"},review:{summary:['Nicely summarized by the authors: "visual experience may have a profoundly beneficial or detrimental influence over the formation of these maps in the developing visual cortex"'],background:["Functional maps can be thought of as belonging to two categories: 1) those that re-capitulate the topography in the LGN, e.g., ocular dominance and retinotopy, and 2) those that emerge 'de-novo' in V1, e.g., orientation and direction selectivity.","The mechanisms of map formation fall into roughly two bins: 1) those that depend on molecular gradients, and 2) those that depend on activity. Group 2 can be further divided into 2a) endogenous activity and 2b) patterned visual experience","It is generally thought that the molecular systems establish coarse maps which are the refined by visual experience, such that there is some residual plasticity in, e.g., orientation/OD/retinotopy maps, but they mostly don't change with normal experience. The authors argue this viewpoint is a little misguided and under-values the role of patterned activity on map formation itself"],approach:["This is a review article, but I'll keep track of the techniques mentioned here in the other points","Intrinsic optical imaging and electrophysiological assessment of young ferrets to investigate orientation and direction selectivity"],results:["While some structure is established in-utero, the majority of synapses in visual cortex are added during periods in which sensory experience influences the structure of neuronal activity in space and time! In macaque, for example, synapse density increases exponentially from the third trimester until two months of life. A consequence of this synaptogenesis appears to be an increase in the maturity of intrinsic connections, e.g., horizontal connections between functionally-similar columns","In ferrets (and probably other organisms), an orientation map is weakly detectable at eye opening, but sharpens substantially thereafter (see my review of Chapman et al. for more). This sharpening is associated with changes to the distribution of GABA and NMDA receptors, so it is hypothesized that excitatory/inhibitory balance (and its associated molecular mechanisms) may explain critical-period sensitivity and precritical-period sensitivity.","Direction-selectivity maps were not readily apparent at birth (while OD and orientation maps were), indicating that different functional maps emerge at different timepoints. Interestingly, following the first detection of the maps (about a week after eye-opening), they rapidly strengthen such that they reach maturity at roughly the same time as other maps. (Figure 2)",'In dark-reared ferrets, the map of orientation tuning does not strengthen, implicating the role of experience (White et al., 2001). This cannot be explained by activity levels, as ferrets "seeing" through closed eyelids likely have higher levels of cortical activity, but the input distribution is low-passed and destroys map structure (Figure 3)','Dark rearing permanently disrupts direction-selectivity but not orientation or OD maps, suggesting that "dark rearing deprives the developing visual system of an important source of spatial, temporal, and/or luminance cues that are required for the formation of direction-ally selective circuits in cortical networks"','The asymmetry in sensitivity of orientation and direction selectivity maps may be due in part to maturing LGN receptive fields and sharpening response latencies. These properties may be sufficient for a weak orientation tuning maps, but insufficient for establishment of direction selectivity. The maturing cortex may "instruct" the LGN and bias spatiotemporal RF properties there to meet the needs of orientation and direction computation in V1'],conclusions:['As much as I hate copy-pasting, this is a really good summary: "Patterns of neural activity driven by sensorye xperience were considered necessary only in later stages of map maintenance when previously established map structures are sustained and fine-tuned according to the demands imposed by the functional ecology of the organism interacting with its environment. However, this framework for understanding functional maps in visual cortex is difficult to reconcile with the explosive phase of circuit construction that ensues after the onset of sensory experience and several important characteristics of orientation- and direction-map development"',"Experience can be either beneficial or detrimental: no input is probably better than bad input (also supported by bilateral cataract studies in human)",'Single unit recordings would go a long way in figuring out how these maps form; are there "pioneer" neurons that establish the columnar nature? If so, how does experience alter their selectivity?'],other:["I really like this paper and think it does an outstanding job of constructing a theoretical framework for the LGN and V1, but I'm left wondering how these results may generalize to extended plasticity in higher visual regions."]}}},192:function(e){e.exports={metadata:{title:"High-field fMRI unveils orientation columns in humans",authors:["Yacoub, E","Harel, N","Ugurbil, KC"],institutions:["University of Minnesota CMRR"],date:"2008-07",journal:"PNAS",doi:"10.1073/pnas.0804110105",url:"http://dx.doi.org/10.1073/pnas.0804110105",keywords:["human","fmri","v1","orientation","7t"],review_date:"2020-01-08",one_sentence:"Yacoub et al. demonstrate ultra-high-resolution imaging of orientation columns in human visual cortex"},review:{summary:["Orientation columns can be detected with reasonably reliability in human primary visual cortex","In both subjects measured, there was an over-representation of vertical orientations (90 degrees)"],background:["While the vast majority of fMRI studies over the past two decades have been conducted on 3-Tesla MRI machines, recent advances in scanning hardware and software have made 7-Tesla scanners viable for human functional MRI","Columns of orientation-selective cells, spanning less than a millimeter along the cortical surface, have been observed in macaque V1, but had not been demonstrated in humans in-vivo prior to this work. However, standard-resolution fMRI had been used to decode orientations from a population of visual cortex voxels.","A separate columnar system, that for ocular dominance, had been demonstrated with fMRI previously, but ocular dominance columns are 1) slightly larger than orientation columns and 2) are identifiable based on anatomy, i.e., staining in a way that orientation columns are not"],approach:["Perform ultra-high-resolution acquisition of human V1 responses to oriented stimuli, with stimuli presented to either eye separately to allow exploration of both ocular dominance and orientation tuning in the same subjects","Measure maps in separate sessions to assess reliability, and also vary stimulus presentation to assess reliability"],results:["In addition to ocular dominance columns, orientation-selective columns were also found in human V1 (Figure 1). Eshed's note: after staring at orientation maps from many different species, these look notably noisier and less periodic than those seen in animals","Pinwheels were identifiable as regions where multiple orientation preferences spatially converged in a radial fashion (Figures 2 and 3)","Iso-orientation contours tended to cross ocular-dominance columns at right angles, as reported previously in other animals (Figures 2 and 3)","Orientation maps were kind of consistent between odd and even runs (r = 0.62 and 0.66 for two subjects), and not-that consistent when stimulus presentation order changed (r = 0.4).","An 'oblique effect' is observed in these data, whereby more voxels are tuned to 0 degrees and 90 degrees than to other orientations (Figure 5a). Note: the authors write that p = 0.07 is significant?","The authors make a case that even if the point-spread-function of the BOLD response is larger than the 1-mm columns we're attempting to detect, such structures can be resolved if the PSF is uniform across the imaging area (Figure 6)"],conclusions:["Orientation columns exist in human V1 and are organized into orderly maps, and there is an over-representation of vertical orientations","This study should open the door for more investigation of sub-mm resolution structures in human fMRI"],other:[]}}},193:function(e){e.exports={metadata:{authors:["Zhuang, C","Zhai, AL","Yamins, DLK"],institutions:["Stanford University"],keywords:["unsupervised learning","neural network","machine learning"],title:"Local Aggregation for Unsupervised Learning of Visual Embeddings",journal:"arXiv / ICCV",doi:"",url:"https://arxiv.org/abs/1903.12355",date:"2019-04",review_date:"2019-11-09",one_sentence:"Zhuang and colleagues demonstrate state-of-the-art performance from unsupervised features by embedding natural images into a lower-dimensional space where similar images are 'soft clustered' into groups at a variety of spatial scales"},review:{summary:["Local aggregation (LA) aims to embed images into a low-dimensional space using a neural network as the embedding function. After the embedding function is learned, such that a local aggregation metric (described later) is optimized, that network can achieve very high performance on the ImageNet task (60% accuracy [top-1])"],background:["Supervised learning is an effective way to train networks but 1) requires way too many labeled examples for arbitrary tasks, i.e., we don't have the ImageNet dataset for everything, and 2) is not at all how humans and other animals learn","Unsupervised or semi-supervised methods can fill that gap, but their performance has historically been poor. One of the most promising previous approaches is instance recognition, where each image is treated as a distinct category, and different categories are embedded to maximize distance between them. ","The downside of instance recognition is that it separates things that _should_ be close together, so a different strategy is to iteratively assign images to clusters based on similarity, and to use those cluster assignments as new labels during the next batch of training. This approach suffers from potentially failing to separate images within a cluster."],approach:['Local aggregation aims to balance clustering and separation by identifying, for a given embedding function, the "close neighbors" and "background neighbors" of an image, then brings close neighbors closer (using background neighbors to calibrate what \'closer\' means; Figure 1)',"For dataset size and batch issues, a memory bank is used to store image vectors from batch to batch. This technique is introduced in the instance recognition work.","For robustness, multiple clusterings are performed per iteration, and results are aggregated across those clusterings"],results:["For four different architectures (Alexnet, VGG16, ResNet-{18,50}), local aggregation achieves better transfer learning than any other unsupervised method, peaking at 60.2% top-1 performance for the largest network (Table 1)","LA also achieves state of the art on scene-categorization and object-detection (Tables 2 and 3)","LA works by effectively increasing local density while keeping the embedding globally sparse (Figure 2)","Inspection of nearest neighbors of images (whether ultimately recognized correctly or not) looks qualitatively strong: visual similarity and image context in embedding neighbors is very high (Figure 3)"],conclusions:["LA is the strongest unsupervised learning algorithm to date, and is generally enough that it could easily be applied to other domains, including video and audio (as the authors plan to do)","It remains to be seen to what extent something like LA is done in the developing brain... If what we want is a biologically-realistic unsupervised learning algorithm, could LA be a reasonable description?"],other:["Disclaimer: this is work from one of the labs I'm in, so I'm biased to have a more favorable view on it. I do think it's really outstanding work, and I'm excited to see how it can be improved and applied to biological data."]},createdAt:{$date:{$numberLong:"1573313945319"}},updatedAt:{$date:{$numberLong:"1573313945319"}},__v:{$numberInt:"0"}}},242:function(e){e.exports={projects:[{name:"Modeling the Functional Architecture of the Visual System",url:"img/margalit_cosyne_poster_2020.png",img_src:"spacenet.png",alt:"Orientation tuning map from a computational model of primary visual cortex"},{name:"Ultra-high-resolution imaging of human higher visual cortex",url:"https://www.jneurosci.org/content/early/2020/02/21/JNEUROSCI.2106-19.2020",img_src:"lateral_vtc_rdm.png",alt:"Representational dissimilarity matrix"},{name:"A V1-like Model for Object Discrimination",url:"http://geon.usc.edu/GJW/",img_src:"gabor_grab.PNG",alt:"Screengrab from application designed to explore gabor-jet model of image processing."},{name:"Interactive Signal Detection Theory Tutorial",url:"#/DPCalc",img_src:"dp_gui_screenshot.png",alt:"Screenshot of interactive webpage describing signal detection theory"}]}},245:function(e){e.exports={CV:{last_updated:"July 2019",pdf:{path:"2019.July.CV.pdf"},awards:[{name:"NSF Graduate Research Fellowship Program (GRFP) Fellow",dates:"2016 - 2021",description:"NSF fellowship recognizes and supports outstanding graduate students in NSF-supported science, technology, engineering, and mathematics disciplines",url:"https://www.nsfgrfp.org/"},{name:"USC Trustee's Award",dates:"2016",description:"Awarded to the undergraduate male with the highest GPA at the University",url:""},{name:"Graduate Trainee, Stanford Mind, Brain, Computation, and Technology",dates:"2018",description:"Traineeship awarded for the pursuit of multi-disciplinary research in the area of computational neuroscience",url:""},{name:"USC Neuroscience Outstanding Student of the Year",dates:"2016",description:"Awarded to USC's best neuroscience student with senior standing",url:"http://dornsife.usc.edu/usc-neuroscience/"},{name:"USC Brian Philip Rakusin Neuroscience Scholarship Award",dates:"2015",description:"Awarded to USC's best neuroscience student with junior standing",url:"http://dornsife.usc.edu/usc-neuroscience/"},{name:"Phi Beta Kappa Honor Society",dates:"2015",description:"Academic honor's society",url:"https://www.pbk.org/"},{name:"USC Discovery Scholar",dates:"2016",description:"Awarded to students who excel in the classroom while demonstrating the ability to create exceptional new scholarship.",url:"http://ahf.usc.edu/scholars/discovery/"},{name:"USC Provost's Undergraduate Research Fellowship",dates:"2013 - 2016",description:"6-time recipient of award supporting exceptional undergraduate research",url:"https://undergrad.usc.edu/experience/research/undergrad_research/"},{name:"George H. Mayr Scholarship",dates:"2015",description:"Awarded to outstanding students from California in the college of letters, arts, and sciences",url:""},{name:"USC S.O.A.R. Grant",dates:"2015",description:"Provides funding to Dornsife undergraduates for participation as a research assistant in a faculty member\u2019s project",url:"https://dornsife.usc.edu/soar"},{name:"USC Dean's Scholar",dates:"2012 - 2016",description:"Quarter tuition scholarship",url:"http://dornsife.usc.edu/trustee-presidential-deans-scholarships"}],publications:{published:[{title:"Neuromelanin marks the spot: Identifying a locus coeruleus biomarker of cognitive reserve in healthy aging",authors:["Clewett, D.","Lee, T.H.","Greening, S.G.","Ponzio, A.","Margalit, E.","Mather, M."],journal:"Neurobiology of Aging",year:"2016",volume:", 37",pages:", 117-126",url:"http://www.neurobiologyofaging.org/article/S0197-4580(15)00476-5/abstract",tag:"noa-2016-neuromelanin"},{title:"The lateral occipital complex shows no net response to object familiarity",authors:["Margalit, E.","Shah, M.P.","Tjan, B.S.","Biederman, I.","Keller, B.","Brenner, R."],journal:"Journal of Vision",year:"2016",volume:", 16(3)",url:"https://jov.arvojournals.org/article.aspx?articleid=2551659",tag:"jov-2016-thelateral"},{title:"An applet for the Gabor scaling of the differences between complex stimuli",authors:["Margalit, E.","Herald, S.B.","Yue, X.","von der Malsburg, C.","Biederman, I."],journal:"Attention, Perception, and Psychophysics",year:"2016",volume:", 78(8)",pages:", 2298-2306",url:"https://link.springer.com/article/10.3758/s13414-016-1191-7",tag:"app-2016-anapplet"},{title:"What is actually affected by the scrambling of objects when localizing the lateral occipital complex?",authors:["Margalit, E.","Biederman, I.","Tjan, B.S.","Shah, M.P."],journal:"Journal of Cognitive Neuroscience",year:"2017",volume:", 20(9)",pages:", 1595-1604",url:"https://www.mitpressjournals.org/doi/abs/10.1162/jocn_a_01144",tag:"jocn-2017-whatis"},{title:"The cognitive neuroscience of person identification",authors:["Biederman, I.","Shilowich, B.E.","Herald, S.B.","Margalit, E.","Maarek, R.","Meschke, E.X.","Hacker, C.M."],journal:"Neuropsychologia",year:"2018",volume:", 116B",pages:", 205-214",url:"https://www.sciencedirect.com/science/article/pii/S0028393218300423",tag:"neuropsychologia-2018-thecognitive"},{title:"A critical assessment of data quality and venous effects in sub-millimeter fMRI",authors:["Kay, K.N.","Jamison, K.","Vizioli, L.","Zhang, R.","Margalit, E.","Ugurbil, K."],journal:"NeuroImage",year:"2019",volume:", 189",pages:", 847-869",url:"https://www.sciencedirect.com/science/article/pii/S1053811919300928",tag:"neuroimage-2019-acritical"},{title:"Visual noise consisting of X-junctions has only a minimal adverse effect on object recognition",authors:["Margalit, E.","Herald, S.B.","Meschke, E.X.","Irawan, I.","Maarek, R.","Biederman, I."],journal:"Attention, Perception, & Psychophysics",year:"2019",volume:"",url:"https://link.springer.com/article/10.3758%2Fs13414-019-01840-2",tag:"atten-percep-2019-xvert"},{title:"Ultra-high-resolution fMRI of human ventral temporal cortex reveals differential representation of categories and domains",authors:["Margalit, E.","Jamison, K.W.,","Weiner, K.S.","Vizioli, L.","Zhang, R.","Kay, K.N.","Grill-Spector, K."],journal:"Journal of Neuroscience",year:"2020",volume:", in press",url:"https://www.jneurosci.org/content/early/2020/02/21/JNEUROSCI.2106-19.2020",tag:"jn-2020-7tfloc"}],preprints:[]},presentations:{posters:[{title:"Pinwheel-like Iso-Orientation Domains in a Convolutional Neural Network Model",authors:["Margalit, E.","Lee, H.","DiCarlo, J.J.","Yamins, D.L.K."],journal:"Annual Meeting of the Vision Sciences Society (VSS)",year:"2018",url:"img/Margalit_et_al_VSS_2018.png",tag:"vss-2018-pinwheel"},{title:"What is the nature of the perceptual deficit in congenital prosopagnosia?",authors:["Biederman, I.","Margalit, E.","Maarek, R.","Meschke, E.X.","Shilowich, B.E.","Hacker, C.M.","Juarez, J.J.","Seamans, T.J.","Herald, S.B."],journal:"Annual Meeting of the Vision Sciences Society",year:"2017",url:"http://geon.usc.edu/~biederman/presentations/PerceptualDeficit_VSS_17.pdf",tag:"vss-2017-whatis"},{title:"What is actually affected by the scrambling of objects when localizing the lateral occipital complex?",authors:["Biederman, I.","Margalit, E.","Tjan, B.S.","Shah, M.P."],journal:"Annual Meeting of the Society of Experimental Psychologists",year:"2016",url:"",tag:"sep-2016-whatis"},{title:"Vertices are effective in perceptual grouping (and ungrouping)",authors:["Irawan, I.","Margalit, E.","Herald, S.B.","Biederman, I."],journal:"Annual Meeting of the Vision Sciences Society",year:"2016",url:"http://geon.usc.edu/~biederman/presentations/Irawan_et_al_VSS_16.pdf",tag:"vss-2016-vertices"},{title:"Impaired face and non-face discrimination in developmental prosopagnosics (DPs)",authors:["Margalit, E.","Yue, X.","Biederman, I."],journal:"Annual Meeting of the Vision Sciences Society",year:"2016",url:"",tag:"vss-2016-impaired"},{title:"What is actually affected by the scrambling of objects when localizing LOC? (Talk)",authors:["Biederman, I.","Margalit, E.","Tjan, B.S.","Shah, M.P."],journal:"Annual Meeting of the Vision Sciences Society",year:"2016",url:"",tag:"vss-2016-whatis"},{title:"Neuromelanin marks the spot: a locus coeruleus substrate of cognitive reserve in healthy aging",authors:["Clewett, D.","Lee, T.H.","Greening, S.G.","Ponzio, A.","Margalit, E.","Mather, M."],journal:"USC Neuroscience Graduate Student Symposium",year:"2015",url:"",tag:"usc-2015-neuromelanin"},{title:"Phonagnosia, a voice homologue to prosopagnosia",authors:["Biederman, I.","Herald, S.B.","Xu, X.","Amir, O.","Shilowich, B.E.","Margalit, E."],journal:"Annual Meeting of the Vision Sciences Society",year:"2015",url:"http://geon.usc.edu/~biederman/presentations/BiedermanEtAl_VSS2015.pdf",tag:"vss-2015-phonagnosia"}],talks:[{title:"Differential representation of object category information across lateral and medial ventral temporal cortex revealed with ultra-high-resolution fMRI",authors:["Margalit, E.","Jamison, K.","Weiner, K.S.","Vizioli, L.","Zhang, R.","Kay, K.N.","Grill-Spector, K."],journal:"Society for Neuroscience Annual Conference (SfN)",year:"2018",url:"https://www.abstractsonline.com/pp8/#!/4649/presentation/41470",tag:"sfn-2018-differential"},{title:"Ultra-high-resolution fMRI reveals differential representation of categories and domains across lateral and medial ventral temporal cortex",authors:["Margalit, E.","Jamison, K.","Weiner, K.S.","Vizioli, L.","Zhang, R.","Kay, K.N.","Grill-Spector, K."],journal:"Annual Meeting of the Vision Sciences Society (VSS)",year:"2019",url:"",tag:"vss-2019-7t"}]}}}},246:function(e){e.exports={projects:[{name:"Modeling the Functional Architecture of the Visual System",description:"The visual system is composed of neurons embedded in detailed topographic maps. Although the past few years have seen huge advances in our ability to predict the activity of neurons in these maps with deep neural networks, models of the structure of the maps themselves is lacking. We're using deep convolutional neural networks to better understand 1) the role of this functional architecture and 2) how it might develop in the brain.",img_src:"img/spacenet.png",url:"img/margalit_cosyne_poster_2020.png",url_text:"COSYNE 2020 Poster (8MB)",alt:"Orientation tuning map from a computational model of primary visual cortex",id:"func-arch"},{name:"Characterizing the Fine Scale Functional Architecture of the Ventral Temporal Cortex",description:"Recent advances in fMRI technology have allowed insights into the functional organization of cortical regions at unprecented resolution. In collaboration with the CVN Lab at the University of Minnesota, we are examining areas of the brain that preferentially respond to different object categories and studying their arrangement in the cortical sheet.",img_src:"img/vtc_faces.png",url:"https://www.jneurosci.org/content/early/2020/02/21/JNEUROSCI.2106-19.2020",url_text:"Read the paper!",alt:"Category tuning maps from ultra-high-res human fMRI",id:"hires"},{name:"Interactive Web Application for Gabor-jet Model",description:"The Gabor-jet model is a tool used to compute the psychophysical dissimilarity between images: an objective metric of how dissimilar two images appear. The model predicts over 90% of the variance in human responses on a match-to-sample task with artificial face stimuli, making it an invaluable tool in psychophysical research. I designed a webpage to act as an interactive guided tour of the model, allowing users to upload and test their own stimuli in-browser. Try it yourself!",img_src:"img/gabor_grab.PNG",url:"http://geon.usc.edu/GJW",url_text:"Online Gabor Filtering App",alt:"Screenshot from online Gabor filtering app",id:"web-app"},{name:"Object Familiarity in LOC",description:"Hundreds of studies have explored the Lateral Occipital Complex (LOC), which is critical for shape perception. Early studies discounted a role of familiarity by showing that \u201cabstract\u201d sculptures, unfamiliar to the subjects, also activated this region. This characterization of LOC as a region that responds to shape independently of familiarity had been accepted but never tested with control of the same low-level features. We assessed LOC\u2019s response to objects that had identical parts in two different arrangements, one familiar and the other novel. The early work was correct: there is no net effect of familiarity in LOC, although multivariate classifiers suggest that LOC does distinguish familiar from novel objects.",img_src:"img/LOC_mricrogl.png",url:"http://jov.arvojournals.org/article.aspx?articleid=2551659",url_text:"Article in Journal of Vision",alt:"Brain representation showing regions activated more by unfamiliar than familiar images",id:"loc-fam"},{name:"Developmental Prosopagnosia",description:"Developmental prosopagnosics (DPs) present no lesions nor have a history of compromised neural functioning. Given that their activation of face-selective cortex is normal, it surprised us that their capacity to perceptually discriminate faces and non-face objects had never been rigorously assessed. Normal discrimination of faces would suggest that the underlying deficit might not be a consequence of a poor perceptual representation but, instead, difficulty in matching a well-defined representation to stored representations in memory. If a deficit in discriminating faces is observed, is it also manifested when discriminating non-face stimuli that differ along the same underlying physical attributes as faces and to an equivalent extent as the faces? We found that, indeed, DPs present deficits in discriminating both faces and tooth-like blobs, but that this deficit does not extend to simple geometric primitives.",img_src:"img/dissim_histogram.png",url:"http://geon.usc.edu/~biederman/presentations/Margalit_et_al_VSS_16.pdf",url_text:"VSS Poster",alt:"Histogram of image dissimilarity for two image classes used to explore face processing",id:"dp-hist"}]}},284:function(e,t,a){e.exports=a(499)},289:function(e,t,a){},291:function(e,t,a){},295:function(e,t,a){},297:function(e,t,a){},299:function(e,t,a){var i={"./LOC_mricrogl.png":121,"./Margalit_et_al_2017_Fig3.png":122,"./Margalit_et_al_VSS_2018.png":123,"./SU_Seal_Red.png":75,"./USC_Seal.png":76,"./clouds_rest.png":71,"./dissim_histogram.png":124,"./dp_gui_screenshot.png":125,"./dp_table.png":77,"./emcog.jpg":78,"./gabor_grab.PNG":126,"./geon_brain.png":79,"./headshot-sq.png":80,"./lateral_vtc_rdm.png":127,"./snail-sq.png":45,"./spacenet.png":128,"./vpnl.png":46,"./vtc_faces.png":129,"./vtc_fade.gif":74};function n(e){var t=r(e);return a(t)}function r(e){var t=i[e];if(!(t+1)){var a=new Error("Cannot find module '"+e+"'");throw a.code="MODULE_NOT_FOUND",a}return t}n.keys=function(){return Object.keys(i)},n.resolve=r,e.exports=n,n.id=299},303:function(e,t,a){},305:function(e,t,a){var i={"./img/LOC_mricrogl.png":121,"./img/Margalit_et_al_2017_Fig3.png":122,"./img/Margalit_et_al_VSS_2018.png":123,"./img/SU_Seal_Red.png":75,"./img/USC_Seal.png":76,"./img/clouds_rest.png":71,"./img/dissim_histogram.png":124,"./img/dp_gui_screenshot.png":125,"./img/dp_table.png":77,"./img/emcog.jpg":78,"./img/gabor_grab.PNG":126,"./img/geon_brain.png":79,"./img/headshot-sq.png":80,"./img/lateral_vtc_rdm.png":127,"./img/snail-sq.png":45,"./img/spacenet.png":128,"./img/vpnl.png":46,"./img/vtc_faces.png":129,"./img/vtc_fade.gif":74};function n(e){var t=r(e);return a(t)}function r(e){var t=i[e];if(!(t+1)){var a=new Error("Cannot find module '"+e+"'");throw a.code="MODULE_NOT_FOUND",a}return t}n.keys=function(){return Object.keys(i)},n.resolve=r,e.exports=n,n.id=305},37:function(e,t,a){"use strict";a.r(t);var i=["koch_pinwheels.json","koulakov_chklovskii_wiring_cost.json","livingstone_hubel_color.json","nauhaus_orthogonality.json","ponce_livingstone_super_stimuli.json","ringach_orientation_selectivity.json","hubel_wiesel_v2v3.json","dicarlo_untangling.json","olshausen_field_sparse_coding.json","kriegeskorte_monkey_human.json","grill_spector_functional_architecture.json","devalois_direction_selectivity.json","ackman_retinal_waves.json","afraz_inactivation_face_gender.json","rajalingham_inactivation.json","bracci_ventral_shape.json","jacobs_jordan_short_connections.json","weliky_katz_disruption.json","garg_color_orientation.json","kim_pasupathy_texture_shape_v4.json","haxby_hyperalignment.json","sato_mosaic.json","arcaro_universal_mechanisms.json","op_de_beeck_factors.json","jang_orientation_classification.json","ohki_pinwheel_order.json","chapman_orientation_development.json","fahey_mouse_global_orientation.json","otoole_face_dcnn.json","durbin_mitchison_som.json","linsker_spatial_opponent.json","khalig-razavi_dnn.json","gunthner_divisive_normalization.json","kietzmann_recurrence_rda.json","datta_computational_neuroethology.json","bartoldson_pruning.json","devalois_spatial_frequency.json","zhuang_local_aggregation.json","smith_v1_correlations.json","arcaro_body_protomap.json","dahne_retinal_wave_sfa.json","gribizis_retinal_waves.json","sharma_ferret_rewiring.json","ringach_tuning_dynamics.json","crair_pinwheel_od.json","yacoub_orientation_columns.json","achille_critical_period_dnn.json","crair_binocular_deprivation.json","white_fitzpatrick_map_development.json","daw_critical_periods.json","nauhaus_efficient_tiling.json","albert_retinal_waves.json","chen_simclr.json","kunin_local_rules.json","hu_curvature.json","murty_haptic_blind.json","vogelsang_acuity.json","vanBergen_recurrence.json"].map(function(e){return"review_jsons/"+e});t.default=i},45:function(e,t,a){e.exports=a.p+"static/media/snail-sq.3afa244f.png"},46:function(e,t,a){e.exports=a.p+"static/media/vpnl.c5822a31.png"},491:function(e,t,a){},493:function(e,t,a){},495:function(e,t,a){},499:function(e,t,a){"use strict";a.r(t);var i=a(2),n=a.n(i),r=a(25),o=a.n(r),s=(a(289),a(9)),l=a(10),c=a(13),h=a(11),u=a(12),d=a(131),m=a(81),p=a(3),g=a(7),f=a(73),y=(a(291),a(45)),v=a.n(y),w=a(46),b=a.n(w),k=a(71),E=a.n(k),_=[{link:"https://scholar.google.com/citations?user=ijttsicAAAAJ&hl=en&oi=ao",text:"Google Scholar",icon:n.a.createElement(g.n,{color:"#8c1313",size:"40px"})},{link:"https://github.com/eshedmargalit",text:"GitHub",icon:n.a.createElement(g.l,{color:"#8c1313",size:"40px"})},{link:"https://twitter.com/eshedmargalit",text:"Twitter",icon:n.a.createElement(g.v,{color:"#8c1313",size:"40px"})},{link:"https://www.linkedin.com/in/eshed-margalit-437222a7",text:"LinkedIn",icon:n.a.createElement(g.r,{color:"#8c1313",size:"40px"})}],x=function(e){function t(){return Object(s.a)(this,t),Object(c.a)(this,Object(h.a)(t).apply(this,arguments))}return Object(u.a)(t,e),Object(l.a)(t,[{key:"render_buttons",value:function(){var e=_.map(function(e){return n.a.createElement(p.e,{lg:"3",xs:"3",key:e.link},n.a.createElement(p.b,{className:"hover-button",color:"link"},n.a.createElement("a",{href:e.link,target:"_blank",rel:"noopener noreferrer"},e.icon)))});return n.a.createElement(p.u,null,e)}},{key:"render",value:function(){return n.a.createElement(p.g,{className:"vertical-align"},n.a.createElement(p.u,{className:"vertical-align"},n.a.createElement(p.e,{xs:"8",lg:"4"},n.a.createElement("h1",null,"Eshed Margalit"),n.a.createElement("h5",null,"PhD Candidate in the"," ",n.a.createElement("a",{href:"https://med.stanford.edu/neurogradprogram.html"},"Stanford Neurosciences Graduate Program")),n.a.createElement("hr",null),n.a.createElement(p.g,null,n.a.createElement(p.u,null,n.a.createElement(p.e,{xs:"8",lg:"7"},n.a.createElement("a",{href:"http://neuroailab.stanford.edu/"},n.a.createElement("h6",null,"Stanford NeuroAI Lab")),n.a.createElement("p",null,"PI: Dan Yamins")),n.a.createElement(p.e,{xs:"4",lg:"5"},n.a.createElement("img",{src:v.a,alt:"SNAIL Logo",className:"full-width"}))),n.a.createElement(p.u,null,n.a.createElement(p.e,{xs:"8",lg:"7"},n.a.createElement("a",{href:"http://vpnl.stanford.edu/"},n.a.createElement("h6",null,"Stanford Vision & Perception Neuroscience Lab")),n.a.createElement("p",null,"PI: Kalanit Grill-Spector")),n.a.createElement(p.e,{xs:"4",lg:"5"},n.a.createElement("img",{src:b.a,alt:"VPNL Logo",className:"full-width"})))),n.a.createElement("hr",null),n.a.createElement(f.a,{to:"/CV"},n.a.createElement(p.b,{outline:!0,color:"primary"},"CV"))," ",n.a.createElement(f.a,{to:"/Research"},n.a.createElement(p.b,{outline:!0,color:"primary"},"Research"))," "),n.a.createElement(p.e,{xs:"8",lg:"4"},n.a.createElement(p.g,null,n.a.createElement(p.u,null,n.a.createElement(p.e,{xs:"12",lg:"12"},n.a.createElement("img",{src:E.a,alt:"Headshot of Eshed",className:"circle full-width",id:"clouds-rest",href:"#"}),n.a.createElement(p.v,{placement:"top",target:"clouds-rest"},"This is me on a hike to Cloud's Rest in Yosemite, CA!"),n.a.createElement("p",{className:"center-text anonymous"},"eshed.margalit [at] gmail [dot] com"))))),n.a.createElement(p.e,{xs:"8",lg:"4"},n.a.createElement(p.g,null,n.a.createElement(p.u,null,n.a.createElement(p.e,{xs:"12",lg:"12"},n.a.createElement("hr",null),n.a.createElement("p",null,"I'm a neuroscience graduate student with an interest in vision, computational neuroscience, artificial intelligence, and neuroimaging! To learn more or to see some of my past work, please check out my"," ",n.a.createElement(f.a,{to:"/Research"},"research interests"),", or my"," ",n.a.createElement(f.a,{to:"/CV"},"CV"),". I also run the"," ",n.a.createElement("a",{href:"http://stanford-cnjc.github.io"},"Stanford Computational Neuroscience Journal Club"),", check it out!"))),this.render_buttons(),n.a.createElement("hr",null)))))}}]),t}(i.Component),j=(a(295),a(74)),C=a.n(j),T=function(e){function t(){return Object(s.a)(this,t),Object(c.a)(this,Object(h.a)(t).apply(this,arguments))}return Object(u.a)(t,e),Object(l.a)(t,[{key:"render",value:function(){return n.a.createElement(p.g,null,n.a.createElement(p.k,null,n.a.createElement(p.u,null,n.a.createElement(p.e,{xs:"12",lg:"12"},n.a.createElement("h3",null,"How does the brain make sense of the things we see?"),n.a.createElement("p",null,"I study the functional architecture of the primate visual system, with an interest in describing and modeling the computations that govern visual cortex and the representations that they operate on. I hope to contribute to our understanding of how information is transformed in neural networks and how such networks develop."),n.a.createElement(f.a,{to:"/Research"},n.a.createElement(p.b,{size:"lg",color:"secondary"},"My Research"))," ",n.a.createElement(f.a,{to:"/CV"},n.a.createElement(p.b,{size:"lg",color:"secondary"},"My CV")))),n.a.createElement("hr",null),n.a.createElement(p.u,null,n.a.createElement(p.e,{xs:"6",lg:"4"},n.a.createElement("img",{src:C.a,className:"full-width",alt:"GIF cycling through fMRI data from human ventral temporal cortex"})),n.a.createElement(p.e,{xs:"12",lg:"6"},n.a.createElement("p",null,"Stimulus preference maps in human higher visual cortex acquired at sub-millimeter resolution. The animation cycles through preference maps for four object categories: faces, places, words, and bodies. Can you tell which is which?")))))}}]),t}(i.Component),S=(a(297),a(242)),I=function(e){function t(){var e,i;Object(s.a)(this,t);for(var r=arguments.length,o=new Array(r),l=0;l<r;l++)o[l]=arguments[l];return(i=Object(c.a)(this,(e=Object(h.a)(t)).call.apply(e,[this].concat(o)))).render_featured_projects=function(e){return e.map(function(e){var t=e.url,i=e.alt,r=a(299)("./".concat(e.img_src));return n.a.createElement(p.e,{xs:"6",lg:"3",key:e.name},n.a.createElement("a",{href:t},n.a.createElement("img",{src:r,className:"full-width project",alt:i}),n.a.createElement("p",null,e.name)))})},i}return Object(u.a)(t,e),Object(l.a)(t,[{key:"render",value:function(){return n.a.createElement(p.g,null,n.a.createElement(p.u,null,n.a.createElement(p.e,{xs:"12",lg:"12"},n.a.createElement("h3",null,"My recent work at a glance"))),n.a.createElement(p.u,null,this.render_featured_projects(S.projects)))}}]),t}(i.Component),F=function(e){function t(){return Object(s.a)(this,t),Object(c.a)(this,Object(h.a)(t).apply(this,arguments))}return Object(u.a)(t,e),Object(l.a)(t,[{key:"render",value:function(){return n.a.createElement(i.Fragment,null,n.a.createElement("br",null),n.a.createElement(x,{className:"left-align"}),n.a.createElement("br",null),n.a.createElement(p.g,null,n.a.createElement("hr",null)),n.a.createElement(T,{className:"left-align"}),n.a.createElement(p.g,null,n.a.createElement("hr",null)),n.a.createElement(I,{className:"left-align"}))}}]),t}(i.Component),M=a(21),N=a(54),V=function(e){function t(e){var a;return Object(s.a)(this,t),(a=Object(c.a)(this,Object(h.a)(t).call(this,e))).toggle=a.toggle.bind(Object(M.a)(a)),a.state={isOpen:!1},a}return Object(u.a)(t,e),Object(l.a)(t,[{key:"toggle",value:function(){this.setState({isOpen:!this.state.isOpen})}},{key:"render",value:function(){return n.a.createElement("div",null,n.a.createElement(p.r,{color:"dark",dark:!0,expand:"md"},n.a.createElement(N.LinkContainer,{to:"/"},n.a.createElement(p.s,null,"Eshed Margalit")),n.a.createElement(p.t,{onClick:this.toggle}),n.a.createElement(p.f,{isOpen:this.state.isOpen,navbar:!0},n.a.createElement(p.o,{className:"ml-auto",navbar:!0},n.a.createElement(N.LinkContainer,{to:"/CV"},n.a.createElement(p.q,null,n.a.createElement(g.u,null)," ","CV")),n.a.createElement(N.LinkContainer,{to:"/Research"},n.a.createElement(p.q,null,n.a.createElement(g.f,null)," ","Research")),n.a.createElement(N.LinkContainer,{to:"/PaperReviews"},n.a.createElement(p.q,null,n.a.createElement(g.t,null)," ","Paper-a-Week")),n.a.createElement(N.LinkContainer,{to:"/DPCalc"},n.a.createElement(p.q,null,n.a.createElement(g.g,null)," ","d' Calculator"))))))}}]),t}(i.Component),z=a(244),A=function(e){function t(){return Object(s.a)(this,t),Object(c.a)(this,Object(h.a)(t).apply(this,arguments))}return Object(u.a)(t,e),Object(l.a)(t,[{key:"render",value:function(){return n.a.createElement(p.g,null,n.a.createElement(p.u,{className:"vertical-align"},n.a.createElement(p.e,{xs:"12",lg:"12"},n.a.createElement("hr",null),n.a.createElement("p",null,"Please get in touch, I'd love to hear from you."," ",n.a.createElement(g.j,{color:"",id:"gmail_address",size:"1.5em"}),n.a.createElement(p.v,{autohide:!1,placement:"top-end",target:"gmail_address"},"eshed.margalit@gmail.com"," ",n.a.createElement(p.b,{color:"link",size:"sm"},n.a.createElement(z.a,{className:"copy-src","data-clipboard-text":"eshed.margalit@gmail.com",size:"1.5em"}))," ")),n.a.createElement("p",null,"Website designed by Eshed, browse the source code on Github:",n.a.createElement(p.b,{color:"link",href:"http://github.com/eshedmargalit/eshedmargalit.github.io"},n.a.createElement(g.l,{size:"1.5em"}))))))}}]),t}(i.Component),q=a(245),R=(a(132),a(75)),O=a.n(R),P=a(76),D=a.n(P),B=a(80),G=a.n(B),L=a(79),H=a.n(L),U=a(78),W=a.n(U),K=a(18),J=a.n(K),Y=function(e){function t(){var e,a;Object(s.a)(this,t);for(var i=arguments.length,r=new Array(i),o=0;o<i;o++)r[o]=arguments[o];return(a=Object(c.a)(this,(e=Object(h.a)(t)).call.apply(e,[this].concat(r)))).render_author=function(e){return"Margalit, E."===e?n.a.createElement("strong",null,e):n.a.createElement("span",null,e)},a.render_authors=function(e){return e.map(function(t,i){var r,o=a.render_author(t);return r=i===e.length-1?1===e.length?n.a.createElement("span",null,o,n.a.createElement("br",null)):n.a.createElement("span",null,"and ",o,n.a.createElement("br",null)):i===e.length-2?n.a.createElement("span",null,o," "):n.a.createElement("span",null,o,","," "),n.a.createElement("span",{key:e.tag+t},r)})},a.render_pub=function(e){var t;return e.authors.tag=e.tag,t=""===e.url?null:n.a.createElement(p.b,{color:"link",target:"_blank",href:e.url},n.a.createElement(g.q,{size:"1.5em"})),n.a.createElement(p.n,null,n.a.createElement("p",null,e.year),n.a.createElement("h5",null,e.title),a.render_authors(e.authors),n.a.createElement("p",null,n.a.createElement("em",null,e.journal,e.volume,e.pages)),t)},a.render_pubs=function(e){return e.map(function(e){return n.a.createElement("span",{key:e.tag},a.render_pub(e))})},a}return Object(u.a)(t,e),Object(l.a)(t,[{key:"render",value:function(){return 0===this.props.pubs.length?null:(this.props.pubs.sort(function(e,t){return-J()(new Date(e.year)).diff(J()(new Date(t.year)))}),n.a.createElement(p.g,null,n.a.createElement(p.u,null,n.a.createElement(p.e,null,n.a.createElement(p.m,null,n.a.createElement("h4",null,this.props.name),this.render_pubs(this.props.pubs))))))}}]),t}(i.Component),Q=function(e){function t(e){var a;return Object(s.a)(this,t),(a=Object(c.a)(this,Object(h.a)(t).call(this,e))).toggle=a.toggle.bind(Object(M.a)(a)),a.state={collapse:e.default_open},a}return Object(u.a)(t,e),Object(l.a)(t,[{key:"toggle",value:function(){this.setState({collapse:!this.state.collapse})}},{key:"render",value:function(){var e=this.state.collapse?n.a.createElement(g.b,null):n.a.createElement(g.a,null);return n.a.createElement(i.Fragment,null,n.a.createElement(p.b,{color:"link",onClick:this.toggle},n.a.createElement("h4",null,this.props.icon," ",this.props.title,e)),n.a.createElement("em",null,this.props.description),n.a.createElement(p.f,{isOpen:this.state.collapse},n.a.createElement(p.c,null,n.a.createElement(p.d,null,this.props.internal))))}}]),t}(i.Component),X=[{link:"https://scholar.google.com/citations?user=ijttsicAAAAJ&hl=en&oi=ao",text:"Google Scholar",icon:n.a.createElement(g.n,{color:"#8c1313",size:"40px"})},{link:"https://github.com/eshedmargalit",text:"GitHub",icon:n.a.createElement(g.l,{color:"#8c1313",size:"40px"})},{link:"https://twitter.com/eshedmargalit",text:"Twitter",icon:n.a.createElement(g.v,{color:"#8c1313",size:"40px"})},{link:"https://www.linkedin.com/in/eshed-margalit-437222a7",text:"LinkedIn",icon:n.a.createElement(g.r,{color:"#8c1313",size:"40px"})}],Z=function(e){function t(e){var a;return Object(s.a)(this,t),(a=Object(c.a)(this,Object(h.a)(t).call(this,e))).render_award=function(e){var t;return t=""===e.url?null:n.a.createElement(p.b,{color:"link",href:e.url},n.a.createElement(g.q,{size:"1.5em"})),n.a.createElement(p.n,null,n.a.createElement("h6",null,e.name),n.a.createElement("p",null,e.dates),n.a.createElement("p",null,e.description),t)},a.cv=q.CV,a}return Object(u.a)(t,e),Object(l.a)(t,[{key:"render_buttons",value:function(){var e=X.map(function(e){return n.a.createElement(p.e,{lg:"3",xs:"3",key:e.link},n.a.createElement(p.b,{className:"hover-button",color:"link"},n.a.createElement("a",{href:e.link,target:"_blank",rel:"noopener noreferrer"},e.icon)))});return n.a.createElement(p.u,null,e)}},{key:"render_education",value:function(){return n.a.createElement(p.m,null,n.a.createElement(p.n,null,n.a.createElement(p.g,null,n.a.createElement(p.u,null,n.a.createElement(p.e,{xs:"6",lg:"2"},n.a.createElement("img",{src:O.a,alt:"Stanford official seal",className:"full-width"})),n.a.createElement(p.e,{lg:"10"},n.a.createElement("h5",null,"Stanford University"),n.a.createElement("p",null,"2016 - Present"),n.a.createElement("p",null,"PhD Candidate, Neurosciences PhD Program"))))),n.a.createElement(p.n,null,n.a.createElement(p.g,null,n.a.createElement(p.u,null,n.a.createElement(p.e,{xs:"6",lg:"2"},n.a.createElement("img",{src:D.a,alt:"USC official seal",className:"full-width"})),n.a.createElement(p.e,{lg:"10"},n.a.createElement("h5",null,"University of Southern California"),n.a.createElement("p",null,"2012 - 2016"),n.a.createElement("p",null,"B.S. with Honors in Computational Neuroscience",n.a.createElement("br",null),"Minor in Computer Science"),n.a.createElement("p",null,"Cumulative GPA: 3.99"))))))}},{key:"render_research",value:function(){return n.a.createElement(i.Fragment,null,n.a.createElement("h5",null,"Current"),n.a.createElement(p.m,null,n.a.createElement(p.n,null,n.a.createElement(p.g,null,n.a.createElement(p.u,null,n.a.createElement(p.e,{lg:"2",xs:"6"},n.a.createElement("img",{src:v.a,className:"full-width",alt:"Stanford NeuroAI Lab Logo"})),n.a.createElement(p.e,{lg:"10",xs:"10"},n.a.createElement("h6",null,"Stanford NeuroAI Lab"," ",n.a.createElement(p.b,{outline:!0,color:"link",size:"1.5em",href:"http://neuroailab.stanford.edu/"},n.a.createElement(g.q,null))),n.a.createElement("p",null,"2016 - Present",n.a.createElement("br",null),"PI: Dan Yamins"),n.a.createElement("p",null,n.a.createElement("em",null,"Modeling structure and development of primate visual cortex")))))),n.a.createElement(p.n,null,n.a.createElement(p.g,null,n.a.createElement(p.u,null,n.a.createElement(p.e,{lg:"2",xs:"6"},n.a.createElement("img",{src:b.a,className:"full-width",alt:"VPNL logo"})),n.a.createElement(p.e,{lg:"10",xs:"10"},n.a.createElement("h6",null,"Stanford Vision and Perception Neuroscience Lab"," ",n.a.createElement(p.b,{outline:!0,color:"link",size:"1.5em",href:"http://vpnl.stanford.edu/"},n.a.createElement(g.q,null))),n.a.createElement("p",null,"2016 - Present",n.a.createElement("br",null),"PI: Kalanit Grill-Spector"),n.a.createElement("p",null,n.a.createElement("em",null,"Characterization of human higher visual cortex via ultra-high-resolution fMRI"))))))),n.a.createElement("br",null),n.a.createElement("h5",null,"Past"),n.a.createElement(p.m,null,n.a.createElement(p.n,null,n.a.createElement(p.g,null,n.a.createElement(p.u,null,n.a.createElement(p.e,{lg:"2",xs:"6"},n.a.createElement("img",{src:H.a,className:"full-width",alt:"IUL lab logo"})),n.a.createElement(p.e,{lg:"10",xs:"10"},n.a.createElement("h6",null,"USC Image Understanding Lab"," ",n.a.createElement(p.b,{outline:!0,color:"link",size:"1.5em",href:"http://geon.usc.edu/"},n.a.createElement(g.q,null))),n.a.createElement("p",null,"2014 - 2016",n.a.createElement("br",null),n.a.createElement("strong",null,"PI: ")," Irving Biederman"),n.a.createElement("p",null,n.a.createElement("em",null,"Interrogating object representations in visual cortex and psychophysical correlates of developmental prosopagnosia")))))),n.a.createElement(p.n,null,n.a.createElement(p.g,null,n.a.createElement(p.u,null,n.a.createElement(p.e,{lg:"2",xs:"6"},n.a.createElement("img",{src:W.a,className:"full-width",alt:"Emotion and Cognition Lab Logo"})),n.a.createElement(p.e,{lg:"10",xs:"10"},n.a.createElement("h6",null,"USC Emotion and Cognition Lab"," ",n.a.createElement(p.b,{outline:!0,color:"link",size:"1.5em",href:"http://gero.usc.edu/labs/matherlab/"},n.a.createElement(g.q,null))),n.a.createElement("p",null,"2013 - 2014",n.a.createElement("br",null),n.a.createElement("strong",null,"PI: ")," Mara Mather"),n.a.createElement("p",null,n.a.createElement("em",null,"Investigating the role of the noradrenergic arousal system in aging and memory"))))))))}},{key:"render_publications",value:function(){return n.a.createElement(i.Fragment,null,n.a.createElement(p.b,{href:"https://scholar.google.com/citations?user=ijttsicAAAAJ&hl=en",color:"secondary"},n.a.createElement(g.m,{size:"1em"})," | My Google Scholar Profile"),n.a.createElement("hr",null),n.a.createElement(Y,{name:"Published",pubs:this.cv.publications.published}),n.a.createElement("br",null),n.a.createElement(Y,{name:"Preprints",pubs:this.cv.publications.preprints}))}},{key:"render_presentations",value:function(){return n.a.createElement(i.Fragment,null,n.a.createElement(Y,{name:"Talks",pubs:this.cv.presentations.talks}),n.a.createElement("br",null),n.a.createElement(Y,{name:"Posters",pubs:this.cv.presentations.posters}))}},{key:"render_skills",value:function(){return n.a.createElement(p.g,null,n.a.createElement(p.u,null,n.a.createElement(p.e,null,"Check out my GitHub to see what I'm up to:",n.a.createElement(p.b,{color:"link"},n.a.createElement(g.l,{size:"2em"})))),n.a.createElement("hr",null),n.a.createElement(p.u,null,n.a.createElement(p.e,{xs:"12",lg:"4"},n.a.createElement("h3",null,"Things I use daily"),n.a.createElement(p.m,null,n.a.createElement(p.n,null,"Python"),n.a.createElement(p.n,null,"MATLAB"),n.a.createElement(p.n,null,"Git / Github"),n.a.createElement(p.n,null,"TensorFlow"),n.a.createElement(p.n,null,"Jupyter"))),n.a.createElement(p.e,{xs:"12",lg:"4"},n.a.createElement("h3",null,"Things I use pretty often"),n.a.createElement(p.m,null,n.a.createElement(p.n,null,"Google TPUv2"),n.a.createElement(p.n,null,"HTML/CSS/React/Redux"),n.a.createElement(p.n,null,"C, C++, Java"),n.a.createElement(p.n,null,"FSL"),n.a.createElement(p.n,null,"FreeSurfer"),n.a.createElement(p.n,null,"Photoshop and Illustrator"))),n.a.createElement(p.e,{xs:"12",lg:"4"},n.a.createElement("h3",null,"Things I've used before"),n.a.createElement(p.m,null,n.a.createElement(p.n,null,"Patch-clamp physiology"),n.a.createElement(p.n,null,"Multi-unit in-vivo recording"),n.a.createElement(p.n,null,"Psychtoolbox, PsychoPy"),n.a.createElement(p.n,null,"Spike sorting (KiloSort)")))))}},{key:"render_awards",value:function(){var e=this;return n.a.createElement(p.g,null,n.a.createElement(p.u,null,n.a.createElement(p.e,null,n.a.createElement(p.m,null,this.cv.awards.map(function(t){return n.a.createElement("span",{key:t.name},e.render_award(t))})))))}},{key:"render_teaching",value:function(){return n.a.createElement(p.g,null,n.a.createElement(p.u,null,n.a.createElement(p.e,null,n.a.createElement(p.m,null,n.a.createElement(p.n,null,n.a.createElement("h6",null,"Teaching Assistant, Introduction to Perception (PSYCH 30)"),n.a.createElement("p",null,"Fall 2017, 2018")),n.a.createElement(p.n,null,n.a.createElement("h6",null,"Teaching Assistant,"," ",n.a.createElement("a",{href:"https://med.stanford.edu/neurogradprogram/prospective_students/bootcamp.html"},"Stanford Intensive Neuroscience Bootcamp")),n.a.createElement("p",null,"Fall 2017")),n.a.createElement(p.n,null,n.a.createElement("h6",null,"Instructor,"," ",n.a.createElement("a",{href:"https://stanfordesp.org/"},"Stanford Splash")),n.a.createElement("p",null,"Fall 2017")),n.a.createElement(p.n,null,n.a.createElement("h6",null,"Instructor,"," ",n.a.createElement("a",{href:"https://med.stanford.edu/neurogradprogram/news_and_events/news/brain-day.html"},"Stanford Brain Day")),n.a.createElement("p",null,"Fall 2017"))))))}},{key:"render_service",value:function(){return n.a.createElement(p.g,null,n.a.createElement(p.u,null,n.a.createElement(p.e,null,n.a.createElement(p.m,null,n.a.createElement(p.n,null,n.a.createElement("h6",null,"Chair, Society for Neuroscience Nanosymposium: Extrastriate Vision"),n.a.createElement("p",null,"2018")),n.a.createElement(p.n,null,n.a.createElement("h6",null,"Co-Leader,"," ",n.a.createElement("a",{href:"http://stanford-cnjc.github.io"},"Stanford Computational Neuroscience Journal Club (CNJC)")),n.a.createElement("p",null,"2018, 2019")),n.a.createElement(p.n,null,n.a.createElement("h6",null,"Stanford Neurosciences PhD Program Student Mentor"),n.a.createElement("p",null,"2019")),n.a.createElement(p.n,null,n.a.createElement("h6",null,"Stanford Neurosciences PhD Program Student Program Committee"),n.a.createElement("p",null,"2018")),n.a.createElement(p.n,null,n.a.createElement("h6",null,"Mentor and workshop leader for NSF GRFP Application"),n.a.createElement("p",null,"2017, 2018, 2019")),n.a.createElement(p.n,null,n.a.createElement("h6",null,"Student Speaker Representative, Stanford Neurosciences PhD Program"),n.a.createElement("p",null,"2017 - 2018")),n.a.createElement(p.n,null,n.a.createElement("h6",null,"Mentor, Stanford Biosciences Student Association"),n.a.createElement("p",null,"2017 - 2019")),n.a.createElement(p.n,null,n.a.createElement("h6",null,"Student Representative, USC Undergraduate Neuroscience Program Executive Committee"),n.a.createElement("p",null,"2015 - 2016")),n.a.createElement(p.n,null,n.a.createElement("h6",null,"Mentor to undergraduate lab members"),n.a.createElement("p",null,"2013 - present"),n.a.createElement("p",null,"Jordan Juarez, Isabel Irawan, Emily Meschke, Rafael Maarek, and Catrina Hacker")),n.a.createElement(p.n,null,n.a.createElement("h6",null,"Team captain, USC Cross Country Club"),n.a.createElement("p",null,"2014 -2015"))))))}},{key:"render_scicomm",value:function(){return n.a.createElement(p.g,null,n.a.createElement(p.u,null,n.a.createElement(p.e,null,n.a.createElement(p.m,null,n.a.createElement(p.n,null,n.a.createElement("h6",null,"Back to Basics with Visual Feedbacks"),n.a.createElement("p",null,"Summary of"," ",n.a.createElement("a",{href:"https://www.ncbi.nlm.nih.gov/pubmed/29662217"},"Marques et al., 2018")),n.a.createElement(p.b,{color:"link",href:"http://www.neuwritewest.org/blog/back-to-basics-with-visual-feedbacks"},n.a.createElement(g.q,{size:"1.5em"})," Link to Blog Post")),n.a.createElement(p.n,null,n.a.createElement("h6",null,"Nurturing the Study of Nature"),n.a.createElement("p",null,"Summary of"," ",n.a.createElement("a",{href:"https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0148405"},"Grunspan et al., 2016")),n.a.createElement(p.b,{color:"link",href:"http://www.neuwritewest.org/blog/nurturing-the-study-of-nature"},n.a.createElement(g.q,{size:"1.5em"})," Link to Blog Post")),n.a.createElement(p.n,null,n.a.createElement("h6",null,"Paper a Week"),n.a.createElement("p",null," ","For more, check out my"," ",n.a.createElement(f.a,{to:"/PaperReviews"},"Paper-A-Week")," page, where I review in detail one paper each week!"," "))))))}},{key:"render",value:function(){return n.a.createElement(p.g,null,n.a.createElement(p.u,null,n.a.createElement(p.e,{xs:"8",lg:"4"},n.a.createElement(p.g,null,n.a.createElement(p.u,null,n.a.createElement(p.e,null,n.a.createElement("br",null),n.a.createElement("h1",{className:"display-5"},"Curriculum Vitae"),n.a.createElement("h6",null,"Current through ",this.cv.last_updated),n.a.createElement("p",null,"An interactive version of my CV can be found below. If you'd prefer a PDF version, please click this link instead."),n.a.createElement("p",null,n.a.createElement(p.b,{color:"secondary",href:"files/"+this.cv.pdf.path},n.a.createElement(g.k,{size:"3em"}),this.cv.pdf.path)))))),n.a.createElement(p.e,{xs:"8",lg:"4"},n.a.createElement(p.g,null,n.a.createElement(p.u,null,n.a.createElement(p.e,{xs:"12",lg:"12"},n.a.createElement("img",{src:G.a,alt:"Headshot of Eshed",className:"circle full-width"}),n.a.createElement("p",{className:"center-text anonymous"},"eshed.margalit [at] gmail [dot] com"))))),n.a.createElement(p.e,{xs:"8",lg:"4"},n.a.createElement(p.g,null,n.a.createElement(p.u,null,n.a.createElement(p.e,{xs:"12",lg:"12"},n.a.createElement("hr",null),n.a.createElement("p",null,"I'm a neuroscience graduate student at Stanford with an interest in vision, computational neuroscience, artificial intelligence, and neuroimaging."),n.a.createElement("hr",null))),n.a.createElement(p.u,null,n.a.createElement(p.e,null,this.render_buttons(),n.a.createElement("hr",null)))))),n.a.createElement("hr",null),n.a.createElement(p.u,null,n.a.createElement(p.e,null,n.a.createElement(Q,{default_open:!0,title:"Education",icon:n.a.createElement(g.w,{size:"1em"}),description:"Education history and current affiliations",internal:this.render_education()}))),n.a.createElement("hr",null),n.a.createElement(p.u,null,n.a.createElement(p.e,null,n.a.createElement(Q,{default_open:!0,title:"Research",icon:n.a.createElement(g.f,{size:"1em"}),description:"Labs I've worked in",internal:this.render_research()}))),n.a.createElement("hr",null),n.a.createElement(p.u,null,n.a.createElement(p.e,null,n.a.createElement(Q,{default_open:!0,title:"Publications",icon:n.a.createElement(g.s,{size:"1em"}),description:"Peer-reviewed publications",internal:this.render_publications()}))),n.a.createElement("hr",null),n.a.createElement(p.u,null,n.a.createElement(p.e,null,n.a.createElement(Q,{default_open:!0,title:"Presentations",icon:n.a.createElement(g.x,{size:"1em"}),description:"Conference posters and talks",internal:this.render_presentations()}))),n.a.createElement("hr",null),n.a.createElement(p.u,null,n.a.createElement(p.e,null,n.a.createElement(Q,{default_open:!0,title:"Skills",icon:n.a.createElement(g.p,{size:"1em"}),description:"Methods, programming, and other training",internal:this.render_skills()}))),n.a.createElement("hr",null),n.a.createElement(p.u,null,n.a.createElement(p.e,null,n.a.createElement(Q,{default_open:!0,title:"Awards",icon:n.a.createElement(g.d,{size:"1em"}),description:"Grants, awards, and recognitions",internal:this.render_awards()}))),n.a.createElement("hr",null),n.a.createElement(p.u,null,n.a.createElement(p.e,null,n.a.createElement(Q,{default_open:!0,title:"Teaching",icon:n.a.createElement(g.h,{size:"1em"}),description:"Teaching experience",internal:this.render_teaching()}))),n.a.createElement("hr",null),n.a.createElement(p.u,null,n.a.createElement(p.e,null,n.a.createElement(Q,{default_open:!0,title:"Service",icon:n.a.createElement(g.o,{size:"1em"}),description:"Mentorship and volunteer work",internal:this.render_service()}))),n.a.createElement("hr",null),n.a.createElement(p.u,null,n.a.createElement(p.e,null,n.a.createElement(Q,{default_open:!0,title:"Science Communication",icon:n.a.createElement(g.i,{size:"1em"}),description:"Writing about others' science",internal:this.render_scicomm()}))))}}]),t}(i.Component),$=(a(303),a(246)),ee=function(e){function t(){var e,i;Object(s.a)(this,t);for(var r=arguments.length,o=new Array(r),l=0;l<r;l++)o[l]=arguments[l];return(i=Object(c.a)(this,(e=Object(h.a)(t)).call.apply(e,[this].concat(o)))).render_projects=function(e){return e.map(function(e){var t;t=""===e.url?null:n.a.createElement(p.b,{color:"secondary",href:e.url},n.a.createElement(g.q,null)," "," "," ",e.url_text);var i=a(305)("./".concat(e.img_src));return n.a.createElement("span",{key:e.name},n.a.createElement(p.n,null,n.a.createElement(p.u,null,n.a.createElement(p.e,{lg:"9"},n.a.createElement("h2",null,e.name),n.a.createElement("p",null,e.description),t),n.a.createElement(p.e,{lg:"3"},n.a.createElement("img",{src:i,alt:e.alt,id:e.id,className:"circle full-width"}),n.a.createElement(p.v,{placement:"top",target:e.id},e.alt)))))})},i}return Object(u.a)(t,e),Object(l.a)(t,[{key:"render",value:function(){return n.a.createElement(p.g,null,n.a.createElement(p.u,null,n.a.createElement(p.e,{xs:"8",lg:"8"},n.a.createElement("br",null),n.a.createElement("h1",null,"Projects I've worked on"),n.a.createElement("p",null,"This is an incomplete list of projects that have interested me, please get in touch if you'd like to chat further about any of these things!"),n.a.createElement(p.b,{href:"https://scholar.google.com/citations?user=ijttsicAAAAJ&hl=en",color:"primary"},n.a.createElement(g.m,{size:"1em"})," | My Google Scholar Profile"))),n.a.createElement("br",null),n.a.createElement(p.m,null,this.render_projects($.projects)))}}]),t}(i.Component),te=a(86),ae=(a(133),a(247)),ie=a.n(ae),ne=a(37),re=function(e){function t(e){var i;return Object(s.a)(this,t),(i=Object(c.a)(this,Object(h.a)(t).call(this,e))).load_and_concat_reviews=function(e){return e.map(function(e){return a(134)("./".concat(e))})},i.get_tag_color=function(e){for(var t=0,a=0;a<e.length;a++)t=e.charCodeAt(a)+((t<<5)-t);return"hsl("+t%360+",80%,30%)"},i.render_comma_sep_list=function(e){return e.map(function(t,a){var i;return i=a===e.length-1?1===e.length?n.a.createElement("span",null,t,n.a.createElement("br",null)):n.a.createElement("span",null,"and ",t,n.a.createElement("br",null)):a===e.length-2?n.a.createElement("span",null,t," "):n.a.createElement("span",null,t,","," "),n.a.createElement("span",{key:e.tag+t},i)})},i.handleSearch=function(e){i.setState({query:e,searchbar_value:e})},i.handleSort=function(e){i.setState({sort_mode:e})},i.render_paper_metadata=function(e){var t=e.keywords,a=null;t&&(a=t.map(function(e){return n.a.createElement("span",{key:e},n.a.createElement(p.a,{style:{background:i.get_tag_color(e)},className:"tag-badge",onClick:function(e){e.stopPropagation(),e.preventDefault(),i.handleSearch("".concat(e.target.innerHTML))},href:"#"},e)," ")}));var r=J()(e.review_date,"YYYY-MM-DD").format("MMMM DD, YYYY"),o=J()(e.date,"YYYY-MM").format("MMMM YYYY");return n.a.createElement("div",null,n.a.createElement("div",null,n.a.createElement("h5",null,e.title),i.render_comma_sep_list(e.authors),"Published ",o),n.a.createElement("div",null,a),n.a.createElement("div",null,n.a.createElement("strong",null,"TL;DR:")," ",e.one_sentence),n.a.createElement("div",null,n.a.createElement("em",null,"Read on ",r)))},i.sort_reviews=function(e){return"review date (descending)"===i.state.sort_mode?e.sort(function(e,t){return-J()(e.metadata.review_date).diff(J()(t.metadata.review_date))}):"review date (ascending)"===i.state.sort_mode?e.sort(function(e,t){return J()(e.metadata.review_date).diff(J()(t.metadata.review_date))}):"publication date (descending)"===i.state.sort_mode?e.sort(function(e,t){return-J()(e.metadata.date).diff(J()(t.metadata.date))}):"publication date (ascending)"===i.state.sort_mode?e.sort(function(e,t){return J()(e.metadata.date).diff(J()(t.metadata.date))}):void 0},i.trim_reviews=function(e){if(""===i.state.query)return e;return new ie.a(e,{shouldSort:!1,threshold:.2,location:0,distance:5e3,maxPatternLength:32,minMatchCharLength:4,keys:["metadata.title","metadata.authors","metadata.keywords","metadata.date"]}).search(i.state.query)},i.review_clicked=function(e){var t=e.metadata.date.substring(0,4),a=e.metadata.authors,n=a.length,r=(2===n?a[0].split(",")[0]+" and "+a[1].split(",")[0]:1===n?a[0].split(",")[0]:a[0].split(",")[0]+" et al.")+", "+t;i.setState({active_paper:e,viewing_paper:!0,current_paper_short_title:r})},i.render_papers=function(e){var t=e.map(function(e){return n.a.createElement(p.n,{action:!0,key:e.metadata.title,className:"review-lgi",onClick:function(t){i.review_clicked(e)}},i.render_paper_metadata(e.metadata))});return n.a.createElement(p.m,null,t)},i.render_review=function(e){var t=n.a.createElement("div",null,n.a.createElement("h6",null,"General Summary"),n.a.createElement("ul",null,e.review.summary.map(function(e){return n.a.createElement("li",{key:e},e)}))),a=n.a.createElement("div",null,n.a.createElement("h6",null,"Background"),n.a.createElement("ul",null,e.review.background.map(function(e){return n.a.createElement("li",{key:e},e)}))),r=n.a.createElement("div",null,n.a.createElement("h6",null,"Approach and Methods"),n.a.createElement("ul",null,e.review.approach.map(function(e){return n.a.createElement("li",{key:e},e)}))),o=n.a.createElement("div",null,n.a.createElement("h6",null,"Results"),n.a.createElement("ul",null,e.review.results.map(function(e){return n.a.createElement("li",{key:e},e)}))),s=n.a.createElement("div",null,n.a.createElement("h6",null,"Conclusions"),n.a.createElement("ul",null,e.review.conclusions.map(function(e){return n.a.createElement("li",{key:e},e)}))),l=null;e.review.other.length>0&&(l=n.a.createElement("div",null,n.a.createElement("h6",null,"Other information"),n.a.createElement("ul",null,e.review.other.map(function(e){return n.a.createElement("li",{key:e},e)}))));var c=J()(e.metadata.date,"YYYY-MM").format("MMMM YYYY"),h=null;return e.metadata.doi&&(h=n.a.createElement("a",{href:"http://dx.doi.org/"+e.metadata.doi,target:"_blank",rel:"noopener noreferrer"},"(",e.metadata.doi,")")),n.a.createElement(p.g,null,n.a.createElement(p.u,null,n.a.createElement(p.e,null,n.a.createElement("h4",null,e.metadata.title),i.render_comma_sep_list(e.metadata.authors),i.render_comma_sep_list(e.metadata.institutions),"Published in ",e.metadata.journal," in ",c," ",h,n.a.createElement("hr",null),t,a,r,o,s,l)),n.a.createElement(p.u,null,n.a.createElement(p.e,null,n.a.createElement(p.b,{onClick:function(e){i.setState({viewing_paper:!1}),window.scrollTo(0,i.navRef.current.offsetTop)},color:"primary"},n.a.createElement(g.c,null)," Back to List of Reviews"))))},i.load_and_concat_reviews=i.load_and_concat_reviews.bind(Object(M.a)(i)),i.papers=i.load_and_concat_reviews(ne.default),i.navRef=n.a.createRef(),i.state={query:"",searchbar_value:"",active_paper:null,viewing_paper:!1,current_paper_short_title:"Margalit et al., 2019",sort_mode:"review date (descending)"},i}return Object(u.a)(t,e),Object(l.a)(t,[{key:"render",value:function(){var e=this,t=null;this.state.searchbar_value&&(t=n.a.createElement(p.e,{lg:"2",xs:"2"},n.a.createElement(p.b,{style:{position:"absolute",bottom:"15px"},onClick:function(t){return e.setState({query:"",searchbar_value:""})},color:"danger"},n.a.createElement(g.e,null)," Clear Search")));var a=n.a.createElement(p.g,null,n.a.createElement(p.u,null,n.a.createElement(p.e,{lg:"7",xs:"7"},n.a.createElement(p.h,null,n.a.createElement(p.i,null,n.a.createElement(p.l,{for:"search_input"},"Search by author, year, title, or keyword:"),n.a.createElement(p.j,{type:"text",id:"search_input",onChange:function(t){return e.handleSearch("".concat(t.target.value))},placeholder:"e.g., orthogonal",value:this.state.searchbar_value})))),n.a.createElement(p.e,{lg:"3",xs:"3"},n.a.createElement(p.h,null,n.a.createElement(p.i,null,n.a.createElement(p.l,{for:"sort_input"},"Sort by:"),n.a.createElement(p.j,{type:"select",id:"sort_input",onChange:function(t){return e.handleSort("".concat(t.target.value))}},n.a.createElement("option",null,"review date (descending)"),n.a.createElement("option",null,"review date (ascending) "),n.a.createElement("option",null,"publication date (ascending)"),n.a.createElement("option",null,"publication date (descending)"))))),t),n.a.createElement(p.u,null,n.a.createElement(p.e,null,this.render_papers(this.sort_reviews(this.trim_reviews(this.papers)))))),i=a,r=n.a.createElement(p.o,{tabs:!0},n.a.createElement(p.p,null,n.a.createElement(p.q,{className:"nav-tab",active:!this.state.viewing_paper},"List of Reviews")));return this.state.active_paper&&(i=this.state.viewing_paper?this.render_review(this.state.active_paper):a,r=n.a.createElement(p.o,{tabs:!0},n.a.createElement(p.p,null,n.a.createElement(p.q,{className:"nav-tab",onClick:function(t){e.setState({viewing_paper:!1,query:"",searchbar_value:""})},active:!this.state.viewing_paper},"List of Reviews")),n.a.createElement(p.p,null,n.a.createElement(p.q,{className:"nav-tab",onClick:function(t){e.setState({viewing_paper:!0})},active:this.state.viewing_paper},this.state.current_paper_short_title)))),n.a.createElement("div",{ref:this.navRef},r,n.a.createElement("br",null),i)}}]),t}(i.Component),oe=function(e){function t(e){var i;return Object(s.a)(this,t),(i=Object(c.a)(this,Object(h.a)(t).call(this,e))).load_and_concat_reviews=function(e){return e.map(function(e){return a(134)("./".concat(e))})},i.load_and_concat_reviews=i.load_and_concat_reviews.bind(Object(M.a)(i)),i.papers=i.load_and_concat_reviews(ne.default),i.review_dates=i.papers.map(function(e){return J()(e.metadata.review_date,"YYYY-MM-DD")}),i}return Object(u.a)(t,e),Object(l.a)(t,[{key:"compute_ppw",value:function(){var e=this.review_dates.sort(function(e,t){return e.diff(t)}),t=e[e.length-1].diff(e[0],"days")/7;return Number.parseFloat(e.length/t).toFixed(3)}},{key:"compute_cumulative_number",value:function(){for(var e=this.review_dates.sort(function(e,t){return e.diff(t)}),t=[],a=0,i=0;i<e.length-1;i++){a+=e[i+1].diff(e[i],"days"),t.push(a)}return t}},{key:"render",value:function(){return n.a.createElement(p.g,null,n.a.createElement(p.u,null,n.a.createElement(p.e,{xs:"8",lg:"8"},n.a.createElement("br",null),n.a.createElement("h1",null,"Paper-a-Week"),"In recent years, I've been trying to develop a literature-reading habit that:",n.a.createElement("ul",null,n.a.createElement("li",null,"allows me to jot down notes in a consistent format"),n.a.createElement("li",null,"provides a way to search through notes and paper metadata"),n.a.createElement("li",null,"keeps me accountable to reading on a schedule")),"Enter ",n.a.createElement("strong",null,"Paper-a-Week"),", my attempt to meet those objectives! I've seen"," ",n.a.createElement("a",{href:"https://github.com/shagunsodhani/papers-I-read",target:"_blank",rel:"noopener noreferrer"},"things like this")," ","elsewhere, but the format and content of these entries are meant to serve my own interests."),n.a.createElement(p.e,null,n.a.createElement("br",null),n.a.createElement("h5",null," How am I doing? "),n.a.createElement("p",null," ","Papers / week: ",n.a.createElement("strong",null,this.compute_ppw())," "),n.a.createElement(te.Sparklines,{data:this.compute_cumulative_number(),height:75},n.a.createElement(te.SparklinesCurve,null),n.a.createElement(te.SparklinesSpots,null)),n.a.createElement("p",{className:"float-right"},n.a.createElement("em",null,"Cumulative review count: ",n.a.createElement("strong",null,this.papers.length))))),n.a.createElement("hr",null),n.a.createElement(p.u,null,n.a.createElement(p.e,null,n.a.createElement("h6",null," How this works "),n.a.createElement("ul",null,n.a.createElement("li",null," ","Click through each paper to see my notes about background, methods, results, conclusions, and more"," "),n.a.createElement("li",null," ","The selected paper stays open in a new 'tab', so you can browse the rest of the list while leaving the current review open"," "),n.a.createElement("li",null," ","Each paper has a number of tags, each of which has a unique background color. Click tags to search by that keyword!"),n.a.createElement("li",null," ","The search bar allows you to search by title, author, publication year, or keyword"," ")))),n.a.createElement("br",null),n.a.createElement(p.u,null,n.a.createElement(p.e,null,n.a.createElement(re,null))))}}]),t}(i.Component),se=a(240),le=function(e){function t(){return Object(s.a)(this,t),Object(c.a)(this,Object(h.a)(t).apply(this,arguments))}return Object(u.a)(t,e),Object(l.a)(t,[{key:"componentDidUpdate",value:function(e){this.props.location.pathname!==e.location.pathname&&window.scrollTo(0,0)}},{key:"render",value:function(){return this.props.children}}]),t}(i.Component),ce=Object(se.a)(le),he=a(22),ue=(a(308),a(77)),de=a.n(ue),me=a(55),pe=a.n(me),ge=a(15),fe=function(e){function t(){return Object(s.a)(this,t),Object(c.a)(this,Object(h.a)(t).apply(this,arguments))}return Object(u.a)(t,e),Object(l.a)(t,[{key:"getData",value:function(){for(var e=[],t=0;t<30;t++){var a=this.signal_gaussian(t),i={x:t,noise:this.noise_gaussian(t),signal:a};e.push(i)}return e}},{key:"signal_gaussian",value:function(e){var t=1/Math.sqrt(2*Math.PI);return e=(e-this.props.signal_mean)/this.props.signal_sigma,t*Math.exp(-.5*e*e)/this.props.signal_sigma}},{key:"noise_gaussian",value:function(e){var t=1/Math.sqrt(2*Math.PI);return e=(e-this.props.noise_mean)/this.props.noise_sigma,t*Math.exp(-.5*e*e)/this.props.noise_sigma}},{key:"render",value:function(){return n.a.createElement(ge.i,{width:"100%",height:this.props.height},n.a.createElement(ge.b,{data:this.getData()},n.a.createElement(ge.j,{dataKey:"x",ticks:[0,5,10,15,20,25,30]},n.a.createElement(ge.d,{value:"x",offset:0,position:"insideBottom"})),n.a.createElement(ge.k,{domain:[0,.2],label:{value:"P(x)",angle:-90,position:"insideLeft"}}),n.a.createElement(ge.a,{name:"Signal Absent",type:"monotone",dataKey:"noise",stroke:this.props.noise_color,strokeWidth:2,fill:this.props.noise_color}),n.a.createElement(ge.h,{x:this.props.noise_mean,stroke:this.props.noise_color,strokeOpacity:.2}),n.a.createElement(ge.a,{name:"Signal Present",type:"monotone",dataKey:"signal",stroke:this.props.signal_color,strokeWidth:2,fill:this.props.signal_color}),n.a.createElement(ge.h,{x:this.props.signal_mean,stroke:this.props.signal_color,strokeOpacity:.2}),n.a.createElement(ge.h,{x:this.props.criterion,stroke:"black",strokeWidth:2,strokeOpacity:.5,label:{value:"Criterion",angle:-90,position:"insideTop",offset:30}}),n.a.createElement(ge.e,{verticalAlign:"top",height:36})))}}]),t}(i.Component),ye=function(e){function t(){return Object(s.a)(this,t),Object(c.a)(this,Object(h.a)(t).apply(this,arguments))}return Object(u.a)(t,e),Object(l.a)(t,[{key:"render",value:function(){return n.a.createElement(ge.g,{width:this.props.size,height:this.props.size,data:this.props.roc_data},n.a.createElement(ge.c,{strokeDasharray:"3 3"}),n.a.createElement(ge.j,{domain:[0,1],type:"number",dataKey:"false_positives"},n.a.createElement(ge.d,{value:"False Positives",offset:-5,position:"insideBottom"})),n.a.createElement(ge.k,{label:{value:"Hits",angle:-90,position:"insideLeft"}}),n.a.createElement(ge.h,{y:this.props.hit_rate,stroke:this.props.signal_color,strokeWidth:2,strokeOpacity:.4}),n.a.createElement(ge.h,{x:this.props.fp_rate,stroke:this.props.noise_color,strokeWidth:2,strokeOpacity:.4}),n.a.createElement(ge.f,{type:"monotone",dataKey:"hits",dot:!1,stroke:"#d47006",strokeWidth:4}),n.a.createElement(ge.f,{type:"monotone",dataKey:"false_positives",dot:!1,stroke:"#bbb",strokeWidth:3}))}}]),t}(i.Component);function ve(e){var t=Math.round(100*e)/100;return t>1?t=1:t<0&&(t=0),t}function we(e,t,i){return a(239)(t,Math.pow(i,2)).cdf(e)}function be(e,t){return a(239)(0,Math.pow(t,2)).ppf(e)}function ke(e,t,a,i){for(var n=[],r=0;r<30;r+=.1){var o=1-we(r,e,t),s={false_positives:1-we(r,a,i),hits:o};n.push(s)}return n.push({false_positives:0,hits:0}),n.push({false_positives:1,hits:1}),n.sort(function(e,t){return e.false_positives-t.false_positives}),n}a(489),a(491);var Ee=function(e){function t(e){var a;return Object(s.a)(this,t),(a=Object(c.a)(this,Object(h.a)(t).call(this,e))).handleOnChangeSignalMu=function(e){a.setState({signal_mean:e}),a.updateMetrics()},a.handleOnChangeSignalSigma=function(e){a.state.sigma_lock?a.setState({signal_sigma:e,noise_sigma:e}):a.setState({signal_sigma:e}),a.updateMetrics()},a.handleOnChangeNoiseMu=function(e){a.setState({noise_mean:e}),a.updateMetrics()},a.handleOnChangeNoiseSigma=function(e){a.state.sigma_lock?a.setState({noise_sigma:e,signal_sigma:e}):a.setState({noise_sigma:e}),a.updateMetrics()},a.handleOnChangeCriterion=function(e){a.setState({criterion:e}),a.updateMetrics()},a.toggleCheckbox=function(e){a.setState({sigma_lock:e.target.checked}),e.target.checked&&a.setState({signal_sigma:a.state.noise_sigma}),a.updateMetrics()},a.updateHitsInput=function(e){var t=e.target,i=t.value,n=t.min,r=t.max;i=Math.max(Number(n),Math.min(Number(r),Number(i)));var o=a.state.criterion+be(i,a.state.signal_sigma);o=Math.round(100*o)/100,a.setState({hits:i,signal_mean:o,dprime:(o-a.state.noise_mean)/Math.sqrt(.5*(Math.pow(a.state.signal_sigma,2)+Math.pow(a.state.noise_sigma,2))),misses:1-i})},a.updateMissesInput=function(e){var t=e.target,i=t.value,n=t.min,r=t.max,o=1-(i=Math.max(Number(n),Math.min(Number(r),Number(i)))),s=a.state.criterion+be(o,a.state.signal_sigma);s=Math.round(100*s)/100,a.setState({misses:i,signal_mean:s,dprime:(s-a.state.noise_mean)/Math.sqrt(.5*(Math.pow(a.state.signal_sigma,2)+Math.pow(a.state.noise_sigma,2))),hits:o})},a.updateFPsInput=function(e){var t=e.target,i=t.value,n=t.min,r=t.max;i=Math.max(Number(n),Math.min(Number(r),Number(i)));var o=a.state.criterion+be(i,a.state.noise_sigma);o=Math.round(100*o)/100,a.setState({fp:i,noise_mean:o,dprime:(a.state.signal_mean-o)/Math.sqrt(.5*(Math.pow(a.state.signal_sigma,2)+Math.pow(a.state.noise_sigma,2))),cr:1-i})},a.updateCRsInput=function(e){var t=e.target,i=t.value,n=t.min,r=t.max,o=1-(i=Math.max(Number(n),Math.min(Number(r),Number(i)))),s=a.state.criterion+be(o,a.state.noise_sigma);s=Math.round(100*s)/100,a.setState({fp:o,noise_mean:s,dprime:(a.state.signal_mean-s)/Math.sqrt(.5*(Math.pow(a.state.signal_sigma,2)+Math.pow(a.state.noise_sigma,2))),cr:i})},a.state={signal_mean:15,signal_sigma:4,noise_mean:10,noise_sigma:4,criterion:12,sigma_lock:!0,isEditingHits:!1,isEditingFP:!1,isEditingMisses:!1,isEditingCR:!1},a.signal_color="#0d5e08",a.noise_color="#960f0f",a.updateMetrics=a.updateMetrics.bind(Object(M.a)(a)),a}return Object(u.a)(t,e),Object(l.a)(t,[{key:"componentDidMount",value:function(){this.updateMetrics()}},{key:"updateMetrics",value:function(){this.setState(function(e,t){return{dprime:(e.signal_mean-e.noise_mean)/Math.sqrt(.5*(Math.pow(e.signal_sigma,2)+Math.pow(e.noise_sigma,2))),hits:ve(1-we(e.criterion,e.signal_mean,e.signal_sigma)),misses:ve(we(e.criterion,e.signal_mean,e.signal_sigma)),fp:ve(1-we(e.criterion,e.noise_mean,e.noise_sigma)),cr:ve(we(e.criterion,e.noise_mean,e.noise_sigma))}})}},{key:"renderSliders",value:function(){return n.a.createElement(p.g,null,n.a.createElement(p.u,null,n.a.createElement(p.e,{lg:"4",xs:"4"},n.a.createElement("div",null,"Signal Present Mean:",n.a.createElement("br",null)," ",n.a.createElement("strong",null,this.state.signal_mean)),n.a.createElement(pe.a,{min:1,max:25,value:this.state.signal_mean,orientation:"horizontal",onChange:this.handleOnChangeSignalMu})),n.a.createElement(p.e,{lg:"4",xs:"4"},n.a.createElement("div",null,"Signal Absent Mean:",n.a.createElement("br",null)," ",n.a.createElement("strong",null,this.state.noise_mean)),n.a.createElement(pe.a,{min:1,max:25,value:this.state.noise_mean,orientation:"horizontal",onChange:this.handleOnChangeNoiseMu})),n.a.createElement(p.e,{lg:"4",xs:"4"},n.a.createElement("div",null,"Criterion:",n.a.createElement("br",null)," ",n.a.createElement("strong",null,this.state.criterion)),n.a.createElement(pe.a,{min:0,max:30,value:this.state.criterion,orientation:"horizontal",onChange:this.handleOnChangeCriterion}))),n.a.createElement(p.u,null,n.a.createElement(p.e,{lg:"4",xs:"4"},n.a.createElement("div",null,"Signal Present Std:",n.a.createElement("br",null)," ",n.a.createElement("strong",null,this.state.signal_sigma)),n.a.createElement(pe.a,{min:1,max:15,value:this.state.signal_sigma,orientation:"horizontal",onChange:this.handleOnChangeSignalSigma})),n.a.createElement(p.e,{lg:"4",xs:"4"},n.a.createElement("div",null,"Signal Absent Std:",n.a.createElement("br",null)," ",n.a.createElement("strong",null,this.state.noise_sigma)),n.a.createElement(pe.a,{min:1,max:15,value:this.state.noise_sigma,orientation:"horizontal",onChange:this.handleOnChangeNoiseSigma})),n.a.createElement(p.e,{lg:"4",xs:"4"},n.a.createElement(p.h,null,n.a.createElement(p.i,{check:!0},n.a.createElement(p.l,{check:!0},n.a.createElement(p.j,{defaultChecked:this.state.sigma_lock,onClick:this.toggleCheckbox,type:"checkbox"})," ","Assume equal standard deviations?"))))))}},{key:"toggleEditingHits",value:function(){this.setState({isEditingHits:!this.state.isEditingHits})}},{key:"toggleEditingMisses",value:function(){this.setState({isEditingMisses:!this.state.isEditingMisses})}},{key:"toggleEditingFPs",value:function(){this.setState({isEditingFP:!this.state.isEditingFP})}},{key:"toggleEditingCRs",value:function(){this.setState({isEditingCR:!this.state.isEditingCR})}},{key:"renderFields",value:function(){var e,t,a,i,r=(Math.round(100*this.state.dprime)/100).toFixed(2),o=(Math.round(100*this.state.hits)/100).toFixed(2),s=(Math.round(100*this.state.misses)/100).toFixed(2),l=(Math.round(100*this.state.fp)/100).toFixed(2),c=(Math.round(100*this.state.cr)/100).toFixed(2);return e=this.state.isEditingHits?n.a.createElement(p.j,{type:"number",name:"hits",id:"hitsInput",value:this.state.hits,onChange:this.updateHitsInput,onBlur:this.toggleEditingHits.bind(this),min:0,max:1,step:.1}):n.a.createElement(p.j,{type:"text",name:"hits",id:"hitsInput",value:o,onFocus:this.toggleEditingHits.bind(this),readOnly:!0}),t=this.state.isEditingMisses?n.a.createElement(p.j,{type:"number",name:"misses",id:"missesInput",value:this.state.misses,onChange:this.updateMissesInput,onBlur:this.toggleEditingMisses.bind(this),min:0,max:1,step:.1}):n.a.createElement(p.j,{type:"text",name:"misses",id:"missesInput",value:s,onFocus:this.toggleEditingMisses.bind(this),readOnly:!0}),a=this.state.isEditingFP?n.a.createElement(p.j,{type:"number",name:"FPs",id:"FPsInput",value:this.state.fp,onChange:this.updateFPsInput,onBlur:this.toggleEditingFPs.bind(this),min:0,max:1,step:.1}):n.a.createElement(p.j,{type:"text",name:"FPs",id:"FPsInput",value:l,onFocus:this.toggleEditingFPs.bind(this),readOnly:!0}),i=this.state.isEditingCR?n.a.createElement(p.j,{type:"number",name:"CRs",id:"CRsInput",value:this.state.cr,onChange:this.updateCRsInput,onBlur:this.toggleEditingCRs.bind(this),min:0,max:1,step:.1}):n.a.createElement(p.j,{type:"text",name:"CRs",id:"CRsInput",value:c,onFocus:this.toggleEditingCRs.bind(this),readOnly:!0}),n.a.createElement(p.g,null,n.a.createElement(p.u,null,n.a.createElement(p.e,null,n.a.createElement("h5",null,"d':"," ",n.a.createElement("strong",null,r)))),n.a.createElement("hr",null),n.a.createElement(p.u,null,n.a.createElement(p.e,null,n.a.createElement("h5",null,"Values at Given Criterion"))),n.a.createElement(p.u,null,n.a.createElement(p.e,{lg:"6",xs:"6"},n.a.createElement(p.i,null,n.a.createElement(p.l,{for:"hitsInput"},"Hits:"),e)),n.a.createElement(p.e,{lg:"6",xs:"6"},n.a.createElement(p.i,null,n.a.createElement(p.l,{for:"missesInput"},"Misses:"),t))),n.a.createElement(p.u,null,n.a.createElement(p.e,{lg:"6",xs:"6"},n.a.createElement(p.i,null,n.a.createElement(p.l,{for:"FPsInput"},"False Positives:"),a)),n.a.createElement(p.e,{lg:"6",xs:"6"},n.a.createElement(p.i,null,n.a.createElement(p.l,{for:"CRsInput"},"Correct Rejections:"),i))))}},{key:"render",value:function(){return n.a.createElement(p.g,null,n.a.createElement(p.u,null,n.a.createElement(p.e,{lg:"8",xs:"12"},n.a.createElement("h4",{className:"center"},"Normal Distributions"),n.a.createElement("br",null),n.a.createElement(fe,{height:250,signal_mean:this.state.signal_mean,signal_sigma:this.state.signal_sigma,signal_color:this.signal_color,noise_mean:this.state.noise_mean,noise_sigma:this.state.noise_sigma,noise_color:this.noise_color,criterion:this.state.criterion})),n.a.createElement(p.e,{lg:"4",xs:"12"},n.a.createElement("h4",{className:"center"},"ROC Curve"),n.a.createElement("br",null),n.a.createElement(ye,{size:250,roc_data:ke(this.state.signal_mean,this.state.signal_sigma,this.state.noise_mean,this.state.noise_sigma),hit_rate:this.state.hits,fp_rate:this.state.fp,signal_color:this.state.signal_color,noise_color:this.state.noise_color}))),n.a.createElement("hr",null),n.a.createElement(p.u,null,n.a.createElement(p.e,{lg:"8",md:"12",xs:"12"},this.renderSliders()),n.a.createElement(p.e,{lg:"4",md:"12",xs:"12"},this.renderFields())))}}]),t}(i.Component),_e=(a(493),function(e){function t(){return Object(s.a)(this,t),Object(c.a)(this,Object(h.a)(t).apply(this,arguments))}return Object(u.a)(t,e),Object(l.a)(t,[{key:"renderDprimer",value:function(){var e=n.a.createElement(he.InlineMath,{math:"\\mu_{SignalPresent}"}),t=n.a.createElement(he.InlineMath,{math:"\\mu_{SignalAbsent}"}),a=n.a.createElement(he.InlineMath,null,"\\sigma"),i=n.a.createElement(he.InlineMath,null,"\\infty");return n.a.createElement(p.g,null,n.a.createElement(p.u,null,n.a.createElement(p.e,null,n.a.createElement("h3",null,"A d' Primer"),n.a.createElement("h5",null," The Basics "),"d', also called the sensitivity index, is the primary statistic used in Signal Detection Theory. At its core, d' is a measure of how far apart two distributions are. If two distributions are perfectly overlapping, d'=0 . If two distributions are 1 standard deviation apart, d'=1.",n.a.createElement("br",null),"The equation is:",n.a.createElement(he.BlockMath,{math:"d' = \\frac{\\mu_{SignalPresent} - \\mu_{SignalAbsent}}{\\sigma}"}),"where ",e," is the mean of the distribution when there is a true signal present, ",t," is the mean of the distribution when there is no signal present, and ",a," is the standard deviation of the distributions. Note that this formula assumes that the standard deviation (",n.a.createElement(he.InlineMath,null,"\\sigma"),") is equal for both distributions. See below for discussion of the case where this assumption is not met.",n.a.createElement("br",null),n.a.createElement("br",null),n.a.createElement("h5",null,"Hits, Misses, False Positives, and Correct Rejections"),'Given the SignalPresent and SignalAbsent distributions, setting a "criterion" tells you the hit rate, miss rate, false positive rate, and correct rejection rate. In plain English, the hit rate is the proportion of the time that a signal is reported when there actually is a signal, and the false positive rate is the proportion of the time that a signal is reported when there is no signal present.',n.a.createElement("br",null),"Remember, the miss rate is just one minus the hit rate, and the correct rejection rate is just one minus the false positive rate, so knowing the hit rate gives you the miss rate, and knowing the false positive rate gives you the correct rejection rate.",n.a.createElement("br",null),n.a.createElement(p.u,null,n.a.createElement(p.e,{lg:{size:3,offset:4},xs:{size:6,offset:3}},n.a.createElement("img",{src:de.a,alt:"outcome table",className:"full-width"}))),n.a.createElement("br",null),n.a.createElement("br",null),n.a.createElement("h5",null," What's the Criterion? "),'The criterion indicates the amount of evidence above which a signal will be reported. Thus, the hit rate is the proportion of responses from the SignalPresent distribution above the criterion, and the miss rate is the propoertion of resopnses from the SignalPresent distribution below the criterion. The false positive rate is the proportion of responses from the SignalAbsent distribution above the criterion, and the correct rejection rate is the propoertion of responses from the SignalAbsent distribution below the criterion. A "conservative" criterion is relatively high, such that both hits and false positives will be low. A "liberal" criterion is relatively low, such that both hits and false positives will be high.',n.a.createElement("br",null),n.a.createElement("br",null),n.a.createElement("h5",null," ROC Curves "),"Receiver Operating Characteristic (ROC) Curves provide a visual representation of discriminability for a sensor. The x-axis is the false positive rate and the y-axis is the hit rate. An ROC curve is composed of all of the false positive rates and hit rates corresponding to all possible criteria one could choose from 0 to"," ",i,". Note that d' is constant along this curve, because different points along the curve only differ in criterion, not in sensor sensitivity. Increasing d' moves the curve up and to the left, whereas decreasing d' brings the curve toward the",n.a.createElement(he.InlineMath,null,"y=x")," unity line. The"," ",n.a.createElement(he.InlineMath,null,"y=x")," unity line represents a d' of 0, since hits and false positives are exactly equal along that line, and thus, the Signal Present and Signal Absent distributions must be perfectly overlapping.",n.a.createElement("br",null),n.a.createElement("br",null),"Often, the area under the ROC curve (AUC) is used to quantify how sensitive a sensor is. AUC ranges from 0 to 1, with 1 being a perfect sensor corresponding to a d' of ",i,". How does the AUC numerically relate to the d' value?",n.a.createElement(he.BlockMath,{math:"d' = \\sqrt{2}Z(AUC)"})," where Z() is the inverse CDF of the Gaussian distribution.",n.a.createElement("br",null),n.a.createElement("br",null),n.a.createElement("h5",null," Advanced Topics "),n.a.createElement("h6",null," Unequal Standard Deviations "),"What if the standard deviations of the two distributions are not equal? No problem, just replace the denominator with the square root of the average variance!",n.a.createElement(he.BlockMath,{math:"d' = \\frac{\\mu_{SignalPresent} - \\mu_{SignalAbsent}}{\\sqrt{\\frac{1}{2}(\\sigma_{SignaPresent}^2 + \\sigma_{SignalAbsent}^2)}}"}),n.a.createElement("br",null),n.a.createElement("h6",null," Computing d' from the hit rate and false alarm rate "),"If you have the hit rate and false alarm rate and can assume the two distributions are approximately normal, you can also compute d' as:",n.a.createElement(he.BlockMath,{math:"d' = Z(HitRate) - Z(FalsePositiveRate)"}),"where Z() is the inverse CDF of the Gaussian distribution.",n.a.createElement("br",null),n.a.createElement("hr",null),n.a.createElement("h5",null," References and Future Reading "),n.a.createElement("a",{href:"http://www.cns.nyu.edu/~david/handouts/sdt/sdt.html",target:"_blank",rel:"noopener noreferrer"},n.a.createElement(p.b,{color:"link"},"David Heeger's Signal Detection Theory Webpage ",n.a.createElement(g.q,null))),n.a.createElement("a",{href:"https://en.wikipedia.org/wiki/Sensitivity_index",target:"_blank",rel:"noopener noreferrer"},n.a.createElement(p.b,{color:"link"},"Wikipedia Page for Sensitivity ",n.a.createElement(g.q,null))))))}},{key:"render",value:function(){return n.a.createElement(p.g,null,n.a.createElement(p.u,null,n.a.createElement(p.e,{xs:"12",lg:"12"},n.a.createElement("br",null),n.a.createElement("h1",null,"d' Calculator"),n.a.createElement("h5",null,"An interactive tool for learning about signal detection theory"),n.a.createElement("p",null,"This webpage was designed to help students in PSYCH 30, a large undergradate class on Perception at Stanford University, understand the basics of Signal Detection Theory and the sensitivity index, d'. Play with the calculator below and scroll down to read more about signal detection and how to compute d'. If you have any issues or find bugs, please let me know via email!"),n.a.createElement(Ee,null),n.a.createElement("hr",null),this.renderDprimer())))}}]),t}(i.Component)),xe=a(257),je=a.n(xe),Ce=(a(495),function(e){function t(e){var a;return Object(s.a)(this,t),a=Object(c.a)(this,Object(h.a)(t).call(this,e)),new je.a(".copy-src"),a}return Object(u.a)(t,e),Object(l.a)(t,[{key:"render",value:function(){return n.a.createElement(d.a,null,n.a.createElement(ce,null,n.a.createElement("div",{className:"App"},n.a.createElement(m.a,{path:"/",component:V}),n.a.createElement(m.a,{exact:!0,path:"/",component:F}),n.a.createElement(m.a,{exact:!0,path:"/CV",component:Z}),n.a.createElement(m.a,{exact:!0,path:"/Research",component:ee}),n.a.createElement(m.a,{exact:!0,path:"/PaperReviews",component:oe}),n.a.createElement(m.a,{exact:!0,path:"/DPCalc",component:_e}),n.a.createElement(m.a,{path:"/",component:A}))))}}]),t}(i.Component));Boolean("localhost"===window.location.hostname||"[::1]"===window.location.hostname||window.location.hostname.match(/^127(?:\.(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)){3}$/));a(497);o.a.render(n.a.createElement(Ce,null),document.getElementById("root")),"serviceWorker"in navigator&&navigator.serviceWorker.ready.then(function(e){e.unregister()})},71:function(e,t,a){e.exports=a.p+"static/media/clouds_rest.837e031f.png"},74:function(e,t,a){e.exports=a.p+"static/media/vtc_fade.99b1e008.gif"},75:function(e,t,a){e.exports=a.p+"static/media/SU_Seal_Red.70ef3ddf.png"},76:function(e,t,a){e.exports=a.p+"static/media/USC_Seal.f3cde9dd.png"},77:function(e,t,a){e.exports=a.p+"static/media/dp_table.e20c8eec.png"},78:function(e,t,a){e.exports=a.p+"static/media/emcog.421510d6.jpg"},79:function(e,t){e.exports="data:image/png;base64,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"},80:function(e,t,a){e.exports=a.p+"static/media/headshot-sq.479e4fef.png"}},[[284,2,1]]]);
//# sourceMappingURL=main.dbd51275.chunk.js.map