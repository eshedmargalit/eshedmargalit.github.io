(window.webpackJsonp=window.webpackJsonp||[]).push([[0],{108:function(e,t,a){e.exports=a.p+"static/media/LOC_mricrogl.028dad47.png"},109:function(e,t,a){e.exports=a.p+"static/media/Margalit_et_al_2017_Fig3.91f0ab2e.png"},110:function(e,t,a){e.exports=a.p+"static/media/Margalit_et_al_VSS_2018.7cec7499.png"},111:function(e,t,a){e.exports=a.p+"static/media/dissim_histogram.81d96289.png"},112:function(e,t,a){e.exports=a.p+"static/media/dp_gui_screenshot.03e9c89f.png"},113:function(e,t,a){e.exports=a.p+"static/media/gabor_grab.95cec5fe.PNG"},114:function(e,t,a){e.exports=a.p+"static/media/spacenet.4f23cab6.png"},115:function(e,t,a){e.exports=a.p+"static/media/vtc_faces.eae46f35.png"},117:function(e,t,a){},118:function(e,t,a){},119:function(e,t,a){var n={"./":36,"./blank_template":120,"./blank_template.json":120,"./index":36,"./index.js":36,"./review_jsons/ackman_retinal_waves":121,"./review_jsons/ackman_retinal_waves.json":121,"./review_jsons/afraz_inactivation_face_gender":122,"./review_jsons/afraz_inactivation_face_gender.json":122,"./review_jsons/arcaro_universal_mechanisms":123,"./review_jsons/arcaro_universal_mechanisms.json":123,"./review_jsons/bracci_ventral_shape":124,"./review_jsons/bracci_ventral_shape.json":124,"./review_jsons/chapman_orientation_development":125,"./review_jsons/chapman_orientation_development.json":125,"./review_jsons/devalois_direction_selectivity":126,"./review_jsons/devalois_direction_selectivity.json":126,"./review_jsons/dicarlo_untangling":127,"./review_jsons/dicarlo_untangling.json":127,"./review_jsons/durbin_mitchison_som":128,"./review_jsons/durbin_mitchison_som.json":128,"./review_jsons/fahey_mouse_global_orientation":129,"./review_jsons/fahey_mouse_global_orientation.json":129,"./review_jsons/garg_color_orientation":130,"./review_jsons/garg_color_orientation.json":130,"./review_jsons/grill_spector_functional_architecture":131,"./review_jsons/grill_spector_functional_architecture.json":131,"./review_jsons/haxby_hyperalignment":132,"./review_jsons/haxby_hyperalignment.json":132,"./review_jsons/hubel_wiesel_v2v3":133,"./review_jsons/hubel_wiesel_v2v3.json":133,"./review_jsons/jacobs_jordan_short_connections":134,"./review_jsons/jacobs_jordan_short_connections.json":134,"./review_jsons/jang_orientation_classification":135,"./review_jsons/jang_orientation_classification.json":135,"./review_jsons/khalig-razavi_dnn":136,"./review_jsons/khalig-razavi_dnn.json":136,"./review_jsons/kim_pasupathy_texture_shape_v4":137,"./review_jsons/kim_pasupathy_texture_shape_v4.json":137,"./review_jsons/koch_pinwheels":138,"./review_jsons/koch_pinwheels.json":138,"./review_jsons/koulakov_chklovskii_wiring_cost":139,"./review_jsons/koulakov_chklovskii_wiring_cost.json":139,"./review_jsons/kriegeskorte_monkey_human":140,"./review_jsons/kriegeskorte_monkey_human.json":140,"./review_jsons/linsker_spatial_opponent":141,"./review_jsons/linsker_spatial_opponent.json":141,"./review_jsons/livingstone_hubel_color":142,"./review_jsons/livingstone_hubel_color.json":142,"./review_jsons/nauhaus_orthogonality":143,"./review_jsons/nauhaus_orthogonality.json":143,"./review_jsons/ohki_pinwheel_order":144,"./review_jsons/ohki_pinwheel_order.json":144,"./review_jsons/olshausen_field_sparse_coding":145,"./review_jsons/olshausen_field_sparse_coding.json":145,"./review_jsons/op_de_beeck_factors":146,"./review_jsons/op_de_beeck_factors.json":146,"./review_jsons/otoole_face_dcnn":147,"./review_jsons/otoole_face_dcnn.json":147,"./review_jsons/ponce_livingstone_super_stimuli":148,"./review_jsons/ponce_livingstone_super_stimuli.json":148,"./review_jsons/rajalingham_inactivation":149,"./review_jsons/rajalingham_inactivation.json":149,"./review_jsons/ringach_orientation_selectivity":150,"./review_jsons/ringach_orientation_selectivity.json":150,"./review_jsons/sato_mosaic":151,"./review_jsons/sato_mosaic.json":151,"./review_jsons/weliky_katz_disruption":152,"./review_jsons/weliky_katz_disruption.json":152};function i(e){var t=r(e);return a(t)}function r(e){var t=n[e];if(!(t+1)){var a=new Error("Cannot find module '"+e+"'");throw a.code="MODULE_NOT_FOUND",a}return t}i.keys=function(){return Object.keys(n)},i.resolve=r,e.exports=i,i.id=119},120:function(e){e.exports={metadata:{title:"",authors:["Doe, JJ","Joe, DD"],institutions:[],date:"YYYY-MM",journal:"",doi:"",url:"",keywords:[],review_date:"YYYY-MM-DD",one_sentence:""},review:{summary:[],background:[],approach:[],results:[],conclusions:[],other:[]}}},121:function(e){e.exports={metadata:{title:"Retinal waves coordinate patterned activity throughout the developing visual system",authors:["Ackman, JB","Burbridge, TJ","Crair, MC"],institutions:["Yale University"],date:"2012-10",journal:"Nature",doi:"doi:10.1038/nature11529",url:"",keywords:["retinal waves","mouse","superior colliculus","V1"],review_date:"2019-05-22",one_sentence:"Retinal waves are measured in vivo for the first time, demonstrating that they propagate to the superior colliculus, V1, and extrastriate visual areas in neonatal mice"},review:{summary:["Retinal waves propagate in vivo in neonatal mice, travel from the retina to V1 and beyond, and depend on cholinergic transmission","The correspondence between in vitro and in vivo values suggests that the in vitro measurements are useful tools, but this approach led to novel findings, such as a preferred wave-initiation site within the retina and coordination of waves between the retina of each eye"],background:["Spontaneous retinal waves had been demonstrated in vitro, and their blockade had been associated with poor refinement of visual circuits in the dorsal LGN and visual cortex. Until this paper, they had never been observed in vivo",""],approach:["Calcium indicator CaGr-Dx was injected into the retina of P3-P9 mice, and wide field imaging was performed over the superior colliculus to identify the retinal terminals","To measure cell activity in the SC, bulk load calcium indicator OGB1- AM. Repeat this procedure in visual cortex to measure responses there too."],results:["Retinal waves spread throughout the most superficial layer of the superior colliculus at speeds consistent with in vitro measurements. Their propagation can be blocked with TTX application the contralateral eye (Figure 1)","Waves propagate within SC cells themselves at the same position, spread, and velocity as the incoming waves measured at RGC terminals, suggesting a direct link between RGC waves and the induced waves in the SC (Figure 2)","Waves preferentially begin in the ventrotemporal retina, and suprisingly, are sometimes coordinated between eyes (Figure 3)","Waves also appear in V1, and their direction matches that of SC waves after correcting for the mirroring and rotation of the retinotopic maps between the two regions (Figure 4)","Waves can be observed in extrastriate areas, but they appear to be more independent, and abolishing waves has less of an impact on their activity (Figure 5)","The mechanism of wave transmission is likely cholinergic, as blocking nicotinic acetylcholine receptors abolishes waves throughout the system (Figure 6)"],conclusions:["In vitro measurements are supported and reinforced by these in vivo results, which demonstrate retinal wave propagation throughout the visual system for at least a week (P3 - P9)","Retinal waves, beginning before birth, may very well play a role in the patterned development of the entire visual system (although these results suggest that past V1, their influence is lower)"],other:["I really like this paper as a clearly-written and communicated tour de force, and it has nice movies you can download too!"]}}},122:function(e){e.exports={metadata:{title:"Optogenetic and pharmacological suppression of spatial clusters of face neurons reveal their causal role in face gender discrimination",authors:["Afraz, A","Boyden, ES","DiCarlo, JJ"],institutions:["MIT"],date:"2015-03",journal:"PNAS",doi:"10.1073/pnas.1423328112",url:"",keywords:["macaque","face","inactivation","gender"],review_date:"2019-05-30",one_sentence:"Inactivation of regions enriched for face-detecting neurons leads to a deficit in face-gender discrimination, both with optogenetic and pharmacological perturbations."},review:{summary:["Optogenetic and pharmacological inactivation of face patches leads to deficits in discrimination of facial gender"],background:["Face cells have been observed for decades, yet no causal evidence exists to support their role in visually-guided behaviors relating to faces","Are the neurons in face-selective clusters necessary to discriminate the gender of a face, or can the distributed IT population handle it alone?"],approach:["Identify the middle face patch in macaque IT of two monkeys, then inactivate a 1mm chunk of face-selective cortex (or nearby nonselective cortex) with either optogenetic tools or pharmacological methods. Note that both methods are reversible and do not require lesions."],results:["The optogenetic manipulation effectively 'deletes' a median of 33% of visually-evoked spikes (Figure 2)","Inactivation reduces face-gender discrimination performance by 2% when faces are in the contralateral visual field, but no significant decrease in the ipsilateral field. The degree of behavioral deficit correlated with the degree of face selectivity for that site (Figure 3)","The face-gender discrimination deficits was not observed for stimulation of nearby sites that were not enriched with face-selective neurons (Figure 3)","The better a site is at separating face gender, the more the monkey is impacted by its inactivation (Figure 4)","Optogenetic effects are consistent with effects from well-established muscimol microinjection. Notably optogenetic perturbation has much higher spatial and temporal resolution than muscimol (Figure 5)"],conclusions:["At least for this one face task, it appears that the monkey relies on activity of face-preferring IT sites more than nearby nonselective sites","It is possible that as a general pattern, regions that are best at some task are used for that task. This is a more general statement that should encompass results beyond the face patch system. And, importantly, it should not hold for regions from which behavior is not linearly decodable, e.g., V4","The contra vs. ipsi results here cast doubt on the idea that receptive fields in IT are so large that laterality is irrelevant"],other:["See Rangalijam et al., 2018 for more on reversible inactivation and task deficits in IT","This paper makes the interesting prediction that tasks that see little effect of, e.g., cortical cooling, may not be choosing tasks that appropriately require the ventral stream (e.g., invariance)"]}}},123:function(e){e.exports={metadata:{title:"Universal Mechanisms and the Development of the Face Network: What You See Is What You Get",authors:["Arcaro, MJ","Schade, PF","Livingstone, MS"],institutions:["Harvard University"],date:"2019-06",journal:"Annual Review of Vision Science",doi:"10.1146/annurev-vision-091718-014917",url:"https://www.annualreviews.org/doi/abs/10.1146/annurev-vision-091718-014917",keywords:["review","macaque","IT","faces","development"],review_date:"2019-07-30",one_sentence:"Arcaro et al. argue against a 'face-specific developmental program', and propose instead that there exists some set of developmental rules that applies equally throughout higher visual cortex"},review:{summary:["The face-processing network in primates can be understood by appealing to simple rules like the dominance of retinotopic maps, plasticity and development rules in other cortical areas, and visual experience. This perspective opposes the alternative view that faces are 'special' and have unique hardware or associated learning rules"],background:["Faces are of obvious ecological relevance, and primates spend an extraordinary amount of time looking at faces. Regions selectively responding to faces have been localized to stereoptypes locations in humans and non-human primates alike (along with regions selective for other categories, e.g., body parts, places), as supported by reports of acquired prosopagnosia","The belief that the face processing system is 'special' depends on the assumption that either 1) the neural hardware involved in face processing is unique or 2) the developmental program acting on that hardware is unique"],approach:["This is a review paper which primarily draws on studies using fMRI and electrophysiological recording in humans and macaques"],results:["Visual Hierarchy: Expansion of cortical territory by the formation of multiple mirrored maps appears to be a universal principle in brain development. Through this lens, the multiplicity of face regions at different stages is unsurprising (Figure 2; see also Conway 2018)","Developmental Rules: In early visual cortex, the patterns of orientation tuning, ocular dominance, etc, can be understood as self-organizing, activity-dependent mechanisms. I'm not sure I agree that this means that V1 doesn't 'serve a particular function but rather arises as a consequence of more general wiring rules', though. The authors propose that activity-dependent self-organization continues into all extrastriate regions, allowing each stage to represent some features across all of visual space","Developmental Rules: The fact that face-selective domains span multiple retinotopic maps instead of being isolated to a single retinotopic map suggests either 1) a break in the expected hierarchy, or 2) a lack of sufficiently high-resolution and high-coverage recording techniques.","How it Gets its Spots: The kinds of push-pull models that succeed in explaining early visual cortical phenomenology, e.g., ocular dominance stripes, might be useful in understanding how higher visual areas get parcelled out for different categories (see Plaut/Behrmann 2011)","How it Gets its Spots: The correlation between semantic category, curvature selectivty, and eccentricity makes it almost impossible to figure out in adults, and figuring out which feature set explains emergence first (in developmental time) has eluded current methods. The authors' stance is that in any case, regions of IT are effectively not specialized until later in life, although retinotopy appears present at birth.","Just Look for the Bare Necessities: Converging lines of evidence suggest that innate preference for face stimuli may result from a combination of an upper-visual-field bias and the saliency of small, round, dark things (i.e., eyes)","The final two sections of the paper discuss how 1) face regions continue to develop in adulthood and 2) how recent results from cross-modal stimulation suggesting that face regions always process face-like-stimuli may be explained by general connectivity patterns from IT cortex to other sensory regions in the brain."],conclusions:["Parsimonious explanations are both desirable and available for understanding the face processing system. The authors argue that the face-processing system emerges from simple rules that apply generally throughout cortex","They interpret this parsimony to mean that instead of asking what a brain region does, we should ask how universal rules act on environmental reinforcement. I mostly agree with that point, although I think the difference is rather semantic at that point. Isn't understanding how a brain region allows complex behavior via enviornmental reinforcement the same as asking what it does, or what it's good for? Ultimately, it may be that theories about how the system actually develops will separate these two apparently similar positions"],other:["This reads in many ways like a rebuttal to other recent reviews worth looking into, including Powell et al., 2019 and Op de Beeck et al., 2019"]}}},124:function(e){e.exports={metadata:{title:"Dissociations and Associations between Shape and Category Representations in the Two Visual Pathways",authors:["Bracci, S","Op de Beeck, H"],institutions:["KU Leuven"],date:"2016-01",journal:"J. Neurosci.",doi:"10.1523/JNEUROSCI.2314-15.2016",url:"http://www.jneurosci.org/cgi/pmidlookup?view=long&pmid=26758835",keywords:["shape","category","fMRI","human","RSA"],review_date:"2019-06-12",one_sentence:"Bracci and Op de Beeck create a clever stimulus set to demonstrate that ventral and dorsal regions represent object category even when accounting for perceived object shape"},review:{summary:["Higher visual cortical regions in both the ventral and dorsal streams represent object shape as well as category","The emergence of category information from posterior to anterior areas (even when explicitly controlling for object shape) supports models of visual cortex function that posit a disentangling of category information along each stream"],background:["Representations of objects in higher visual areas are thought to represent object category as well as visual features. For example, we can describe regions as being 'face-selective' or 'curvature-selective' depending on the specific question.","Most studies do not account for correlation between category and object shape (although many do account for correlations between, e.g., Fourier decompositions of stimuli."],approach:["Create a stimulus set in which shape and category are orthogonal: each object in a given category has a distinct shape, and for each shape, every category has an exemplar of that shape","Show the stimulus set to subjects to 1) collect responses in an event-related fMRI design and 2) judge behavioral similarity by the 'multiarrangement method'. Note that the subject pools for (1) and (2) are independent","Formalize three models: similarity in representations based on silhouettes, on shapes, or on categories, and identify regions whose activity is consistent with the predictions of those models","Use localizer and anatomical atlas to define ROIs in early visual cortex, the ventral stream, and the dorsal stream in each subject"],results:["The placement of objects in 2D when subjects were instructed to organize based on shape was indpendent of the placement when organizing by category (Figure 2), supporting the validity of the stimulus set","The activity of many ROIs correlates significantly with the shape-based model, whereas a smaller subset correlates with the category-based model (Figure 4). To me, this is the core result: that even when controlling for shape explicitly, traditionally category-selective regions still show substantial category-level information","Similarity in responses across ROIs, across subjects, suggests organization primarily along the posterior-to-anterior axis and secondarily along a ventral-to-dorsal axis (Figure 4)","Although both have category-level information, dorsal and ventral regions cluster differently, suggesting different types of category information in each subset of ROIs. Ventral ROIs are more consistent with an animacy model, whereas dorsal ROIs were more consistent with an action vs. non-action model (Figure 5)."],conclusions:["Many visual areas contain information about both shape and category, but this sitmulus set allows us to conclude that there is category-information above and beyond shape.","Category-information appears to proceed primarily along an A-P axis, consistent with existing theories of the emergence of category information outside of primary visual cortex (see DiCarlo et al., 2012)"],other:["The main result is interesting, but I don't know what to make of the post-hoc analyses. The stimulus set was designed to test a clear hypothesis, but the additional models (animacy, action, etc) used to explain the dissociation in activity between clusters were not explicitly controlled in the same way."]}}},125:function(e){e.exports={metadata:{title:"Development of orientation preference maps in ferret primary visual cortex",authors:["Chapman, B","Stryker, MP","Bonhoeffer, T"],institutions:[" Max Planck Society"," University of California San Francisco"],date:"1996-10",journal:"The Journal of Neuroscience",doi:"10.1523/JNEUROSCI.16-20-06443.1996",url:"http://www.jneurosci.org/content/16/20/6443.full.pdf",keywords:["ferret","orientation","development","V1","optical imaging"],review_date:"2019-08-22",one_sentence:"Chapman and colleagues show that while orientation tuning maps in ferrets begin emerging at different times, they are remarkably stable over the course of maturation"},review:{summary:["Ferret orientation maps appear between postnatal day 31 and 36, then mature with remarkable stability of iso-orientation domain position, size, etc","The emergence of orientation maps does not appear to be dependent on visual experience"],background:["Electrophysiological studies indicate that orientation maps in cats, monkeys, and other animals are mature very early in life, but the development of these maps in individual animals has not been observed","Ferrets are a desirable model organism as their gestation time is much shorter than the cat, although the development of the visual system is about equal in the two species. Thus, ferrets are physiologically more robust to do experiments on even as visual neuron selectivity is developing"],approach:["Intrinsic optical imaging of ferret V1 was conducted between P31 and P55 in eight animals","Animals were shown images of gratings at four orientations (horizontal, vertical, two obliques)","Orientation tuning was evaluated at each of multiple timepoints, then compared across timepoints or animals with a cross-correlation-inspired approach"],results:["V1 starts untuned to orientations, but develops orientation tuning between day 31 and 36, although this start is different in each animal (compare Figures 1, 2)","During maturation, tuning strength increases, although the location and size of iso-orientation domains is remarkably constant (Figures 1-3)",'Selectivity emerged first for vertical and horizontal orientations in "some, but not all, animals"','The pattern of orientation tuning along the cortical surface varied from animal to animal: in some it was spotty, in others stripy, and in at least one, not organized at all (Figure 5). This seems to conflict with the "common design" proposed by Wolf, Kaschube, and colleagues? In animals that have stripy patterns, those strips mostly run perpendicular to the V1/V2 boundary (Figure 6)',"Cells appear to acquire orientation tuning two days earlier when assessed with electrophysiology than when assessed with intrinsic imaging (Figure 7B)"],conclusions:["The stability of the maps, despite other changes thought to occur in V1 due to visual experience, points to the putative importance of orientation maps for... something","That orientation preference is detected earlier with electrophysiology suggests that either 1) something is insensitive about the optical imaging technique or 2) that cells with different tuning are more spatially mixed up early on (I don't think there's much support for this idea, in this paper or elsewhere)"],other:["The stability of maps aside, it's interesting how variable the onset of maturity and pattern of tuning on the cortical surface is across animals. In fact, it leads me to question how readily these results should be ported over to our thinking about other species, including humans"]}}},126:function(e){e.exports={metadata:{title:"Inputs to directionally selective simple cells in macaque striate cortex",authors:["De Valois, RL","Cottaris, NP"],institutions:["UC Berkeley"],date:"1998-11",journal:"PNAS",doi:"10.1073/pnas.95.24.14488",url:"https://www.pnas.org/content/95/24/14488.short",keywords:["macaque","V1","direction","magnocellular","parvocellular","biphasic"],review_date:"2019-05-16",one_sentence:"De Valois and Cottaris demonstrate that two classes of direction-nonselective simple cells, likely inheriting responses from distinct LGN pathways, drive direction-selective V1 cell activity"},review:{summary:["Spatiotemporal receptive fields of V1 cells in macaque can be strongly biphasic, and many are direction selective","The activity of direction-selective cells can be explained by two principal components, a biphasic and a monophasic component","These components match the timecourses of two classes of V1 simple cells without direction selectivity nearly perfectly"],background:["Theories had suggested that the inputs to direction-selective cells might be matched in preference for orientation and spatial frequency, but differ in phase. No experimental evidence had existed to support that claim","Existing motion models posit that the inputs to these cells would both have biphasic temporal response profiles"],approach:["Single cells in the central 5 degrees of macaque V1 were recorded, and selectivity for spatial frequency, orientation, and phase was mapped with drifting gratings (for determining preference) and black and white bars (for determing STRFs)","For each cell, a direction-selectivity index and a biphasic index were computed. Cells with a biphasic index less than 0.3 were considered weakly biphasic (except in Figure 3, for which the threshold switches to 0.4)","SVD was used to determine the first two principal components of the STRFs of direction-selective V1 cells"],results:["Many V1 cells have receptive fields that vary in time (Figure 1A, C), although some have monophasic and unchanging responses (Figure 1B)","Many V1 cells are biphasic in their temporal responses, although the majority are not (Figure 2)","The timecourses of non-directional-selective simple cells match the first two principal components of the direction-selective cells' receptive field (Figure 3)","Alternative models which posit that all inputs to direction-selective cells are biphasic do not fit the empirical data as well (Figure 4)","Most of the strongly biphasic cells preferred black and white to chromatic stimuli, while most of the weakly biphasic cells exhibited spectral opponency."],conclusions:["The inputs to direction-selective V1 cells are likely arriving from simple cells who, in turn, receive inputs from magnocellular and parvocellular LGN neurons. This conclusions is supported by measurements of repsonse latency and spectral opponency."],other:["It would have been great to see any attempt at quantification for either 1) how well the simple cell timecourses actually match the derived principal components and 2) how well the models described fit the data recorded."]}}},127:function(e){e.exports={metadata:{title:"Untangling invariant object recognition",authors:["DiCarlo, JJ","Cox, DD"],institutions:["MIT"],date:"2007-07",journal:"Trends Cogn. Sci.",doi:"10.1016/j.tics.2007.06.010",url:"http://dx.doi.org/10.1016/j.tics.2007.06.010",keywords:["IT","object recognition","manifold","hierarchy"],review_date:"2019-04-18",one_sentence:"DiCarlo and Cox argue that the role of the primate ventral stream functions to untangle retinal representations into representations from which useful information can be linearly decoded"},review:{summary:["Linear decoding of information such as facial identity is more difficult from the representation of a group of retinal ganglion cells than it is from a group of IT neurons","The role of the primate visual system may be to progressively untangle the retinal representation","The untangling perspective largely ignores biological features such as recurrence, noting that the timecourse of feedforward decodability precludes feedback as a necessity"],background:["The ability of primates to rapidly and reliably recognize visual objects under a wide range of lighting/viewpoint/pose conditions is attributed to a series of cortical areas termed the 'ventral visual stream'","The last stage of the ventral visual stream in macaque, inferotemporal (IT) cortex contains neurons from which object identity can be easily, i.e., linearly, decoded","One important point is that all of the information available to IT comes from the retina! It's just that the information in the retina is not formatted properly for efficient readout"],approach:["Theoretical arguments supported by some simulations of what manifolds for different objects in different representational spaces might look like"],results:["Pixel-like (maybe retina-like?) representations of two facial identities at different viewpoints are deeply tangled such that linear decoding would fail (Figure 1)","A V1-like simulation shows some disentangling of the retinal representation, but the IT-like representation is quite good at separating identities while maintaining some sensitivity to variables that may matter, such as viewpoint and pose. The authors not this differs from the 'classical' model of IT, in which information about those variables is discarded to maximize sensitivity to identity (Figure 2)"],conclusions:["Untangling of representations moves us away from thinking about single neuron response properties, and instead emphasizes where representations lie in a space spanned by a population of neurons","The task of the ventral stream is to separate identities of objects, but to simultaneously balance tolerance and generalizability to identity-preserving transformations of that object","Local and iterative 'flattening' of manifolds may be the implementational strategy of the ventral stream"],other:[]}}},128:function(e){e.exports={metadata:{authors:["Durbin, R","Mitchison, G"],institutions:["King's College","Stanford University"],keywords:["self-organizing map","orientation","wiring"],title:"A dimension reduction framework for understanding cortical maps",journal:"Nature",doi:"10.1038/343644a0",url:"https://www.nature.com/articles/343644a0",date:"1990-01",one_sentence:"Durbin and Mitchison argue that maps in early visual cortex can be understood as the solution to a self-organizing map problem that minimizes wiring length and reduces dimensionality",review_date:"2019-09-10"},review:{summary:["A self-organizing map approach to modeling the development of cortical maps is wiring-efficient, fast, and biologically plausible via Hebbian learning","Application of the self-organizing algorithm to stimuli defined by orientations and retinal position yield maps that are qualitatively similar to orientation maps observed in cat visual cortex"],background:["Retinotopy, ocular dominance, and orientation are all mapped relatively smoothly in the primary visual cortex of cats and macaques, which can be considered effectively 2D due to the columnar nature of V1 functional architecture","Because cortex is 2D and there are more than 2 stimulus dimensions to map (x, y, orientation, ocular dominance), the mapping from 'parameter space' to the cortical surface must contain imperfections","Prior work from these authors demonstrated that mapping a 2D parameter space onto a theoretical 1D cortex yields relatively smooth parameter maps when using a biological cost function that favors many short connections and allows relatively few long-range connections","Searching the space of all layouts becomes computationally difficult (and probably biologically implausible) in the mapping of 4+ dimensions to a 2D cortex, so we turn instead to self-organizing algorithms, e.g., Kohonen maps"],approach:["Create a Kohonen map in which each unit responds to some stimulus defined by (x, y, orientation angle, orientation tuning strength), and the update rule is based on a Gaussian neighborhood function","Compare the resulting map to orientation maps measured in cat primary visual cortex"],results:["For a simplified 2D parameter space -> 1D cortex mapping, applying a self-organizing algorithm yields results that are very similar in wiring cost and structure to the optimal layout (Figure 1c, d)","Mapping a 4D parameter space (x, y, orientation angle, orientation tuning) to a 2D space yields a mostly retinotopic map with singularities for orientation, as shown in cat (Figure 2). Of note is that the retinotopic map is warped near pinwheel centers in this model"],conclusions:["The self-organizing problem is both more biologically plausible and more efficient than direct minimization of wiring cost, yet it yields maps that are score very well on wire length minimization metrics","Brain evolution may have favored short connections and self-organizing maps as a way to quickly and robustly (although not perfectly) bootstrap the layout of stimulus features on the cortical surface"],other:["The self-organizing map algorithms require inputs that are fully formed stimuli, i.e., in this case they are points in a 4D space defining an orientation at some position in space. However, the maps are fully formed at birth. So, what are the stimuli/inputs to the model in utero? The authors cite Linsker '86 and Miller '89 for evidence that oriented, local inputs could drive development before eye opening."]}}},129:function(e){e.exports={metadata:{title:"A global map of orientation tuning in mouse visual cortex",authors:["Fahey, PG","Muhammad, T","Smith, C","Froudarakis, E","Cobos, E","Fu, J","Walker, EY","Yatsenko, D","Sinz, FH","Reimer, J","Tolias, AS"],institutions:["Baylor College of Medicine","University of Tubingen","Rice University"],doi:"10.1101/745323",date:"2019-08",journal:"bioRxiv",keywords:["mouse","V1","orientation","pinwheel","calcium imaging"],review_date:"2019-08-26",one_sentence:"Fahey and colleagues demonstrate that although small regions of mouse V1 may be salt-and-pepper (or micro-clustered) w.r.t. orientation tuning, there is a global orientation map that centers around one central pinwheel"},review:{summary:["Wide-field calcium imaging in eight mice indicates a large, area-wide gradient of orientation tuning that converges at a central pinwheel","The function and origins of this map remain unknown, but its existence suggests 1) a revision of how we think about orientation maps in mice and rats and 2) a role for emerging systems neuroscience techniques to identify novel phenomena in a field that's been studied to death"],background:['Maps of orientation tuning in primary visual cortex have been identified and rigorously quantified in primates and most carnivores (e.g., cats), but the story in mice and rats has always been that they exhibit "salt and pepper" tuning, or, at best, micro-columns of small degrees of clustering by orientation preference',"Cardinal bias (the over-representation of cells preferring vertical and horizontal orientations) has been previously reported in mice, and to a lesser degree, in primates. Already, this bias suggests that the orientation map in mice is not entirely random"],approach:["Measure calcium activity from L2/3 - L4 neurons in primary visual cortex in eight mice. Total imaging volume: 1800 x 1800 x 250-350 microns (very large!)","Play dynamic pink noise stimuli with coherent orientation and direction, then fit neuron responses with a two-peak von Mises function (similar to a circular Gaussian) for neurons that had more than 2.5% of variance explained. Total cells: 17k - 29k / animal"],results:["Many cells are strongly orientation tuned in V1, with a very large cardinal bias (Figure 1)","Collapsing neurons across depths makes it clear that there is a global orientation map, and projecting these points into stimulus-monitor space shows a clear pinwheel around the center of the screen (and thus, the center of retinotopic space; Figures 2, 3)","The pinwheel-like nature of the maps is relatively consistent across cortical depths (Figure 4)","The global pattern is obvious in individual animals, but the degree to which it's clearly pinwheel-like is less convincing (Extended Data Figures 3, 5)","The orientation bias cannot be easily explained by border effects, as it persists when stimuli are restricted away from the edges of the monitor (Extended Data Figure 7)"],conclusions:["The unprecedented scale of this recording was critical to discovering this large map that spans all of V1 and extends into neighboring areas -- if only focusing on small imaging regions, there is a lot of heterogeneity in tuning. Eshed's note: how do we reconcile this with the extreme precision of pinwheels in cats (Ohki et al., 2006)? What's different about the development of mice?","Large field-of-view imaging may be a useful tool in continuing to 'revisit' principles of cortical organization"],other:["I think this paper will be very impactful, as it disrupts a difficult-to-reconcile notion that despite the presence of orientation tuned cells, their position is totally irrelevant. In my mind, it's easier to think about these maps, both within and across species, as points on a continuum between extremely precise organization and more lax organization","What could be the computational or metabolic advantages of having one mega-pinwheel instead of regularly tiled pinwheels?"]}}},130:function(e){e.exports={metadata:{title:"Color and orientation are jointly coded and spatially organized in primate primary visual cortex",authors:["Garg, AK*","Li, P*","Rashid, MS","Callaway, EM"],institutions:["Salk Institute","UCSD"],date:"2019-06",journal:"Science",doi:"10.1126/science.aaw5868",url:"https://science.sciencemag.org/content/364/6447/1275",keywords:["macaque","color","orientation","CO"],review_date:"2019-07-01",one_sentence:"Garg, Li, et al. find that many orientation-tuned macaque V1 cells are also tuned to specific hues (and organized with respect to CO blobs), challenging standing models of V1 functional architecture that posit spatial and functional separation between color and oriention tuning"},review:{summary:["Instead of being strictly segregated, many V1 neurons jointly code stimulus color and orientation","Neurons were clustered with respect to hue preference and generally (but not completely) aligned with CO-dense regions"],background:["The canonical model of V1 is that there are orientation-selective cells outside of CO blobs, and these cells are generally not color-selctive","Eshed's note: when training deep convolutional neural networks to do object recognition tasks, many of the early filters violate this separate and show both chromatic and orientation tuning in the same units"],approach:["GCaMP6f imaging in macaque of 4,531 cells in superficial layers of V1 from four animals and fourteen imaging sites","Responses collected in response to square-wave gratings (eight orientations, four spatial frequencies) that were either achromatic or one of 12 equally spaced colors in CIE space","CO staining after euthanasia at imaging sites to determine correspondence between color/orientation tuning and CO","Measurements at multiple depths to determine columnar structure","Only consider responses from visually-responsive neurons"],results:["Over 46% of cells preferred chromatic stimuli to achromatic stimuli, and 11% of all cells were both strongly orientation selective and strongly color selective (Figure 3A)","Orientation selectivity generally decreased as CO intensity increased, while color preference increased with CO intensity, consistent with prior findings (Figure 3B, 3C)","97% of color preferring neurons were also tuned to at least one specific hue, with the majority of hue-selective neurons tuned to blue and red (likely due to cone contrast in the retina)","Neurons generally clustered in the xy-plane on the basis of hue preference and were organized similarly at nearby cortical depths (Figure 4A)"],conclusions:["Shape and color may be jointly processed by a population of cells outside of CO blobs that likely project to V2 pale stripes","These recordings were made in superficial layers, but deeper layers (closer to LGN input in layer IV) may show different functional signatures"],other:["Was there any tuning with respect to spatial frequency? If not, how were those data used?","Looking at the supplement, it's clear that there is clustering with respect to orientation and color tuning, but the typical patterns (e.g., pinwheels) are hard to see in these maps. I imagine they would look cleaner in ISI maps than these fine-grained 2P imaging maps"]}}},131:function(e){e.exports={metadata:{title:"The functional architecture of the ventral temporal cortex and its role in categorization",authors:["Grill-Spector, K","Weiner, KS"],institutions:["Stanford University"],date:"2014-08",journal:"Nat. Rev. Neurosci.",doi:"10.1038/nrn3747",url:"https://www.nature.com/articles/nrn3747",keywords:["VTC","human","higher visual cortex","functional architecture"],review_date:"2019-05-10",one_sentence:"Grill-Spector and Weiner review evidence for the central role of VTC in object categorization"},review:{summary:["VTC is organized in a nested spatial hierarchy, such that coarse object distinctions are represented at coarse spatial scales and fine-scale distinctions are represented at fine spatial scales."],background:["The ventral visual stream (V1, V2, V3, V4, IT) is involved in visual perception and the recognition of objects","Neurons are VTC are not arranged randomly, but instead organize into reliable topologies"],approach:["Most results reported use either fMRI (human and non-primate) or electrophysiology (non-human primate)"],results:["VTC is bounded by a set of anatomical landmarks: from the ptCoS to the anterior tip of the MFS along the A-P axis, and by the OTS and CoS on either side (Box 1)","VTC representations support categorization, but with some degree of tolerance for identity-preserving transformations. That is, generalization with specificity (Figures 1, 2)","Neurons selective for different ecological object categories cluster in higher visual cortex (Figure 3)","The MFS marks transitions in anatomical and function properties. Within each half of VTC (split at the MFS) there is a nested spatial hierarchy of object information (Figures 4, 5)"],conclusions:["The functional architecture of VTC, and in particular, the superimposition of representations at different semantic scales, allows information to both converge and diverge through different routes","This flexibility allows rapid and reliable categorization, but the exact implementation details of how representations are superimposed remains an open question"],other:[]}}},132:function(e){e.exports={metadata:{title:"A common, high-dimensional model of the representational space in human ventral temporal cortex",authors:["Haxby, JV","Guntupalli, JS","Connolly, AC","Halchenko, YO","Conroy, BR","Gobbini, MI","Hanke, M","Ramadge, PJ"],institutions:["Dartmouth College","Universita degli studi di Trento","Princeton University","Universita di Bologna"],date:"2011-10",journal:"Neuron",doi:"10.1016/j.neuron.2011.08.026",url:"https://www.ncbi.nlm.nih.gov/pubmed/22017997",keywords:["human","fMRI","MVPA","hyperalignment","VTC"],review_date:"2019-07-16",one_sentence:"Haxby and colleagues introduce a method for aligning brain data between subjects that outperforms anatomical alignment in between-subjects classification"},review:{summary:["Individual brain data can be mapped into a high-dimensional space that is common to all subjects. This space can be used to achieve very high between-subject's classification that does not require anatomical alignment"],background:["Multi-variate pattern analyses are widespread in human neuroimaging. In many studies, classifiers are built to predict a variable of interest from the distributed pattern of activity in a single brain area. But because each brain is different, this requires a new representational space (defined by a set of voxels) in each subject, making intersubject comparison tricky","Getting around this problem has been approach in a couple of ways: 1) the anatomy is abstracted away in approaches like representational similarity analysis (RSA), or 2) individual anatomies are warped into a space common to all subjects"],approach:["3 experiments: In E1, 21 subject watched Raiders of the Los Ark. In E2, 10 of those 21 also viewed images of object categories, and in E3 11 subjects viewed images of animal species.","Hyperaligment in E1 begins with 42 matrices (21 subjects, left and right VTC). Each matrix is 500 voxels x 2205 time points. A procedure based on iterative Procrustean transformation (rigid transforms) are performed to bring every subject into a common 1000-D space (2 hemispheres x 500 dimensions x 2205 time points). Finally, those 1000-D vectors are reduced to 35 dimensions via PCA","Perform between-subject classification to determine the validity of the common space. Predict one subject's brain activity from the other 20. The between-subject performance is then compared to both within-subject performance and Talairach-space-aligned brains"],results:["Hyperaligned between-subjects classification of which portion (18 seconds) of the movie was being seen at a given time was at 70.6%, far higher than the anatomically-aligned classifier at 32.0%. (Figure 2)","In E2, hyperalignment performs at 63.9%, which was significantly higher than the 44.6% afforded by anatomical alignment. For comparison, within-subjects classification also achieves 63% (Figure 2) In E3, hyperalignment also performs much better than anatomical alignment (Figure 2)","Classification accuracy improves up to about 20 PCs (from the original 1000), but somewhat plateaus above that. It only takes about 12 PCs to accurately decode the faces and objects, and only 9 to decode the animal species (Figure 3)","Building the common space with complex stimuli (the movie) is generally effective regardless of the 'test' data, but creating the common space based on simple or narrow datasets does not generalize well to complex stimuli (Figure 4)","The first few principal components of the 35D space don't map onto hand-drawn ROIs (e.g., FFA; Figure 5) very well, but specifically searching for dimensions that have a high faces vs. places contrast aligns very well with hand-drawn ROIs (Figure 6)"],conclusions:["Hyperalignment is useful for estimating the dimensionality of human VTC as well as predicting brain activity in held out subjects. The validity of this common space depends on acquiring rich, complex data"],other:["See Guntupalli et al., 2016 (Cereb. Cortex) for an application and extension of hyperalignment","I haven't seen this method get used as often as anatomical alignment techniques. Is that because acquiring a rich enough dataset is difficult?"]}}},133:function(e){e.exports={metadata:{title:"Receptive Fields and Functional Architecture in Two Nonstriate Visual Areas (18 and 19) of the Cat",authors:["Hubel, DH","Wiesel, TN"],institutions:["Harvard"],date:"1965-03",journal:"J. Neurophysiol.",doi:"10.1152/jn.1965.28.2.229",url:"http://dx.doi.org/10.1152/jn.1965.28.2.229",keywords:["cat","V2","V3","complex","hypercomplex"],review_date:"2019-04-11",one_sentence:"Hubel and Wiesel's discovery of hypercomplex cells in cat V2 and V3 fuels a tradition of hand-picking stimuli that might cause cells to spike"},review:{summary:["V2 and V3 are topographically arranged in the cat brain and exhibit columnar organization","Hypercomplex cells, which are typically end-stopped, compose 10% of cells in V2 and about 50% in V3","V2 and V3 align with regions defined by microscopic anatomy and receive projections from V1"],background:["In H&W's 1962 exploration of V1, simple cells were established by their sensitivity to bar orientation, spatial frequency, and position. Complex cells additionally have position invariance and are found co-columnar with simple cells of the same orientation and spatial frequency preference","Cytoarchitectonically distinct regions 18 and 19 had been established in the cat visual cortex, but the functional nature of these cells was unkonwn"],approach:["Microelectrode penetrations in 17 cats in and around areas 17, 18, and 19","Pick some stimuli to show to the cell, try to identify its preferred stimulus","Histology to see where the cells were in the cortical sheet"],results:["V2 and V3 have a topographic representation of visual space (Figures 1-3)","Transitions between areas identified functionally coincide with boundaries seen in Nissl and myelin stains (very unclear to the untrained eye; Figure 3)","Both V2 and V3 have hypercomplex cells, which prefer bars or slits of light (or dark) within the 'activating region' of the receptive field, but decrease activity if the light (or dark) extends into the 'antagonistic region' (Figures 8-22)","Some hypercomplex cells are invariant to 90 degree rotations, and both orientations are found in the preferences of cells in the same column (Figure 30). These cells are given the designation 'higher-order'","Orientation tuning changes gradually parallel to the cortical surface and is stable perpendicular to the cortical surface in both V2 and V3 (Figure 32)","Puncturing V1 with a 25 gauge needle and tracing degenerating axons shows that V1 projects to discrete parts of V2, V3, and the suprasylvian gyrus"],conclusions:["Much as simple cells are thought to form from LGN projections into layer 4C of V1 and complex cells are thought to form from simple cell projections within a V1 column, hypercomplex cells might form from a combination of excitatory and inhibitory projections onto cells in V2 and V3","Higher-order hypercomplex cells are a neat example of a transformation that both increases generalization and specificity","These experiments are a far way off from explaining perception, but they are a start"],other:["On the term 'hypercomplex', they write 'The word hypercomplex, while ugly and  unwieldy, and perhaps also unwise in view of the  possibility of finding even more complex cells,  is  at least relatively neutral.'","While the results have been undoubtedly impactful, I have two main criticisms of this work, although both may be artifacts of their time. First, there is tremendous bias in the neurons chosen to record from and the images selected by the authors to present to the animals. Second, these results have not propagated very far in the modern visual neuroscience discourse -- the term hypercomplex is rarely used and no longer taught in many introductory courses. Do characterizations of this nature still have value?"]}}},134:function(e){e.exports={metadata:{title:" Computational Consequences of a Bias toward Short Connections",authors:["Jacobs, RA","Jordan, MI"],institutions:["MIT"],date:"1992-09",journal:"Journal of Cognitive Neuroscience",doi:"10.1162/jocn.1992.4.4.323 ",url:"https://www.mitpressjournals.org/doi/abs/10.1162/jocn.1992.4.4.323",keywords:["neural network","wiring cost","spatial"],review_date:"2019-06-19",one_sentence:"Jacobs and Jordan show that simple neural networks optimized for task and a 'short connection bias' allows the network to learn multiple tasks in a spatially-efficient way"},review:{summary:["Short connections obtained via pruning are sufficient to explain modularity in a variety of computational settings","The emergence of topographic maps may be explained by a realtively simple supervised cost that favors short connections"],background:["Whether the apparent modularity of the brain is genetic or learned is a matter of debate among neuroscientists","Modularity is efficient from a space, energy, and multi-task learning perspective; that is, it is one way to avoid 'catastrophic forgetting'","Much work on modularity occurs in an unsupervised context, e.g., Kohonen maps that represent inputs to the system topographically. Supervised approaches to understanding modularity are more rare","Modularity could arise either by selective addition of connections, or selective pruning of connections"],approach:["Assign neural network units positions in a 3D space, such that the distance between any pair of units can be evaluated","Train relatively simple (my modern standards) neural networks to perform tasks while adhering to a spatial constraint. The network can learn to set some weights to zero, akin to the pruning hypothesis for modularity","The spatial constraint used is for a pair of units is d * w ** 2 / (1 + w ** 2), where d is the distance between them and w is the weight between them","Evaluate the 'spatial' structure of network weights following optimization"],results:["For a network trained on a simulataneous 'what/where' task (identifying or locating a pattern), half the weights are used for the first task and half are used for the second task (Figures 5, 6). If the output nodes for the two tasks are brought closer together, there is more spatial continuity to the structure of the non-zero weights (Figure 7)","In a temporal shift detection task, units in adjacent layers form short local loops (as found in cortex; Figure 9)","In a dimensionality reduction task, hidden layers units form a topographic representation of the input, serving as a supervised analogue to the unsupervised methods of Kohonen or Durbin/Mitchison"],conclusions:["This work supports the framework that cortex is initially homogenous, and that a learning rule that favors short local connections can explain the emergence of modularity. Of course, that hypothesis neglects the tremendous genetic variability in the developing brain, but is nevertheless useful in thinking about neurodevelopment","That all connections are initialized as non-zero here, and some are learned to be zero, is taken as evidence that elimination of synapses may be the dominant mechanism in pruning that contributes to modularity","Whether systems start with long connections and prune down or have many short connections that are pruned reamins an open question"],other:["'If biological neural systems are biased to use short connections, then it is important to know how neurons determine their relative positions.' Very true! This is likely genetic, but may also be instructed in some way by spontaneous activity preceding eye opening","How would the supervised loss (applied via backprop) actually be biologicall instantiated?"]}}},135:function(e){e.exports={metadata:{title:"Classification of columnar and salt-and-pepper organization in mammalian visual cortex",authors:["Jang, J","Song, M","Paik, S-B"],institutions:["Korea Advanced Institute of Science and Technology"],date:"2019-07",journal:"biorXiv",doi:"10.1101/698043",url:"https://www.biorxiv.org/content/10.1101/698043v1",keywords:["orientation","retina","Moire","V1","Nyquist","pinwheel"],review_date:"2019-08-10",one_sentence:"Jang et al., demonstrate that whether an animal's V1 is organized in a columnar or salt-and-pepper pattern can be predicted empirically by the ratio of retinal neurons to V1 neurons and theoretically by the Nyquist sampling ratio"},review:{summary:["While the presence of orientation columns or salt-and-pepper tuning cannot be predicted from a single parameter, e.g., V1 size, it can be predicted by the combination of V1 size and retina size","The retina/V1 size ratio that separates columnar from salt-and-pepper species is predicted by the Nyquist sampling theorem"],background:["While neurons in V1 are tuned to specific orientations in many species, the organization of such orientation-selective neurons varies both parallel and perpendicular to the cortical sheet. The organization parallel to the cortical sheet has been the subject of intense study, as in some (but not all) mammals the neurons are organized into columns that change smoothly in their orientation preference","A somewhat common viewpoint is that the difference between species emerges not from species-specific developmental mechanisms, but from a universal developmental mechanism that acts on differences in particular parameters between species","To date, V1 size, retina size, and orientation tuning spatial structure have been thoroughly described in eight mammalian species: Tree shrew [C(olumnar)], Ferret [C], Cat [C], Macaque [C], Mouse [S(alt and ) P(epper)], Rat [SP], Squirrel [SP], and Rabbit [SP]"],approach:["Modeling of retinal ganglion cell (RGC) connectivity to V1 by assuming Gaussian receptive fields, then computing orientation tuning of model V1 neurons (Extended Data Figure 2)","The specific model used here is that of retinal mosaics in a Moire interference pattern (see Paik/Ringach '11 for details). This model uses the observed offsets in positions of ON/OFF RGC feedforward afferent pairs to V1 to predict the resulting orientation tuning in the retinotopically matched V1 region to which they project.","Comparison to 1) measured V1 size, retinal area, and number of RGCs in each animal species and 2) orientation tuning patterns observed (or not observed) in each animal"],results:["The presence of columnar organization cannot be well predicted by V1 acuity, body weight, retina size, or V1 size (Eshed's note: you can come very close to a clean separation with V1 size alone, suggesting that projecting onto an additional dimension that is even slightly independent will allow perfect classification. This seems like an area where more data from other species would be very helpful; Figure 1)","Classifying species in a 2-D space defined jointly by V1 and retina size allows for perfect separation, whether going by surface area or by estimated number of neurons (Figure 1)","Assuming the same exact retinal mosaic and varying only the ratio of V1 to RGC cells, the resulting orientation maps can shift from columnar to salt-and-pepper (Figure 2). Values near those recorded for the cat (V1:RGC = 2.5) yield columnar structure, while those recorded for the mouse (V1:RGC = 0.1) yield salt-and-pepper structure. Eshed's note: kind of? The salt-and-pepper map is so sparse that even if it were columnar it would be hard to tell.","Because a V1:RGC ratio of less than 2 might be susceptible to sample aliasing by the Nyquist Sampling Theorem, i.e., the sampling of orientations from the full spectrum might appear lower frequency than it really is, the authors predict and show that there is a phase transition between columnar and salt-and-pepper maps at a sampling ratio of 2. Indeed, this value separates the 6 species for which the data are fully available into their correct pattern groups (columnar or not). (Figure 2)"],conclusions:["Columnar and salt-and-pepper organizations, while obviously different, might share a common developmental mechanism and act on differences in the V1:RGC sampling ratio","There are many differences between, e.g., mice and cats, including RGC types and convergence ratios from RGCs to the LGN neurons. However, the authors here argue that those results are dwarfed by the similarities in ON/OFF afferents across species.","Eshed's note: in general I support the idea that common developmental mechanism act on different substrates, and I think the appeal to alias sampling theory is elegant here. However, it appears that a more rigorous match to specific patterns, e.g., pinwheel density, is lacking."],other:["This paper is the latest in an installment of arguments between two groups: the Paik/Ringach group (this paper) and the Wolf/Kaschube group, who argue that this retinal mosaic model fails to capture the correct statistics of V1 in four divergent species and ignores large-scale cortical organization principles in favor of randomness. See Kaschube et al., 2010, Schottdorf et al., 2014, 2015 for the Wolf/Kaschube perspective, and this paper + Paik & Ringach, 2011, for more."]}}},136:function(e){e.exports={metadata:{title:"Deep supervised, but not unsupervised, models may explain IT cortical representation",authors:["Khaligh-Razavi S-M","Kriegeskorte N"],institutions:["Medical Research Council, Cognition and Brain Sciences Unit"],journal:"PLOS Computational Biology",doi:"10.1371/journal.pcbi.1003915",url:"http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003915",date:"2014-11",review_date:"2019-09-25",one_sentence:"Khaligh-Razavi and Kriegeskorte compare 37 computer vision models to the representational geometry of human and macaque IT, and find that only features from an ImageNet trained neural network (with recombination of features) was able to explain the data",keywords:["macaque","human","rsa","deep neural network","computer vision"]},review:{summary:["Not-strongly-supervised models (those with no or relatively little supervision on object tasks) do not explain IT geometry even after feature reweighting and remixing","Features from a deep convolutional neural network are able to explain the representational geometry, but only after reweighting and remixing of features"],background:["Yamins et al., 2014 demonstrated that learning a linear mapping from object-recognition-trained neural networks to IT neurons beats a linear mapping from any other computer vision model. However, this approach requires a large amount of high-quality data","Representational similarity analysis (RSA) allows an alternative method to matching feature spaces to brain data in which the representational geometry of a stimulus set can be compared","There is an open question as to whether the response patterns in IT can be explained by an intelligently-selected feature space, or if supervised learning of massive neural networks is the only approach that yields a good fit to biological measurements"],approach:["Datasets include monkey electrophysiology from Kiani et al., 2007 and human fMRI data Kriegeskorte et al., 2008. In both cases, RDMs are constructed","37 candidate computer vision models are tested, ranging from V1-like models, to HMAX, to deep convnets. A variety of approaches to re-weighting the feature spaces produced by these models (stretching the feature space toward some end) or remixing the feature spaces (finding new linear combinations, i.e., affine transformations of the space) are performed","Kendall's Tau is used to compare feature spaces to biology. Furthermore, an interesting approach is introduced in which model RDMs are weighted in a least-squares regression to explain the observed RDM",'In this context, "remixing" features means using SVM decision boundaries on three tasks: body vs nonbody, face vs. nonface, and animate vs. inanimate as new features and constructing RDMs from those. One expects that if the representation is able to perform these three tasks well, then these remixed features will match the IT representation (which we know is strongly categorical; Figure 4)'],results:['Of the not-strongly-supervised models (those without many labeled images used while learning the representation), most have only a small amount of similarity to human and macaque IT and do not show much categorality. A combination via feature concatenation of all of these models, termed "combi27", is the closest match to biological data but still leaves a substantial gap to be explained (Figures 1, 2, 3, 4)',"Remixing or reweighting of the not-strongly-supervised models does not close the gap to biological data, presumably because their representations are not categorical in the way IT is (Figure 5)","Alexnet (trained on Imagenet) matches IT much better than the not-strongly-supervised models, but still requires feature remixing and reweighting to completely close the gap (Figures 6, 7, 8, 9, 10)"],conclusions:["Strong supervision is required to build a representation that matches that in human and macaque IT. However, strong supervision may not be enough: the categorical boundaries that are important to IT, e.g., animate vs. inanimate, need to be weighted more heavily than other boundaries.",'This work raises an interesting question regarding the definition of visual object "categories", especially given that low-level differences explicitly available in, e.g., the retina, often correlate with semantic labels. They write that "a representation is \'categorical\' when it affords better category discriminability than any feature set that can be learned without category supervision, i.e., when it is designed to emphasize category divisions." This is an interesting, but perhaps nonstandard definition in my opinion.'],other:["Compared to direct linear fits, the RSA approach seems to be less computationally intensive, but also requires complex post-hoc manipulation of features, e.g. constructing a new, warped feature space from SVMs fit on separate tasks. In practice I assume that the choice of methods (RSA vs. direct fits) likely doesn't matter all that much"]}}},137:function(e){e.exports={metadata:{title:"Neural Coding for Shape and Texture in Macaque Area V4",authors:["Kim, T","Blair, W","Pasupathy, A"],institutions:["University of Washington"],date:"2019-06",journal:"J. Neurosci",doi:"10.1523/JNEUROSCI.3073-18.2019",url:"http://www.jneurosci.org/content/jneuro/39/24/4760.full.pdf",keywords:["macaque","V4","shape","texture"],review_date:"2019-07-08",one_sentence:"Kim et al. demonstrate that V4 neurons encode both an object's shape and its texture, with stronger selectivity and faster responses on average for shape stimuli"},review:{summary:["Individual V4 neurons lie on a continuum between strongly shape-selective and strongly texture-selective","The populations are thus 'partially overlapping'. Indeed, individual neurons that are strongly selective for shape can be observed"],background:["V4 neurons have been found to be selective for shape and surface properties such as color and brightness, but less attention has been given to texture","Adelson, Movshon, Simoncelli, and others argue that early/intermediate visual areas are responsible for the representation of 'stuff' instead of 'things'","It is unknown how V4 accomplishes the joint representation of shape, color, texture, and more. It could be performed by groups of neurons, each tackling part of the problem, or it could be the case that individual neurons jointly encode multiple dimensions of stimuli"],approach:["Single unit recordings (N=101) from V4 in two macaques. Grayscale stimuli presented at two sizes: entirely within unit's RF or at twice the RF diameter","Stimuli were drawn from 225 shapes (from prior work) or 168 textures defined by three axes: directional/nondirectional, regular/irregular, and coarse/fine"],results:["52 of the 101 neurons had a higher peak response for shapes than peak response for texture (after controlling for number of stimuli in each set), whereas only 19 neurons had a stronger peak response to texture (Figure 2)","HMax can predict neuron responses to shape stimuli, but not texture stimuli (Figure 3)","Tuning for shape and texture in individual neurons (note, this is a subset of the original 101 neurons) is largely independent (Figure 7)","The onset of selectivity was significantly later for texture stimuli than shape stimuli (Figure 8)"],conclusions:["Given that shape generally yielded stronger and faster selectivity than texture is consistent with frameworks suggesting that it is an object's outline that determines our percept (see the work of I. Biederman), instead of texture","Notably, recent work (Geirhos et al., 2018) has shown that leading neural network models default to using texture information, indicating a disconnect between artificial and biological vision systems","We shouldn't completely ignore texture, since a substantial portion of V4 neurons do encode texture along seemingly relevant dimensions"],other:["Given my personal interest in functional architecture, it would have been great to see where these units were and how their responses correlated with their cortical positions. Are functionally overlapping populations also spatially overlapping?"]}}},138:function(e){e.exports={metadata:{title:"Functional implications of orientation maps in primary visual cortex",authors:["Koch, E","Jin, J","Alonso, JM","Zaidi, Q"],institutions:["State University of New York"],date:"2016-11",journal:"Nature Communications",doi:"10.1038/ncomms13529",url:"https://www.ncbi.nlm.nih.gov/pubmed/27876796",keywords:["pinwheel","orientation","V1","cat","BA17","contrast"],review_date:"2019-02-28",one_sentence:"This work establishes the functional properties of pinwheel-center neurons in cat V1 and proposes that these neurons may be useful for recognition of multi-orientation textures"},review:{summary:["Neurons in orientation pinwheels are more broadly tuned to orientations and do not saturate as quickly with increasing contrast","Neurons in pinwheels are not as strongly suppressed by cross-orientation plaid gratings","Normalizing responses by local divisive normalization cannot account for empirical results, but normalizing by response of local interneurons can","Argues for functional implications of having pinwheels: having neurons be broadly tuned without suppression saturation is useful for recognizing textures composed of multiple orientations"],background:["Pinwheels in orientation-tuning maps are common in many animals but lacking in others. There is no consensus on whether there is functional utility to having orientation maps","Neurons in pinwheel centers are generally less orientation-selective, which may help with processing stimuli composed of many orientations. But! V1 neurons generally exhibit cross-orientation suppression, which would greatly weaken those responses."],approach:["Record from cat V1 using horizontal linear track electrodes separated by 100 microns.","Evaluate neuronal response characteristics for cells in iso-orientation domains and pinwheel centers.","Develop an image-computable model of V1 neurons that can explain the empirical data"],results:["Neurons in iso-orientation domains are strongly orientation selective (Figure 1a), exhibit strong response saturation (Figure 1c; 1g), and have strong cross-orientation suppression (Figures 1-3)","Model of thalamocortical projections and cortical normalization can reproduce empirical results if an excitation-normalization (EN) model (normalization by non-oriented nearby neurons) is used (Figure 4). The model predicts responses in an experiment with novel stimuli and 3 new subjects (cats), Figure 5)","Divisive normalization would yield results opposite the empirical findings unless unbiological assumptions are made re: the origin of inhibitory projections (Figure 6)","Using a horizontal linear track to record (as opposed to a 2D method) leads to similar conclusions re: whether you're in a pinhweel or iso-orientation domain"],conclusions:["Neurons in pinwheels respond differently to inputs and experience different normalization from surround cells. This allows them to represent multi-orientation patterns without suffering from suppression and saturation","Animals without pinwheels (or any orientation map) may sacrifice pattern recognition for robust edge detection","Local divisive normalization does not account for the empirical results, as it would have the effect of broadening tuning curves in iso-orientation domains and narrowing them at pinwheel centers."],other:["Follow-up reading: Levy, M., Lu, Z., Dion, G. & Kara, P. The shape of dendritic arbors in different functional domains of the cortical orientation map. J. Neurosci. 34, 3231\u20133236 (2014)."]}}},139:function(e){e.exports={metadata:{title:"Orientation Preference Patterns in Mammalian Visual Cortex: A Wire Length Minimization Approach",authors:["Koulakov, KA","Chklovskii, DB"],institutions:["Salk Institute","Cold Spring Harbor Laboratory"],date:"2001-02",journal:"Neuron",doi:"",url:"",keywords:["pinwheel","wiring cost","icecube","orientation","V1"],review_date:"2019-03-07",one_sentence:"A simple wiring-cost minimzation model is suggested as the underlying explanation for the appearance of orientation-tuning maps in primary visual cortex"},review:{summary:["Purpose of pinwheels in orientation tuning maps is unknown -- which cost function (task-based, physiological, etc) do they help minimize?"],background:["Self-organizing map models, e.g., Durbin and Mitchison, suggest that pinwheels are a developmental irregularity that occurs if what you're _actually_ trying to do is end up with an Icecube-like structure (repeating bands), see Wolf and Geisel 1998","60% of cortical volume is axons and dendrites (Brait- enberg and Sch\xfcz, 1998), so minimizing neurite length is important for keeping brains small","Most inputs to cortical cells are cortical as opposed to thalamic, so minimizing intracortical wiring length was likely selected for over evolutionary time"],approach:["Model orientation tuning maps by minimizing wiring-cost functions of different forms. Specifically, compute wiring-length after deciding on a neuron-connection function (which neurons connect with which other neurons, given their orientation preference?)","Test which functions give maps consistent with biological findings"],results:["If each neuron connects to an equal number of neurons of all preferred orientations, i.e., connection does not depend on orientation preference, the optimal layout is a salt and pepper pattern as observed in rats (Figure 2a-b)","A wide Gaussian connection function, i.e., generally connected to neurons of like orientations, yields the Icecube layout (Figure 2c-d)","A narrow Gaussian function yields pinwheel-like structures, as neurons need to connect both to like-orientation neurons and cross-orientation neurons (Figure 3)","Periodicity of icecube and pinwheel maps depends on the number of connections each neuron is thought to make-- more connections means a higher frequency pattern","Wire-length minimization applied to direction-preference maps are consistent with empirical findings, suggesting it may apply in that context as well (Figure 5)","Irregularities in maps in other animals can be attributed, in part, to incomplete development due to biological noise, blood vessels, anchoring to other maps, etc, see Figure 6."],conclusions:["Emergence of pinwheels can be explained by a simple wiring-cost minimization paradigm. This same rule can explain the emergence of linear zones (as on the V1/V2 boundary) if one assumes a different connection function for those neurons -- this seems reasonable to me","Lack of structure, as in rat, may also be due to a different connection function for V1 neurons","Coupling with other maps is likely weak, given that the same orientation map is seen in many species, including those without maps like ocular dominance (EM: but lots of evidence for alignment between maps nonetheless)","Continuity-based approaches like self-organizing maps are implicitly enforcing a wiring-cost minimization"],other:["Koulakov and Chklovskii write 'we postpone the treatment of the interaction between different cortical maps until more experimental evidence is available.' -- is there sufficient evidence now?","The authors find these maps by updating one neuron's orientation preference, recomputing connections, then recomputing wiring, which can be really slow-- they write that it takes almost 20 days to optimize a grid of 3600 neurons!"]}}},140:function(e){e.exports={metadata:{title:"Matching categorical object representations in inferior temporal cortex of man and monkey",authors:["Kriegeskorte, N","Mur, M","Ruff, DA","Kiani, R","Bodurka, J","Esteky, H","Tanaka, K","Bandettini, PA"],institutions:["NIH","Mastricht University","University of Washington","Shaheed Beheshti University","Institute for Studies in Theoretical Physics and Mathematics","RIKEN Brain Science Institute"],date:"2008-12",journal:"Neuron",doi:"10.1016/j.neuron.2008.10.043",url:"http://www.sciencedirect.com/science/article/pii/S0896627308009434",keywords:["macaque","human","RSA","MDS"],review_date:"2019-04-30",one_sentence:"Monkey and human representations of objects are strikingly similar despite differences in species and acquisition techniques"},review:{summary:["Images of 92 objects were presented to both human and macaque subjects, and representational distance matrices (RDM) were compared","Both RDMs and multi-dimensional scaling point to striking similarity between representations in the two species"],background:["Macaque is one of the most often used model organisms for human vision, yet direct comparisons of the visual system are mired by difference in neuroanatomy and recording techniques","Some progress has been made by using fMRI in both species, but this doesn't solve the issue of mapping parts of the monkey brain to parts of the human brain"],approach:["In two macaques, record 674 single neurons in anterior IT","In four humans, run a rapid event-related design at a spatial resolution of 1.95 x 1.95 x 2 mm, then define human-IT anatomically","Construct RDMs and MDS embeddings in each species, then compare","Also compare early visual cortex responses in humans to human-IT responses"],results:["RDMs, MDS embeddings, and hierarchical clustering trees look qualitatively similar between monkeys and humans (Figures 1, 2, and 4)","Quantitatively, the similarity of the dissimmilarity vectors across species is fairly high for many different kinds of stimuli. One place this interspecies similarity breaks down is within images of human faces or within images of macaque faces (Figure 3)","Category boundaries are strong in macaque IT and human-IT, but weak in early visual cortex in humans. Still more category structure than expected by chance in V1, but much noisier than in IT. The dissimilarity in early visual cortex is predictive of dissimilarity in higher visual cortex (Figures 5 and 6)"],conclusions:["'[...] even if a fine-grained representation is unique in each individual, like a fingerprint, the region containing the representation may be homologous, like a finger\u2014serving the same function in both species' (p. 1126)","fMRI and single-cell physiology may not be that different after all, at least not at this level of analysis","Exclusion of category-selective regions doesn't really change the representational structure, suggesting that the strong modularity hypothesis is not totally correct","Monkeys are probably a pretty good model system for human object recognition, keep plugging away!"],other:["The fiber-flow diagram in Figure 2 is really cool, I'd be curious to find out how they made it although I'm not entirely convinced it's an intuitive representation of interspecies similarity"]}}},141:function(e){e.exports={metadata:{authors:["Linsker, R"],institutions:["IBM"],keywords:["hierarchical","unsupervised","neural network","Hebbian"],title:"From basic network principles to neural architecture: emergence of spatial-opponent cells",journal:"PNAS",doi:"10.1073/pnas.83.19.7508",url:"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC386748/",date:"1986-09",review_date:"2019-09-18",one_sentence:"In the first of a 3-paper series, Linsker demonstrates how center-surround receptive fields might form in a modular, Hebbian network with random initial activity"},review:{summary:["A three layer neural network, in which random activity patterns are generated in the first layer, gives rise to center-surround receptive fields in the third layer when trained with a  Hebbian learning rule and spatially localized synaptic projections from layer to layer","This architecture will serve as the foundation for the emergence of orientation-selective cells, and eventually columns in later papers in this series (Linsker 1986b,c)","Linsker emphasizes that the results here are not restricted to the visual system, but are easily understood through that lens"],background:["A highly reduced description of the early visual system includes center-surround cells in the retina and LGN, followed by orientation selectivity in primary visual cortex. Most of this structure is apparent at birth, ruling out strict forms of supervised learning (as performed in leading machine-learning frameworks today)","Spontaneous activity is known to be important in the early visual system (mostly in the form of retinal waves in mammals), although this paper takes as a starting point a theoretical system that has spatially random activity patterns"],approach:["Cells are laid out in a series of two-dimensional sheets; in this case there are three layers. The first layer is the input layer, which generates random activity patterns at some resolution (imagine a grid of some density, with each spot in the grid either being on or off at random)","Connect units in layer i to units in layer i + 1 such that corresponding points in each 2D sheet are more densely innervated. In practice this means that the connection strengths are initialized according to a Gaussian centered at the same location in the next sheet",'Use a Hebbian update rule to learn layer 2 given a series of "presentations" in layer 1, then after those weights have stabilized, learn layer 3 from the activity of layer 2 (which in turn is a processed form of the input form layer 1)',"Evaluate the mature weights of the system and compare to known biology"],results:["The weights in this system are unstable until they reach some boundary condition, e.g., +/- 1. That is, for a given postsynaptic cell, all (or all except one) synaptic weights onto that cell will be at extreme values.","There is a range of learning hyperparameters for which layer 2 develops all-excitatory, all-inhibitory, or mixed-sign responses. The author analyzes the all-excitatory case. For this case, layer 2 exhibits spatially local correlations.","Layer 3 emerges as a sheet of cells with center-surround tuning, as found in the retina and LGN of primates. Sometimes orientation-selective cells here, but the conditions under which that occurs are not described in detail."],conclusions:["A multilayer, unsupervised feedforward neural network with a Hebb-type learning rule yields spatial-opponent (center surround) cells in the final layer. A similar result could likely be obtained with other learning rules, although that isn't explored in this paper.","Linsker makes a point to say that this is not meant to be a model of the retina or its development, but is instead a proof of principle that a simple architecture with a simple learning rule and reasonable assumptions can give rise to a fairly complicated motif of the functional architecture of the visual system. I personally find that approach very inspiring, and believe there's a lot of work in this vein yet to be done!"],other:["This paper uses a TON of variable names and symbols. At some point it'd be great to see a simulation or even just a table of parameter names and their meaning..."]}}},142:function(e){e.exports={metadata:{title:"Anatomy and Physiology of a Color System in the Primate Visual Cortex",authors:["Livingstone, MS","Hubel, DH"],institutions:["Harvard Medical School"],date:"1984-01",journal:"J. Neurosci",doi:"10.1523/JNEUROSCI.04-01-00309.1984",url:"http://www.jneurosci.org/content/4/1/309.long",keywords:["color","CO","V1","V2","macaque","blob"],review_date:"2019-03-21",one_sentence:"A seminal paper in the field of color vision, this work establishes the functional properties of cytochrome oxidase blobs in V1 and their relationship to stripes in V2"},review:{summary:["A blob thus seems to be made up of a core of unoriented cells surrounded by a shell of poorly oriented cells (p. 315).","Blobs are color-selective","Blobs project to stripes in V2"],background:["V1 has modular organization defined by functional characteristics such as retinotopic position, ocular dominance, and orientation. There are also blobs that can be identified with cytochrome oxidase (CO) staining. The blobs are found along the tangential pattern of ocular dominance columns in animals that have ocular dominance columns.","Blobs probably consume more energy than non-blobs, as indicated by 1) their labeling with CO, a mitochondrial enzyme, and 2) their uptake of 2-deoxyglucose. That means that there's either different neuronal machinery in blobs, are that they're less selective to simuli and thus fire more often in general","Unknown at the time how these neurons may differ functionally from non-blobs neighbors and if they have different pattersn of projection to downstream areas"],approach:["Present stimuli to anesthetized macaques and squirrel monkeys while making single-unit recordings. Electrode penetrations were about 4-5mm each, with responses recorded every 50 microns on the track","After stimulus presentation and recording, perfuse the animal and stain for CO"],results:["Upon approaching a blob in a penetration, orientation selectivity fell off dramatically and background activity increased. See track reconstructions in Figure 2. Lack of orientation found in 63/72 recorded cells in macaque and 8/8 in squirrel monkey","In macaque, CO blobs are more monocular than surrounding cells. Not so in squirrel monkey, where there is no registration of CO blobs with ocularity maps","Blobs interrupt orientaiton selectivity, but do not perturb it: the smooth varying pattern is usually preserved on the other side of the blob (Figure 5)","Of unoriented cells, the vast majority were double-opponent (center-surround with flipped color preference in center and surround), followed by broad-band (generally center-surround without strong color tuning) -- see Table 1 on p. 322","Staining of V1/V2 demonstrate that blobs project to thin stripes in V2, and interblobs project to interstripe regions in V2"],conclusions:["CO blobs in primary visual cortex are functionally and physiologically distinct from inter-blob regions, but do not disrupt existing patterns (e.g., of orientation tuning) and are in register with other maps (e.g., of ocular dominance)","CO blobs are color selective and project to stripes in area V2"],other:["Interesting self-reference: The historically minded reader may have wondered how so prominent a group of cells could have been missed by so prominent a pair of investigators (Hubel and Wiesel, 1968,1974a).' They conclude that these unoriented cells were either considered damaged, had no clear spatial pattern, were in layer 4C, or that the impression that they weren't there was 'ill-begotten'"]}}},143:function(e){e.exports={metadata:{title:"Orthogonal micro-organization of orientation and spatial frequency in primate primary visual cortex",authors:["Nauhaus, I","Nielsen, KJ","Disney, AA","Callaway, EM"],institutions:["Salk Institute"],date:"2012-12",journal:"Nature Neuroscience",doi:"10.1038/nn.3255",url:"https://www.nature.com/articles/nn.3255",keywords:["orientation","spatial frequency","calcium","macaque","V1"],review_date:"2019-03-14",one_sentence:"Ca2+ imaging in macaque V1 reveals ordered tuning to orientation and spatial frequency, as well as an orthogonal relationship between the two maps"},review:{summary:["Orientation and spatial frequency maps are consistent across cortical depth in macaque V1","At both microscopic (200x200 microns) and mesoscopic (800x800 microns) scales, orientation and spatial frequency maps are clustered and run orthogonal to each other","Orientation and spatial frequency maps are surprisingly smooth at the scale of a few tens of microns, suggesting that smoothness in intrinsic imaging maps is not solely an artifact of blur"],background:["Maps of spatial frequency tuning have been in rare in macaque and rarely give complete picture due to methodological limitations (resolution and/or sparsity of sampling)","While interactions between orientation and spatial frequency maps have been reported in cats and ferrets, direct evidence in macaques has been elusive"],approach:["2P Ca2+ imaging of layer 2/3 macaque V1 neurons at two spatial scales and two depths","Compute tuning curves for each pixel in the large scale map and each neuron in the fine-scale map","Compare tuning for orientations and spatial frequency"],results:["Both orientation and spatial frequency maps are clustered (tuning curves more correlated at closer distances), although spatial frequency maps have more scatter (Figure 4)","Large scale (smooth) maps for orientation and spatial frequency intersect each other at 90-degree angles (Figure 5)","Maps are periodic (2D autocorrelations begin to rise again after about 500 microns (Figure 5)","At fine scale, direction of maximum tuning curve change is orthogonal for orientation and spatial frequency maps (Figure 6)","For a pair of cells, the correlation between tuning curves for orientations is anti-correlated with correlation between tuning curves for spatial frequencies at larger distances"],conclusions:["Maps for both parameters are relatively continuous and have lawful relationships to one another","This is the first study to examine both maps with sufficiently high spatial resolution and density of sampling","Fine-scale maps reveal very smoooth progression of peak tuning curve values"],other:["Would be interesting to see color tuning in these imaging regions!"]}}},144:function(e){e.exports={metadata:{title:"Highly ordered arrangement of single neurons in orientation pinwheels",authors:["Ohki, K","Chung, S","Kara, P","H\xfcbener, M","Bonhoeffer, T","Reid, RC"],institutions:["Harvard Medical School","Max Planck Institute of Neurobiology","Korea Institute of Science and Technology","Medical University of South Carolina"],date:"2006-08",journal:"Nature",doi:"10.1038/nature05019",url:"http://ctx.kist.re.kr/Teams/ctx/PDFs/Ohki%20et%20al%20Nature%202006.pdf",keywords:["cat","V1","calcium imaging","pinwheel","orientation"],review_date:"2019-08-16",one_sentence:"Ohki et al. demonstrate that at a single-neuron level, pinwheel structure is held up by sharply tuned neurons that obey the radial structure of the pinwheel"},review:{summary:["Single neurons in orientation pinwheels are tightly arranged, suggesting that the pinwheel center goes down to the single neuron level instead of being an artifact of intrinsic imaging methods","Neurons in pinwheel centers have lower activity levels and are more broadly tuned than neurons peripheral to the pinwheel center"],background:["Prior study of orientation tuning maps have either lacked coverage (electrode/tetrode recording) or spatial specificity (electrophysiology, intrinsic optical imaging), leaving open the question of how pinwheel centers were organized at the single neuron level","Intrinsic imaging has suggested that pinwheel centers are more broadly tuned than iso-orientation domains (see, e.g., Bartfeld and Grinvald '92). Note: this is also observed in subsequent calcium imaging studies, e.g., Nauhaus et al., 2008"],approach:["Label neurons near pinwheel centers in V1 of 28-35 day old cats with a calcium indicator (Oregon Green BAPTA-1)","Perform 2-photon calcium imaging at 9 different cortical depths while presenting bars at orientations separated by 45 degrees to the cat"],results:["Neurons near pinwheel centers are organized with a very low degree of 'scatter', indicating that the pinwheel structure is valid down to individual cell bodies (Figure 1, 3B)","Neuron tuning is columnar, with highly similar orientation preference across depths (Figure 1)","At the pinwheel center, spatially adjacent neurons can have very different tuning (Figure 2)","Neurons in the pinwheel center had lower responses and broader tuning in every experiment performed (Figure 3). The difference in tuning width between pinwheel center and periphery increased even more when bar orientations were separated by smaller steps (22.5 degrees), but they were still fairly sharply tuned (or at least, more than people might have expected?)"],conclusions:["Neurons in pinwheel centers are sharply tuned and don't deviate from the expected radial structure of the maps by much. This suggests a precise developmental mechanism at play, given that these patterns are fairly well-established early in life","One important mechanism to figure out is how dendritic arbors might be A) arranged to sample input differently, or B) might receive inputs of different strengths that drive this sharp tuning"],other:["This paper stands out to me as an example where biology isn't messy... The precision of these maps suggests that pinwheel centers might be important either for performing some task (see my review of Koch et al., 2016), or for reducing some metabolic cost."]}}},145:function(e){e.exports={metadata:{title:"Emergence of simple-cell receptive field properties by learning a sparse code for natural images",authors:["Olshausen, BA","Field, DJ"],institutions:["Cornell"],date:"1996-06",journal:"Nature",doi:"10.1038/381607a0",url:"https://www.nature.com/articles/381607a0",keywords:["V1","Gabor","sparse coding","natural image"],review_date:"2019-04-25",one_sentence:"Olshausen and Field provide a normative argument for V1 receptive fields: they are the optimal basis for sparse reconstruction of patches of natural images"},review:{summary:["Minimizing a combination of image reconstruction loss and a sparsity constraint via gradient descent leads to a Gabor-like filterbank","This coding scheme gives more biological results than PCA, due to the non-Gaussian distribution of image pixel values"],background:["V1 neurons have receptive fields that are tuned to spatial frequency, orientation, and stimulus position","Nearly all natural images exhibit higher-order correlations, making methods relying on pairwise linear correlation, e.g., PCA, unhelpful in recovering the image structure"],approach:["Learn a 192-filter bank of 16x16 black and white filters, trained on small portions of natural images of the American northwest","Define the loss function as a sum of pixel-wise reconstruction loss and a penalty on large filter coefficients","Whiten images, then perform conjugate gradient method (more efficient than steepest descent) for roughly 400,000 image presentations to minimize the desired loss function"],results:["PCA fails to recover biological filters. Instead, it finds unbiological checkerboard-like patterns (Figure 1)","The resulting filters are an overcomplete set of Gabor-like filters selective for different orientations, spatial frequencies, and positions (Figure 4)","The result depends on the use of natural images, as learning sparse decompositions of other types of stimuli is ineffective (Figure 3)"],conclusions:["V1 receptive fields are the way that they are because it's a good way to encode the visual scene under sparsity constraints!"],other:["This is one of my favorite results in visual neuroscience. In a way, these results have been reproduced in the deep-learning era, as networks trained with regularization and a limited number of channels in early convolutional layer learn that Gabor-like filters are a good basis set for representing natural images. Whether or not the basis is as good as the one in this paper is a difficult question to answer, but may be a relevant one in years to come."]}}},146:function(e){e.exports={metadata:{title:"Factors Determining Where Category-Selective Areas Emerge in Visual Cortex",authors:["Op de Beeck, HP","Pillet, I","Ritchie JB"],institutions:["KU Leuven"],date:"2019-07",journal:"TICS",doi:"10.1016/j.tics.2019.06.006",url:"https://www.sciencedirect.com/science/article/abs/pii/S1364661319301585",keywords:["human","VTC","fMRI","category","topography","development"],review_date:"2019-08-04",one_sentence:"Op de Beeck et al. argue that the functional organization of human VTC can be understood by considering three factors: which visual features a region represents, how the visual features area processed, and the cognitive/semantic domain associated with the category"},review:{summary:["The functional architecture of higher visual cortex can be understood by considering the features represented, how those features factor into a hierarchical computational framework, and the cognitive domain they are associated with","Visual features and categories cannot be meaningfully divorced from each other; instead, we should consider pre-existing preference for visual features predictors of where regions for categories consisting of those features will emerge"],background:["Category-selective regions have long been studied in human VTC, but theories predicted their specific spatial organization with respect to anatomy and to each other have been lacking","Whether these regions are strictly experience-dependent, activity-dependent, or innate/genetically-programmed has been a matter of debate"],approach:["This is a review paper which primarily draws on studies using fMRI in humans"],results:["VTC category preference is a lot stickier than people like to make it seem. Regions often have graded selectivity and cluster according to superordinate dimensions such as object size and animacy. 'Metaphorically speaking, a category-selective area is not a lone functional peak in an otherwise flat functional landscape, like a neural Kilimanjaro. Rather, it is one of many hilltops in a Yellowstone-like complex, of which the origin can only be understood by investigating the larger-scale environment'","Categories and category-typical features are tightly correlated, e.g., faces have a typical shape and are viewed in the same position of the visual field. Nevertheless, category-selective areas have been shown to respond to a given category even when controlling for such factors (Box 1)","Within the occiptotemporal cortex (OTC), regions with similar preference connect with each other. Connections reaching outside OTC, however, diverge in their targets. For example, word-form areas preferentially connect to left hemisphere language regions","Congenitally blind individuals provide a unique perspective on the developmental emergence of category-selective regions. The fact that the same general topography emerges in these individuals suggests that visual experience itself is not required for the formation of category-selective regions (but see Arcaro et al., 2017)","The eccentricity bias theory (that VTC layout is determined by typical viewing eccentricity; see Gomez et al., 2019's Pokemon study) has been popular in the field, but studies that support it have insufficiently separated typical viewing eccentricity from the other factors listed here"],conclusions:["While the three factors described (features, computation, and domain/connectivity) are clearly important, none of them is sufficient in isolation to explain the functional organization observed"],other:["Reviewing this paper was inspired by last week's review of Arcaro et al., 2019","A huge missing piece in my reading of this is: 'what about anatomy?! Beyond connectivity, that is.'. We know that different regions of higher visual cortex have microanatomical differences, and presumably these dictate the layout at a much more specific level than what is offered here.","While I agree with virtually everything in this very nice review, I find it a bit unsatisfying that these observations don't bring us closer to answering a questions like 'why is there a face-selective region on the lateral fusiform, and not somewhere else? What determines its size?"]}}},147:function(e){e.exports={metadata:{title:"Face Space Representations in Deep Convolutional Neural Networks",authors:["Alice J. O\u2019Toole","Carlos D. Castillo","Connor J. Parde","Matthew Q. Hill","Rama Chellappa"],institutions:[" University of Texas at Dallas"," University of Maryland College Park"],date:"2018-08",journal:"Trends in Cognitive Sciences",doi:"10.1016/j.tics.2018.06.006",url:"https://www.sciencedirect.com/science/article/pii/S1364661318301463",keywords:["face","DCNN","face space"],review_date:"2019-09-03",one_sentence:"O'Toole et al. summarize recent advances in the use of deep convolutional neural networks for face recognition and as models of biological face processing"},review:{summary:["Deep convolutional neural networks (DCNNs) achieve state-of-the-art on  demanding face recognition tasks",'By considering how individual identities are represented in a high-dimensional "face-space", we see that DCNNs accomplish this task via rich yet compact representations, reminiscent of observations about how objects are represented in ImageNet-trained DCNNs'],background:["Early attempts at modeling the representation of faces relied on an image-based space commonly constructed by performing PCA directly on pixel inputs. This technique isn't terrible, but breaks down quickly when there are changes in viewpoint or luminance","Active appearance models (AAMs) emerged as an alternative to image-based models. They were more flexible in that they represented shape and texture information separately, allowing a generative model of faces at any viewpoint or illumination condition. Notably, though, there's no mechanistic understanding of how this division of labor might be accomplished in the brain, although there is evidence that face-selective neurons care about combinations of these dimensions (see Change and Tsao, 2017)","DCNNs have become clear favorites for object recognition tasks, and under some formulations of the task, one can consider face recognition an extension of object recognition task: critically, both require tolerance to identity-preserving transformations but specificity across identities","An important shortcoming of previous face-recognition models is that they don't account for 1) familiarity/\"training\" with certain identities and 2) similarity-based models don't explain changes in illumination and pose neatly, they only describe how similar two faces may be","As recently as 2012-2013, models trained to recognize faces only reached human performance on frontal views of faces but could not generalize to other viewpoints effectively"],approach:["This is a review paper that mostly cites work using deep convolutional neural networks trained on Labeled Faces in the Wild, IJB-A, and Mega-Face",'The networks are often trained with some combination of good-old softmax loss and "triplet loss", which when given three faces (two of which are the same identity) is penalized for failing to separate the two unique identities',"Networks are often, but not always, pre-trained on an object recognition task, then fine tuned on a face training set."],results:["Face representations in later DCNN layers provide readouts of many relevant variables, including identity, face viewpoint, pose, etc! This is consistent with similar observations for non-face objects reported in Hong et al., 2016 (Nature)",'Some faces end up being coded robustly across many viewpoints, whereas others seem "bound" to a specific viewpoint. Whether this is a shortcoming of the training data or an indicator of properties specific to those identities remains to be seen',"The DCNN face space appears to care quite a bit about image quality: images at low resolution or with a lot of occlusion are represented as more similar to each other than images where the face is clear, high-resolution, and unobstructed. Eshed's note: I doubt this is specific to face-trained networks, but is likely a property of any network in which classification becomes hard under occlusion"],conclusions:["The paper ends with a number of really important and interesting questions. In fact, this was probably my favorite part of the paper!","Q1) What should the training set be? Objects + faces? Objects then faces? Faces alone? How do we think face-recognition emerges in humans? As part of a more general system of networks (this is my camp), or as specialized hardware with a specialized training routine?","Q2) If you take a network and train it for faces or for objects, in which layer do you start to see appreciable differences? My guess is that it would be fairly late.","Q3) How can DCNNs be used to explain activity in face-selective brain areas?","Q4) Do DCNNs represent characteristics of faces such as gender and age?","Q5) How does the quality of the data (specifically, as a function of training stage) affect learned representations?","Q6) How much does the architecture of the DCNN matter?","Q7) How does neuroscience need to change to measure axes of the neural space that map onto semantically meaningful concepts?"],other:["I found this review very well-written and thoughtful, and I learned a lot about the state-of-the-art before the deep learning boom!"]}}},148:function(e){e.exports={metadata:{title:"Evolving super stimuli for real neurons using deep generative networks",authors:["Ponce, CR","Xiao, W","Schade, PF","Hartmann, TS","Kreiman, G","Livingstone, MS"],institutions:["Harvard University"],date:"2019-01",journal:"bioRxiv",doi:"10.1101/516484",url:"https://www.biorxiv.org/content/10.1101/516484v1",keywords:["generative","genetic algorithm","IT","macaque","face"],review_date:"2019-03-28",one_sentence:"A closed loop paradigm to identify the best stimulus to drive a neuron is applied to recordings of macaque IT"},review:{summary:["A combination of a deep generative network and a genetic algorithm synthesizes stimuli that drive neurons better than natural images","Natural images that are similar to synthesized images, given the encoding used, also drive higher firing rates than those dissimilar to the synthesized images"],background:["Identifying the optimal stimulus for a neuron is difficult, and is often approximated with hand-crafted stimuli of a certain form. This approach breaks down further in the ventral visual stream","Deep generative networks have recently been used to produce realistic stimuli that drive neural network units optimally"],approach:["Begin with a group of images, convert them to textures via the Portilla and Simoncelli algorithm, convert those to 4096-dim vectors. Evaluate 'fitness' of each code by using a deep generative network to conver the code to an image and measure the firing rate in response to that image. Finally, use a genetic algorithm to modify the code such that it will lead to higher firing rates in the future. Repeat.","Recordings were made from IT in 5 monkeys and V1 in 1 monkey as a control. Mostly chronic array recordings were used, but one monkey had a recording chamber."],results:["The synthesis procedure gradually produces better and better stimuli, and at the end stage of synthesis, responses to these images are higher than to a large collection of natural images (Figures 4 and 5)","The images are (sort of?) realistic, and produce different categories of objects for different neurons, such as faces for face-selective neurons and scene-like images for scene-selective neurons (Figure 6)","Finding natural images that are close to the synthetic image in the encoded space yields higher firing rates than expected by choosing random images, suggesting that the synthetic images lie near a 'manifold' of natural stimuli that drive high firing rates","Firing rates are more invariant to natural image rotation, scaling, and position shifts than to the synthesized images (Figure S3)"],conclusions:["This generative evolutionary approach is successful in identifying stimuli that drive the target neurons","The synthesized images are abstract and hard to describe, maybe an indicator that IT neurons aren't selective to objects per se, but to abstract axes of objects, as posed by Doris Tsao?","The approach is closed-loop and requires a lot of iterative data acquisition. Furthermore, it's not clear to what degree the choice of initial images constrains the space of images that the synthesizer might find","Most importantly, I was left unsure of why such a complicated approach was needed -- what was the justification for the genetic algorithm? Would gradient ascent not work for these neurons for some reason?"],other:["Related work: Neural Population Control via Deep Image Synthesis by Bashivan, Kar, and DiCarlo. The advantage of this alternative approach is that it is more generally flexible (does not impose priors on what sorts of image can be generated) and only requires data acquisition at two time points (once to establish a linear mapping from a model, e.g., a convolutional neural network, to the neuron of interest, and again to validate the method with the stimuli"]}}},149:function(e){e.exports={metadata:{title:"Reversible inactivation of different millimeter-scale regions of primate IT results in different patterns of core object recognition deficits",authors:["Rajalingham, R","DiCarlo, JJ"],institutions:["MIT"],date:"2019-04",journal:"Neuron",doi:"10.1016/j.neuron.2019.02.001",url:"",keywords:["inactivation","macaque","IT","object recognition"],review_date:"2019-06-06",one_sentence:"Muscimol injections in arbitrarily-selected millimeter scale regions of IT suggests a heterogeneous yet spatially continuous layout of IT with respect to core object recognition"},review:{summary:["Inactivation of small regions of IT negatively affects some object discrimination subtasks, but not others","Nearby injections yield similar patterns of behavioral deficits","Decoding models suggest that local discriminability, not local selectivity, are best predictive of the behavioral effects of inactivation"],background:["A large body of work supports the idea that activity in IT is correlated with behavior. However, in the visual motion literature, recent evidence has suggested that regions with behavior-correlated activity are not necessary to support that behavior, i.e., it is epiphenomenal","Previous attempts at causal manipulations have overwhelming been targeted to face patches and to face-specific tasks"],approach:["Inactivate many regions along the ventral surface of IT with muscimol injections (about 2.5mm spread), evaluate effects on object recognition performance on a set of 6 (or 10, in one experiment) subtasks, where each subtask is a pairwise discrimination such as dog vs. elephant","Collect neuron responses in the same regions that are inactivated during a passive fixation task, then compare decoders that predict how neuronal activity translates to behavioral deficits during inactivation"],results:["Inactivation results are hetereogeneous: inactivation of any given site only affects some subtasks (Figures 2, 4), but no task is affected more than others when averaging across all subtasks (Figure 5)","The strongest effect of inactivation is seen when the object of interest is in the visual field contralateral to the injection site and peaks around a d' difference of 0.6 (Figure 3)","Inactivation of nearby sites yields similar behavioral deficits (Figures 6, 7), with correlations that drop to 0 after about 10mm","Models that predict inactivation from response selectivity perform worse than models that predict inactivation from the accuracy of an SVM trained on responses of those neurons (Figure 8)"],conclusions:["IT is necessary for object discrimination behavior, and different regions support different subtasks. It is not the case that all non-face regions are necessary for all non-face tasks","Local discriminability is a strong predictor of the effects of inactivation, suggesting one possible readout mechanism from IT to downstream regions"],other:["See last week's paper-a-week (Afraz et al., 2015) for more inactivation work from the DiCarlo lab, but targeted specifically to face patches","These effects are small in terms of balanced accuracy, and the monkeys are generally at ceiling on these tasks to begin with. There's a very fair and careful justification of using d' instead of accuracy/choice bias/other, and in general the work is incredible carefully done and impressive"]}}},150:function(e){e.exports={metadata:{title:"Orientation Selectivity in Macaque V1: Diversity and Laminar Dependence",authors:["Ringach, DL","Shapley, RM","Hawken, MJ"],institutions:["UCLA","NYU"],date:"2002-07",journal:"J. Neurosci",doi:"10.1523/JNEUROSCI.22-13-05639.2002",url:"http://www.jneurosci.org/content/22/13/5639.long",keywords:["macaque","V1","orientation","circular variance","bandwidth","laminar"],review_date:"2019-04-04",one_sentence:"Ringach and colleagues establish a variety of orientation selectivity metrics throughout different cortical layers of macaque V1"},review:{summary:["Many V1 cells are poorly tuned for orientation with a median circular variance of 0.6 (0 is perfectly orientation-selective, 1 is perfectly non-selective)","Orientation selectivity correlates weakly with spontaneous activity and more strongly with modulation ratio (a measure of how simple or complex a cell is)","Despite both measuring orientation tuning selectivity, circular variance and bandwidth can give different views on a distribution of cells"],background:["V1 neurons are known to be selective for particular orientations, but the exact distribution of selectivity in different cortical layers in macaque is unknown","Bandwidth of orientation tuning is an often used but potentially inaccurate method, as it does not consider responses distant from the preferred orientation of a cell","Unoriented cells were primarily thought to reside in layers 2/3, where CO blobs are often found (see Livingstone and Hubel, 1984 for more), and to some degree in layer 4C"],approach:["Electrode penetrations from 26 anesthetized macaques","Monocular stimulation with drifting sinusoidal grating in steps of 10, 15, or 20 degrees (depending on experimenter's interpretation of how sharply tuned a cell was)","Compute modulation ratio, circular variance, bandwidth, and spontaneous activity level for all isolated single units. See paper for definitions of these quantities"],results:["V1 cells span the range of circular variances, with a median circular variance of 0.61, and skew to lower bandwidths (median == 23.5 deg) (Figures 1, 2)","Circular variance and bandwidth both vary substantially at all cortical depths (Figures 4, 5)","Bandwidth and circular variance are correlated, but there are many low-bandwidth neurons with high circular variance (Figure 3).","Both metrics are correlated with a third metric: the ratio of response to the anti-preferred (orthogonal) orientation and the preferred orientation. This relationship is tighter for circular variance (Figure 6)","Low spontaneous activity appears to be associated with higher orientation selectivity (Figure 8)","Modulation ratio (low == more like a complex cell, high == more like a simple cell) varies by cortical depth, with highest values in layer 4C and lowest values in layer 3B and 5. Cells with high circular variance typically have low modulation ratios, suggesting that complex cells have lower selectivity than simple cells (Figures 11, 12)","Complex cells have higher spontaneous activity than simple cells (Figure 14)"],conclusions:["V1 has a diversity of functional properties across cortical depth, challenging the idea that there are sharp differences in tuning at layer boundaries. Importantly, cells poorly tuned for orientation are found throughout the cortical depth","Metrics matter! Circular variance appears reliable and is a measure of global tuning curve structure, but in other cases one may genuinely care about other metrics. Whenever possible, comparison to previously reported metrics helps bring many results into alignment, and is a really useful approach"],other:[]}}},151:function(e){e.exports={metadata:{title:"Object Representation in Inferior Temporal Cortex Is Organized Hierarchically in a Mosaic-Like Structure",authors:["Sato, T","Uchida, G","Lescroart, MD","Kitazono, J","Okada, M","Tanifuji, M"],institutions:["RIKEN","UC Berkeley","University of Tokyo","Waseda University","National Yang-Ming University"],date:"2013-10",journal:"J. Neurosci",doi:"10.1523/JNEUROSCI.5557-12.2013",url:"http://www.jneurosci.org/content/jneuro/33/42/16642.full.pdf",keywords:["macaque","IT","category","topography"],review_date:"2019-07-25",one_sentence:"Sato and colleagues demonstrate that macaque IT is organized hierarchically in its responses: columns form 'domains', and similar domains cluster on the cortical surface"},review:{summary:["Electrophysiological assessment of IT reveals domains on the scale of multiple mm that are tuned to particular categories of objects, e.g., faces and bodies. This supports and explains existing literature using fMRI and intrinsic signal imaging."],background:["Two competing models for IT organization are the modular theory, which posits that computation proceeds from spatially-distinct module to module (e.g., the face patch network), and the distributed theory, which posits that computation proceeds from the assembly of general visual features that represent objects","Much of the support for the two theories comes from fMRI and optical intrinsic signal imaging (OISI), which are indirectly coupled to neuron activity and have poor spatial resolution. So it remains unclear what the fine-scale spatial organization of IT is with respect to rich visual features."],approach:["Extracellular recordings in the right hemisphere of three monkeys (total of 99 sites across monkeys). At each site, record from three electrodes in a triangle pattern and at five cortical depths. These 15 sites (3 electrodes x 5 depths) are averaged to produce the 'local activity' for that site","Present images of 7 categories: faces, scrambled faces, monkey hands, monkey bodies, nonprimate bodies, foods/veggies, and man-made objects","Determine correlation of responses to each image across sites, then perform hierarchical clustering to try and identify structure in the correlations"],results:["Hierarchical clustering suggests that there are between 7-8 distinct response groups, and these groups also tend to be spatially co-localized (Figure 2). The extent of these 'domains' were multiple mm in diameter.","Hierarchical clustering suggests that there are between 7-8 distinct response groups, and these groups also tend to be spatially co-localized (Figure 2). The extent of these 'domains' were multiple mm in diameter. Note: this implies functional domains are larger than single columns!","The identified clusters are robust to variations in stimuli like color or silhouette/full, and are further robust to the choice of clustering algorithm (Figures 3, 4)","Selectivity analysis of the domains indicates that some are face selective, some are body selective, some are anti face-selective, and some appear nonselective. (Eshed's note: this is cool, I've always wondered if there are spatially-contiguous regions that respond especially poorly to faces!) (Figure 5)","Optical intrinsic imaging suggests that the domains are composed of columns (sites on the order of 0.5mm in diameter) (Figure 10)"],conclusions:["IT is organized hierarchically with respect to topography; domains are composed of multiple feature columns, and regions are likely to be contiguous domains","Results indicating that modality of measurement doesn't matter (e.g., Kriegeskorte et al., 2008) may arise from the fact that randomly sampling within a domain is the same as recording aggregate domain activity"],other:["The outstanding question in my mind is why we observe this hierarchial structure. That is both 1) why is it hierarchical, and 2) why this particular structure with respect to faces, bodies, etc","See Rajalingham et al., 2018 for a causal manipulation of arbitrarily selected IT sites. Their results are generally aligned with these; that IT is a heterogeneous landscape with some local spatial dependencies in the response profiles"]}}},152:function(e){e.exports={metadata:{title:"Disruption of orientation tuning in visual cortex by artificially correlated neuronal activity",authors:["Weliky, M","Katz, LC"],institutions:["HHMI","Duke University"],date:"1997-04",journal:"Nature",doi:"10.1038/386680a0",url:"https://www.ncbi.nlm.nih.gov/pubmed/9109486",keywords:["ferret","V1","orientation","stimulation"],review_date:"2019-06-28",one_sentence:"Weliky and Katz demonstrate that the introduction of artifically correlated activity in the ferret optic nerve weakens orientation selectivity in cortex while leaving orientation maps intact"},review:{summary:["Stimulation of the optic nerve during development reduces orientation tuning and direction selectivity for single cells, but leaves the spatial pattern of orientation tuning across the cortical surface relatively intact"],background:["Orientation selectivity is apparent at birth, but that selectivity is weak relative to adults","Visual deprivation experiments suggest that visual experience is necessary for orientation map development, but it's unclear whether that's because visual experience provides general input (i.e., preserves # of action potentials) or because the input is structured and thus, instructive"],approach:["'Play in' competing patterns of activity to the visual system via artificial stimulation of the optic nerve in ferrets. Current delivered to a cuff was used to drive optic nerve activity","Stimulate between P25 and P35, then allow 7-10 days of natural visual experience (same as for control ferrets)","Extracellular, spike-sorted recordings in visual cortex in response to drifting bars at 18 orientations","Compute the orientation selectivity index (OSI), where higher values indicate more selectivity","Intrinsic optical imaging (ISI) used to evaluate the spatial pattern of orientation tuning"],results:["Compared to control ferrets (60% of cells significantly orientation selective), ferrets receiving cuff stimulation had significantly fewer orientation selective cells (17%). The decreases in orientation selectivity was accompanied by a decrease in direction selective (Figure 2)","The decrease in orientation selectivity was not due simply to decreased firing rate: in both control and stimulated animals, there was no correlation between orientation selectivity and peak firing rate (but could there be a difference in mean firing rate?)","In stimulated animals, no observed effects of cortical depth on orientation selectivity, although along an electrode penetration, OSI varied smoothly. In control animals however, OSI were significantly higher in superficial and deep cortical depths (Figure 3)","ON/OFF LGN responses appeared similar in control and stimulated animals, suggesting a cortical origin in the orientation selectivity deficits (Figure 4)","Despite changes in OSI, the pattern of orientation selectivity across cortex, including size, spacing, and pattern of iso-orientation domains, appears unaffected. The authors note that the signal magnitude in these imaging experiments is markedly reduced in stimulated animals (Figure 5)"],conclusions:["Replacing patterned spontaneous activity from retinal waves with artificially correlated activity disrupts the development of sharp orientation tuning","The persistence of the orientation maps might be due to the earlier origins of those maps, where thalamic projections begin to colonize layer 4 around P10.","One shortcoming of this technique is that it doesn't entirely replace ongoing endogenous activity, it simply adds to it in an artifical way. Still, this proves an effective technique for influencing the developing circuit"],other:["This work finds significantly higher OSI in superficial and deep layers than in middle layers, but Ringach et al., 2002 find uniformity in orientation selectivity across cortical depths. Is this due to species differences, metric differences (OSI vs. CV), stimulus differences, or recording differences?"]}}},214:function(e){e.exports={projects:[{name:"A V1-like Model for Object Discrimination",url:"http://geon.usc.edu/GJW/",img_src:"gabor_grab.PNG",alt:"Screengrab from application designed to explore gabor-jet model of image processing."},{name:"Modeling the Functional Architecture of the Visual System",url:"img/Margalit_et_al_VSS_2018.png",img_src:"spacenet.png",alt:"Orientation tuning map from a computational model of primary visual cortex"},{name:"Interactive Signal Detection Theory Tutorial",url:"#/DPCalc",img_src:"dp_gui_screenshot.png",alt:"Screenshot of interactive webpage describing signal detection theory"},{name:"How are object parts represented in the human brain?",url:"http://www.mitpressjournals.org/doi/abs/10.1162/jocn_a_01144",img_src:"Margalit_et_al_2017_Fig3.png",alt:"Outlines of brain areas shown on inflated cortical reconstruction"}]}},217:function(e){e.exports={CV:{last_updated:"July 2019",pdf:{path:"2019.July.CV.pdf"},awards:[{name:"NSF Graduate Research Fellowship Program (GRFP) Fellow",dates:"2016 - 2021",description:"NSF fellowship recognizes and supports outstanding graduate students in NSF-supported science, technology, engineering, and mathematics disciplines",url:"https://www.nsfgrfp.org/"},{name:"USC Trustee's Award",dates:"2016",description:"Awarded to the undergraduate male with the highest GPA at the University",url:""},{name:"Graduate Trainee, Stanford Mind, Brain, Computation, and Technology",dates:"2018",description:"Traineeship awarded for the pursuit of multi-disciplinary research in the area of computational neuroscience",url:""},{name:"USC Neuroscience Outstanding Student of the Year",dates:"2016",description:"Awarded to USC's best neuroscience student with senior standing",url:"http://dornsife.usc.edu/usc-neuroscience/"},{name:"USC Brian Philip Rakusin Neuroscience Scholarship Award",dates:"2015",description:"Awarded to USC's best neuroscience student with junior standing",url:"http://dornsife.usc.edu/usc-neuroscience/"},{name:"Phi Beta Kappa Honor Society",dates:"2015",description:"Academic honor's society",url:"https://www.pbk.org/"},{name:"USC Discovery Scholar",dates:"2016",description:"Awarded to students who excel in the classroom while demonstrating the ability to create exceptional new scholarship.",url:"http://ahf.usc.edu/scholars/discovery/"},{name:"USC Provost's Undergraduate Research Fellowship",dates:"2013 - 2016",description:"6-time recipient of award supporting exceptional undergraduate research",url:"https://undergrad.usc.edu/experience/research/undergrad_research/"},{name:"George H. Mayr Scholarship",dates:"2015",description:"Awarded to outstanding students from California in the college of letters, arts, and sciences",url:""},{name:"USC S.O.A.R. Grant",dates:"2015",description:"Provides funding to Dornsife undergraduates for participation as a research assistant in a faculty member\u2019s project",url:"https://dornsife.usc.edu/soar"},{name:"USC Dean's Scholar",dates:"2012 - 2016",description:"Quarter tuition scholarship",url:"http://dornsife.usc.edu/trustee-presidential-deans-scholarships"}],publications:{published:[{title:"Neuromelanin marks the spot: Identifying a locus coeruleus biomarker of cognitive reserve in healthy aging",authors:["Clewett, D.","Lee, T.H.","Greening, S.G.","Ponzio, A.","Margalit, E.","Mather, M."],journal:"Neurobiology of Aging",year:"2016",volume:", 37",pages:", 117-126",url:"http://www.neurobiologyofaging.org/article/S0197-4580(15)00476-5/abstract",tag:"noa-2016-neuromelanin"},{title:"The lateral occipital complex shows no net response to object familiarity",authors:["Margalit, E.","Shah, M.P.","Tjan, B.S.","Biederman, I.","Keller, B.","Brenner, R."],journal:"Journal of Vision",year:"2016",volume:", 16(3)",url:"https://jov.arvojournals.org/article.aspx?articleid=2551659",tag:"jov-2016-thelateral"},{title:"An applet for the Gabor scaling of the differences between complex stimuli",authors:["Margalit, E.","Herald, S.B.","Yue, X.","von der Malsburg, C.","Biederman, I."],journal:"Attention, Perception, and Psychophysics",year:"2016",volume:", 78(8)",pages:", 2298-2306",url:"https://link.springer.com/article/10.3758/s13414-016-1191-7",tag:"app-2016-anapplet"},{title:"What is actually affected by the scrambling of objects when localizing the lateral occipital complex?",authors:["Margalit, E.","Biederman, I.","Tjan, B.S.","Shah, M.P."],journal:"Journal of Cognitive Neuroscience",year:"2017",volume:", 20(9)",pages:", 1595-1604",url:"https://www.mitpressjournals.org/doi/abs/10.1162/jocn_a_01144",tag:"jocn-2017-whatis"},{title:"The cognitive neuroscience of person identification",authors:["Biederman, I.","Shilowich, B.E.","Herald, S.B.","Margalit, E.","Maarek, R.","Meschke, E.X.","Hacker, C.M."],journal:"Neuropsychologia",year:"2018",volume:", 116B",pages:", 205-214",url:"https://www.sciencedirect.com/science/article/pii/S0028393218300423",tag:"neuropsychologia-2018-thecognitive"},{title:"A critical assessment of data quality and venous effects in sub-millimeter fMRI",authors:["Kay, K.N.","Jamison, K.","Vizioli, L.","Zhang, R.","Margalit, E.","Ugurbil, K."],journal:"NeuroImage",year:"2019",volume:", in press",url:"https://www.sciencedirect.com/science/article/pii/S1053811919300928",tag:"neuroimage-2019-acritical"}],preprints:[{title:"Visual Noise Consisting of X-Junctions Has Only a Minimal Effect on Object Recognition",authors:["Margalit, E.","Herald, S.B.","Meschke, E.X.","Irawan, I.","Maarek, R.","Biederman, I."],journal:"PsyArxiv",year:"2019",volume:"",url:"https://psyarxiv.com/cje3y/",tag:"psyarxiv-2019-xvert"}]},presentations:{posters:[{title:"Pinwheel-like Iso-Orientation Domains in a Convolutional Neural Network Model",authors:["Margalit, E.","Lee, H.","DiCarlo, J.J.","Yamins, D.L.K."],journal:"Annual Meeting of the Vision Sciences Society (VSS)",year:"2018",url:"img/Margalit_et_al_VSS_2018.png",tag:"vss-2018-pinwheel"},{title:"What is the nature of the perceptual deficit in congenital prosopagnosia?",authors:["Biederman, I.","Margalit, E.","Maarek, R.","Meschke, E.X.","Shilowich, B.E.","Hacker, C.M.","Juarez, J.J.","Seamans, T.J.","Herald, S.B."],journal:"Annual Meeting of the Vision Sciences Society",year:"2017",url:"http://geon.usc.edu/~biederman/presentations/PerceptualDeficit_VSS_17.pdf",tag:"vss-2017-whatis"},{title:"What is actually affected by the scrambling of objects when localizing the lateral occipital complex?",authors:["Biederman, I.","Margalit, E.","Tjan, B.S.","Shah, M.P."],journal:"Annual Meeting of the Society of Experimental Psychologists",year:"2016",url:"",tag:"sep-2016-whatis"},{title:"Vertices are effective in perceptual grouping (and ungrouping)",authors:["Irawan, I.","Margalit, E.","Herald, S.B.","Biederman, I."],journal:"Annual Meeting of the Vision Sciences Society",year:"2016",url:"http://geon.usc.edu/~biederman/presentations/Irawan_et_al_VSS_16.pdf",tag:"vss-2016-vertices"},{title:"Impaired face and non-face discrimination in developmental prosopagnosics (DPs)",authors:["Margalit, E.","Yue, X.","Biederman, I."],journal:"Annual Meeting of the Vision Sciences Society",year:"2016",url:"",tag:"vss-2016-impaired"},{title:"What is actually affected by the scrambling of objects when localizing LOC? (Talk)",authors:["Biederman, I.","Margalit, E.","Tjan, B.S.","Shah, M.P."],journal:"Annual Meeting of the Vision Sciences Society",year:"2016",url:"",tag:"vss-2016-whatis"},{title:"Neuromelanin marks the spot: a locus coeruleus substrate of cognitive reserve in healthy aging",authors:["Clewett, D.","Lee, T.H.","Greening, S.G.","Ponzio, A.","Margalit, E.","Mather, M."],journal:"USC Neuroscience Graduate Student Symposium",year:"2015",url:"",tag:"usc-2015-neuromelanin"},{title:"Phonagnosia, a voice homologue to prosopagnosia",authors:["Biederman, I.","Herald, S.B.","Xu, X.","Amir, O.","Shilowich, B.E.","Margalit, E."],journal:"Annual Meeting of the Vision Sciences Society",year:"2015",url:"http://geon.usc.edu/~biederman/presentations/BiedermanEtAl_VSS2015.pdf",tag:"vss-2015-phonagnosia"}],talks:[{title:"Differential representation of object category information across lateral and medial ventral temporal cortex revealed with ultra-high-resolution fMRI",authors:["Margalit, E.","Jamison, K.","Weiner, K.S.","Vizioli, L.","Zhang, R.","Kay, K.N.","Grill-Spector, K."],journal:"Society for Neuroscience Annual Conference (SfN)",year:"2018",url:"https://www.abstractsonline.com/pp8/#!/4649/presentation/41470",tag:"sfn-2018-differential"},{title:"Ultra-high-resolution fMRI reveals differential representation of categories and domains across lateral and medial ventral temporal cortex",authors:["Margalit, E.","Jamison, K.","Weiner, K.S.","Vizioli, L.","Zhang, R.","Kay, K.N.","Grill-Spector, K."],journal:"Annual Meeting of the Vision Sciences Society (VSS)",year:"2019",url:"",tag:"vss-2019-7t"}]}}}},218:function(e){e.exports={projects:[{name:"Modeling the Functional Architecture of the Visual System",description:"The visual system is full of rich and interesting phenomena, many of which are poorly understood. We are using deep convolutional neural networks to model the development of the ventral visual system, which is thought to be responsible for recognizing objects. Large-scale models of the development of this system will enable us to understand how an animal's experience may affect the structure and development of its visual system.",img_src:"img/spacenet.png",url:"img/Margalit_et_al_VSS_2018.png",url_text:"VSS Poster (11MB)",alt:"Orientation tuning map from a computational model of primary visual cortex",id:"func-arch"},{name:"Characterizing the Fine Scale Functional Architecture of the Ventral Temporal Cortex",description:"Recent advances in fMRI technology have allowed insights into the functional organization of cortical regions at unprecented resolution. In collaboration with the CVN Lab at the University of Minnesota, we are examining areas of the brain that preferentially respond to different object categories and studying their arrangement in the cortical sheet.",img_src:"img/vtc_faces.png",url:"https://www.abstractsonline.com/pp8/#!/4649/presentation/41470",url_text:"SfN 2018 Abstract",alt:"Category tuning maps from ultra-high-res human fMRI",id:"hires"},{name:"Interactive Web Application for Gabor-jet Model",description:"The Gabor-jet model is a tool used to compute the psychophysical dissimilarity between images: an objective metric of how dissimilar two images appear. The model predicts over 90% of the variance in human responses on a match-to-sample task with artificial face stimuli, making it an invaluable tool in psychophysical research. I designed a webpage to act as an interactive guided tour of the model, allowing users to upload and test their own stimuli in-browser. Try it yourself!",img_src:"img/gabor_grab.PNG",url:"http://geon.usc.edu/GJW",url_text:"Online Gabor Filtering App",alt:"Screenshot from online Gabor filtering app",id:"web-app"},{name:"Object Familiarity in LOC",description:"Hundreds of studies have explored the Lateral Occipital Complex (LOC), which is critical for shape perception. Early studies discounted a role of familiarity by showing that \u201cabstract\u201d sculptures, unfamiliar to the subjects, also activated this region. This characterization of LOC as a region that responds to shape independently of familiarity had been accepted but never tested with control of the same low-level features. We assessed LOC\u2019s response to objects that had identical parts in two different arrangements, one familiar and the other novel. The early work was correct: there is no net effect of familiarity in LOC, although multivariate classifiers suggest that LOC does distinguish familiar from novel objects.",img_src:"img/LOC_mricrogl.png",url:"http://jov.arvojournals.org/article.aspx?articleid=2551659",url_text:"Article in Journal of Vision",alt:"Brain representation showing regions activated more by unfamiliar than familiar images",id:"loc-fam"},{name:"Developmental Prosopagnosia",description:"Developmental prosopagnosics (DPs) present no lesions nor have a history of compromised neural functioning. Given that their activation of face-selective cortex is normal, it surprised us that their capacity to perceptually discriminate faces and non-face objects had never been rigorously assessed. Normal discrimination of faces would suggest that the underlying deficit might not be a consequence of a poor perceptual representation but, instead, difficulty in matching a well-defined representation to stored representations in memory. If a deficit in discriminating faces is observed, is it also manifested when discriminating non-face stimuli that differ along the same underlying physical attributes as faces and to an equivalent extent as the faces? We found that, indeed, DPs present deficits in discriminating both faces and tooth-like blobs, but that this deficit does not extend to simple geometric primitives.",img_src:"img/dissim_histogram.png",url:"http://geon.usc.edu/~biederman/presentations/Margalit_et_al_VSS_16.pdf",url_text:"VSS Poster",alt:"Histogram of image dissimilarity for two image classes used to explore face processing",id:"dp-hist"}]}},253:function(e,t,a){e.exports=a(468)},258:function(e,t,a){},260:function(e,t,a){},264:function(e,t,a){},266:function(e,t,a){},268:function(e,t,a){var n={"./LOC_mricrogl.png":108,"./Margalit_et_al_2017_Fig3.png":109,"./Margalit_et_al_VSS_2018.png":110,"./SU_Seal_Red.png":65,"./USC_Seal.png":66,"./clouds_rest.png":61,"./dissim_histogram.png":111,"./dp_gui_screenshot.png":112,"./dp_table.png":67,"./emcog.jpg":68,"./gabor_grab.PNG":113,"./geon_brain.png":69,"./headshot-sq.png":70,"./snail-sq.png":43,"./spacenet.png":114,"./vpnl.png":44,"./vtc_faces.png":115,"./vtc_fade.gif":64};function i(e){var t=r(e);return a(t)}function r(e){var t=n[e];if(!(t+1)){var a=new Error("Cannot find module '"+e+"'");throw a.code="MODULE_NOT_FOUND",a}return t}i.keys=function(){return Object.keys(n)},i.resolve=r,e.exports=i,i.id=268},272:function(e,t,a){},274:function(e,t,a){var n={"./img/LOC_mricrogl.png":108,"./img/Margalit_et_al_2017_Fig3.png":109,"./img/Margalit_et_al_VSS_2018.png":110,"./img/SU_Seal_Red.png":65,"./img/USC_Seal.png":66,"./img/clouds_rest.png":61,"./img/dissim_histogram.png":111,"./img/dp_gui_screenshot.png":112,"./img/dp_table.png":67,"./img/emcog.jpg":68,"./img/gabor_grab.PNG":113,"./img/geon_brain.png":69,"./img/headshot-sq.png":70,"./img/snail-sq.png":43,"./img/spacenet.png":114,"./img/vpnl.png":44,"./img/vtc_faces.png":115,"./img/vtc_fade.gif":64};function i(e){var t=r(e);return a(t)}function r(e){var t=n[e];if(!(t+1)){var a=new Error("Cannot find module '"+e+"'");throw a.code="MODULE_NOT_FOUND",a}return t}i.keys=function(){return Object.keys(n)},i.resolve=r,e.exports=i,i.id=274},36:function(e,t,a){"use strict";a.r(t);var n=["koch_pinwheels.json","koulakov_chklovskii_wiring_cost.json","livingstone_hubel_color.json","nauhaus_orthogonality.json","ponce_livingstone_super_stimuli.json","ringach_orientation_selectivity.json","hubel_wiesel_v2v3.json","dicarlo_untangling.json","olshausen_field_sparse_coding.json","kriegeskorte_monkey_human.json","grill_spector_functional_architecture.json","devalois_direction_selectivity.json","ackman_retinal_waves.json","afraz_inactivation_face_gender.json","rajalingham_inactivation.json","bracci_ventral_shape.json","jacobs_jordan_short_connections.json","weliky_katz_disruption.json","garg_color_orientation.json","kim_pasupathy_texture_shape_v4.json","haxby_hyperalignment.json","sato_mosaic.json","arcaro_universal_mechanisms.json","op_de_beeck_factors.json","jang_orientation_classification.json","ohki_pinwheel_order.json","chapman_orientation_development.json","fahey_mouse_global_orientation.json","otoole_face_dcnn.json","durbin_mitchison_som.json","linsker_spatial_opponent.json","khalig-razavi_dnn.json"].map(function(e){return"review_jsons/"+e});t.default=n},43:function(e,t,a){e.exports=a.p+"static/media/snail-sq.3afa244f.png"},44:function(e,t,a){e.exports=a.p+"static/media/vpnl.c5822a31.png"},460:function(e,t,a){},462:function(e,t,a){},464:function(e,t,a){},468:function(e,t,a){"use strict";a.r(t);var n=a(2),i=a.n(n),r=a(25),o=a.n(r),s=(a(258),a(9)),l=a(10),c=a(12),u=a(11),h=a(13),m=a(105),d=a(71),p=a(3),g=a(7),f=a(63),y=(a(260),a(43)),v=a.n(y),b=a(44),w=a.n(b),E=a(61),k=a.n(E),_=[{link:"https://scholar.google.com/citations?user=ijttsicAAAAJ&hl=en&oi=ao",text:"Google Scholar",icon:i.a.createElement(g.n,{color:"#8c1313",size:"40px"})},{link:"https://github.com/eshedmargalit",text:"GitHub",icon:i.a.createElement(g.l,{color:"#8c1313",size:"40px"})},{link:"https://twitter.com/eshedmargalit",text:"Twitter",icon:i.a.createElement(g.v,{color:"#8c1313",size:"40px"})},{link:"https://www.linkedin.com/in/eshed-margalit-437222a7",text:"LinkedIn",icon:i.a.createElement(g.r,{color:"#8c1313",size:"40px"})}],j=function(e){function t(){return Object(s.a)(this,t),Object(c.a)(this,Object(u.a)(t).apply(this,arguments))}return Object(h.a)(t,e),Object(l.a)(t,[{key:"render_buttons",value:function(){var e=_.map(function(e){return i.a.createElement(p.e,{lg:"3",xs:"3",key:e.link},i.a.createElement(p.b,{className:"hover-button",color:"link"},i.a.createElement("a",{href:e.link,target:"_blank",rel:"noopener noreferrer"},e.icon)))});return i.a.createElement(p.u,null,e)}},{key:"render",value:function(){return i.a.createElement(p.g,{className:"vertical-align"},i.a.createElement(p.u,{className:"vertical-align"},i.a.createElement(p.e,{xs:"8",lg:"4"},i.a.createElement("h1",null,"Eshed Margalit"),i.a.createElement("h5",null,"PhD Candidate in the"," ",i.a.createElement("a",{href:"https://med.stanford.edu/neurogradprogram.html"},"Stanford Neurosciences Graduate Program")),i.a.createElement("hr",null),i.a.createElement(p.g,null,i.a.createElement(p.u,null,i.a.createElement(p.e,{xs:"8",lg:"7"},i.a.createElement("a",{href:"http://neuroailab.stanford.edu/"},i.a.createElement("h6",null,"Stanford NeuroAI Lab")),i.a.createElement("p",null,"PI: Dan Yamins")),i.a.createElement(p.e,{xs:"4",lg:"5"},i.a.createElement("img",{src:v.a,alt:"SNAIL Logo",className:"full-width"}))),i.a.createElement(p.u,null,i.a.createElement(p.e,{xs:"8",lg:"7"},i.a.createElement("a",{href:"http://vpnl.stanford.edu/"},i.a.createElement("h6",null,"Stanford Vision & Perception Neuroscience Lab")),i.a.createElement("p",null,"PI: Kalanit Grill-Spector")),i.a.createElement(p.e,{xs:"4",lg:"5"},i.a.createElement("img",{src:w.a,alt:"VPNL Logo",className:"full-width"})))),i.a.createElement("hr",null),i.a.createElement(f.a,{to:"/CV"},i.a.createElement(p.b,{outline:!0,color:"primary"},"CV"))," ",i.a.createElement(f.a,{to:"/Research"},i.a.createElement(p.b,{outline:!0,color:"primary"},"Research"))," "),i.a.createElement(p.e,{xs:"8",lg:"4"},i.a.createElement(p.g,null,i.a.createElement(p.u,null,i.a.createElement(p.e,{xs:"12",lg:"12"},i.a.createElement("img",{src:k.a,alt:"Headshot of Eshed",className:"circle full-width",id:"clouds-rest",href:"#"}),i.a.createElement(p.v,{placement:"top",target:"clouds-rest"},"This is me on a hike to Cloud's Rest in Yosemite, CA!"),i.a.createElement("p",{className:"center-text anonymous"},"eshed.margalit [at] gmail [dot] com"))))),i.a.createElement(p.e,{xs:"8",lg:"4"},i.a.createElement(p.g,null,i.a.createElement(p.u,null,i.a.createElement(p.e,{xs:"12",lg:"12"},i.a.createElement("hr",null),i.a.createElement("p",null,"I'm a neuroscience graduate student with an interest in vision, computational neuroscience, artificial intelligence, and neuroimaging! To learn more or to see some of my past work, please check out my"," ",i.a.createElement(f.a,{to:"/Research"},"research interests"),", or my"," ",i.a.createElement(f.a,{to:"/CV"},"CV"),". I also run the"," ",i.a.createElement("a",{href:"http://stanford-cnjc.github.io"},"Stanford Computational Neuroscience Journal Club"),", check it out!"))),this.render_buttons(),i.a.createElement("hr",null)))))}}]),t}(n.Component),x=(a(264),a(64)),S=a.n(x),C=function(e){function t(){return Object(s.a)(this,t),Object(c.a)(this,Object(u.a)(t).apply(this,arguments))}return Object(h.a)(t,e),Object(l.a)(t,[{key:"render",value:function(){return i.a.createElement(p.g,null,i.a.createElement(p.k,null,i.a.createElement(p.u,null,i.a.createElement(p.e,{xs:"12",lg:"12"},i.a.createElement("h3",null,"How does the brain make sense of the things we see?"),i.a.createElement("p",null,"I study the functional architecture of the primate visual system, with an interest in describing and modeling the computations that govern visual cortex and the representations that they operate on. I hope to contribute to our understanding of how information is transformed in neural networks and how such networks develop."),i.a.createElement(f.a,{to:"/Research"},i.a.createElement(p.b,{size:"lg",color:"secondary"},"My Research"))," ",i.a.createElement(f.a,{to:"/CV"},i.a.createElement(p.b,{size:"lg",color:"secondary"},"My CV")))),i.a.createElement("hr",null),i.a.createElement(p.u,null,i.a.createElement(p.e,{xs:"6",lg:"4"},i.a.createElement("img",{src:S.a,className:"full-width",alt:"GIF cycling through fMRI data from human ventral temporal cortex"})),i.a.createElement(p.e,{xs:"12",lg:"6"},i.a.createElement("p",null,"Stimulus preference maps in human higher visual cortex acquired at sub-millimeter resolution. The animation cycles through preference maps for four object categories: faces, places, words, and bodies. Can you tell which is which?")))))}}]),t}(n.Component),T=(a(266),a(214)),I=function(e){function t(){var e,n;Object(s.a)(this,t);for(var r=arguments.length,o=new Array(r),l=0;l<r;l++)o[l]=arguments[l];return(n=Object(c.a)(this,(e=Object(u.a)(t)).call.apply(e,[this].concat(o)))).render_featured_projects=function(e){return e.map(function(e){var t=e.url,n=e.alt,r=a(268)("./".concat(e.img_src));return i.a.createElement(p.e,{xs:"6",lg:"3",key:e.name},i.a.createElement("a",{href:t},i.a.createElement("img",{src:r,className:"full-width project",alt:n}),i.a.createElement("p",null,e.name)))})},n}return Object(h.a)(t,e),Object(l.a)(t,[{key:"render",value:function(){return i.a.createElement(p.g,null,i.a.createElement(p.u,null,i.a.createElement(p.e,{xs:"12",lg:"12"},i.a.createElement("h3",null,"My recent work at a glance"))),i.a.createElement(p.u,null,this.render_featured_projects(T.projects)))}}]),t}(n.Component),M=function(e){function t(){return Object(s.a)(this,t),Object(c.a)(this,Object(u.a)(t).apply(this,arguments))}return Object(h.a)(t,e),Object(l.a)(t,[{key:"render",value:function(){return i.a.createElement(n.Fragment,null,i.a.createElement("br",null),i.a.createElement(j,{className:"left-align"}),i.a.createElement("br",null),i.a.createElement(p.g,null,i.a.createElement("hr",null)),i.a.createElement(C,{className:"left-align"}),i.a.createElement(p.g,null,i.a.createElement("hr",null)),i.a.createElement(I,{className:"left-align"}))}}]),t}(n.Component),O=a(21),V=a(202),F=function(e){function t(e){var a;return Object(s.a)(this,t),(a=Object(c.a)(this,Object(u.a)(t).call(this,e))).toggle=a.toggle.bind(Object(O.a)(a)),a.state={isOpen:!1},a}return Object(h.a)(t,e),Object(l.a)(t,[{key:"toggle",value:function(){this.setState({isOpen:!this.state.isOpen})}},{key:"render",value:function(){return i.a.createElement("div",null,i.a.createElement(p.r,{color:"dark",dark:!0,expand:"md"},i.a.createElement(V.LinkContainer,{to:"/"},i.a.createElement(p.s,null,"Eshed Margalit")),i.a.createElement(p.t,{onClick:this.toggle}),i.a.createElement(p.f,{isOpen:this.state.isOpen,navbar:!0},i.a.createElement(p.o,{className:"ml-auto",navbar:!0},i.a.createElement(V.LinkContainer,{to:"/CV"},i.a.createElement(p.q,null,i.a.createElement(g.u,null)," ","CV")),i.a.createElement(V.LinkContainer,{to:"/Research"},i.a.createElement(p.q,null,i.a.createElement(g.f,null)," ","Research")),i.a.createElement(V.LinkContainer,{to:"/PaperReviews"},i.a.createElement(p.q,null,i.a.createElement(g.t,null)," ","Paper-a-Week")),i.a.createElement(V.LinkContainer,{to:"/DPCalc"},i.a.createElement(p.q,null,i.a.createElement(g.g,null)," ","d' Calculator"))))))}}]),t}(n.Component),z=a(216),N=function(e){function t(){return Object(s.a)(this,t),Object(c.a)(this,Object(u.a)(t).apply(this,arguments))}return Object(h.a)(t,e),Object(l.a)(t,[{key:"render",value:function(){return i.a.createElement(p.g,null,i.a.createElement(p.u,{className:"vertical-align"},i.a.createElement(p.e,{xs:"12",lg:"12"},i.a.createElement("hr",null),i.a.createElement("p",null,"Please get in touch, I'd love to hear from you."," ",i.a.createElement(g.j,{color:"",id:"gmail_address",size:"1.5em"}),i.a.createElement(p.v,{autohide:!1,placement:"top-end",target:"gmail_address"},"eshed.margalit@gmail.com"," ",i.a.createElement(p.b,{color:"link",size:"sm"},i.a.createElement(z.a,{className:"copy-src","data-clipboard-text":"eshed.margalit@gmail.com",size:"1.5em"}))," ")),i.a.createElement("p",null,"Website designed by Eshed, browse the source code on Github:",i.a.createElement(p.b,{color:"link",href:"http://github.com/eshedmargalit/eshedmargalit.github.io"},i.a.createElement(g.l,{size:"1.5em"}))))))}}]),t}(n.Component),q=a(217),A=(a(117),a(65)),R=a.n(A),P=a(66),D=a.n(P),B=a(70),G=a.n(B),H=a(69),U=a.n(H),L=a(68),W=a.n(L),K=a(18),J=a.n(K),Y=function(e){function t(){var e,a;Object(s.a)(this,t);for(var n=arguments.length,r=new Array(n),o=0;o<n;o++)r[o]=arguments[o];return(a=Object(c.a)(this,(e=Object(u.a)(t)).call.apply(e,[this].concat(r)))).render_author=function(e){return"Margalit, E."===e?i.a.createElement("strong",null,e):i.a.createElement("span",null,e)},a.render_authors=function(e){return e.map(function(t,n){var r,o=a.render_author(t);return r=n===e.length-1?1===e.length?i.a.createElement("span",null,o,i.a.createElement("br",null)):i.a.createElement("span",null,"and ",o,i.a.createElement("br",null)):n===e.length-2?i.a.createElement("span",null,o," "):i.a.createElement("span",null,o,","," "),i.a.createElement("span",{key:e.tag+t},r)})},a.render_pub=function(e){var t;return e.authors.tag=e.tag,t=""===e.url?null:i.a.createElement(p.b,{color:"link",target:"_blank",href:e.url},i.a.createElement(g.q,{size:"1.5em"})),i.a.createElement(p.n,null,i.a.createElement("p",null,e.year),i.a.createElement("h5",null,e.title),a.render_authors(e.authors),i.a.createElement("p",null,i.a.createElement("em",null,e.journal,e.volume,e.pages)),t)},a.render_pubs=function(e){return e.map(function(e){return i.a.createElement("span",{key:e.tag},a.render_pub(e))})},a}return Object(h.a)(t,e),Object(l.a)(t,[{key:"render",value:function(){return 0===this.props.pubs.length?null:(this.props.pubs.sort(function(e,t){return-J()(new Date(e.year)).diff(J()(new Date(t.year)))}),i.a.createElement(p.g,null,i.a.createElement(p.u,null,i.a.createElement(p.e,null,i.a.createElement(p.m,null,i.a.createElement("h4",null,this.props.name),this.render_pubs(this.props.pubs))))))}}]),t}(n.Component),Q=function(e){function t(e){var a;return Object(s.a)(this,t),(a=Object(c.a)(this,Object(u.a)(t).call(this,e))).toggle=a.toggle.bind(Object(O.a)(a)),a.state={collapse:e.default_open},a}return Object(h.a)(t,e),Object(l.a)(t,[{key:"toggle",value:function(){this.setState({collapse:!this.state.collapse})}},{key:"render",value:function(){var e=this.state.collapse?i.a.createElement(g.b,null):i.a.createElement(g.a,null);return i.a.createElement(n.Fragment,null,i.a.createElement(p.b,{color:"link",onClick:this.toggle},i.a.createElement("h4",null,this.props.icon," ",this.props.title,e)),i.a.createElement("em",null,this.props.description),i.a.createElement(p.f,{isOpen:this.state.collapse},i.a.createElement(p.c,null,i.a.createElement(p.d,null,this.props.internal))))}}]),t}(n.Component),X=[{link:"https://scholar.google.com/citations?user=ijttsicAAAAJ&hl=en&oi=ao",text:"Google Scholar",icon:i.a.createElement(g.n,{color:"#8c1313",size:"40px"})},{link:"https://github.com/eshedmargalit",text:"GitHub",icon:i.a.createElement(g.l,{color:"#8c1313",size:"40px"})},{link:"https://twitter.com/eshedmargalit",text:"Twitter",icon:i.a.createElement(g.v,{color:"#8c1313",size:"40px"})},{link:"https://www.linkedin.com/in/eshed-margalit-437222a7",text:"LinkedIn",icon:i.a.createElement(g.r,{color:"#8c1313",size:"40px"})}],Z=function(e){function t(e){var a;return Object(s.a)(this,t),(a=Object(c.a)(this,Object(u.a)(t).call(this,e))).render_award=function(e){var t;return t=""===e.url?null:i.a.createElement(p.b,{color:"link",href:e.url},i.a.createElement(g.q,{size:"1.5em"})),i.a.createElement(p.n,null,i.a.createElement("h6",null,e.name),i.a.createElement("p",null,e.dates),i.a.createElement("p",null,e.description),t)},a.cv=q.CV,a}return Object(h.a)(t,e),Object(l.a)(t,[{key:"render_buttons",value:function(){var e=X.map(function(e){return i.a.createElement(p.e,{lg:"3",xs:"3",key:e.link},i.a.createElement(p.b,{className:"hover-button",color:"link"},i.a.createElement("a",{href:e.link,target:"_blank",rel:"noopener noreferrer"},e.icon)))});return i.a.createElement(p.u,null,e)}},{key:"render_education",value:function(){return i.a.createElement(p.m,null,i.a.createElement(p.n,null,i.a.createElement(p.g,null,i.a.createElement(p.u,null,i.a.createElement(p.e,{xs:"6",lg:"2"},i.a.createElement("img",{src:R.a,alt:"Stanford official seal",className:"full-width"})),i.a.createElement(p.e,{lg:"10"},i.a.createElement("h5",null,"Stanford University"),i.a.createElement("p",null,"2016 - Present"),i.a.createElement("p",null,"PhD Candidate, Neurosciences PhD Program"))))),i.a.createElement(p.n,null,i.a.createElement(p.g,null,i.a.createElement(p.u,null,i.a.createElement(p.e,{xs:"6",lg:"2"},i.a.createElement("img",{src:D.a,alt:"USC official seal",className:"full-width"})),i.a.createElement(p.e,{lg:"10"},i.a.createElement("h5",null,"University of Southern California"),i.a.createElement("p",null,"2012 - 2016"),i.a.createElement("p",null,"B.S. with Honors in Computational Neuroscience",i.a.createElement("br",null),"Minor in Computer Science"),i.a.createElement("p",null,"Cumulative GPA: 3.99"))))))}},{key:"render_research",value:function(){return i.a.createElement(n.Fragment,null,i.a.createElement("h5",null,"Current"),i.a.createElement(p.m,null,i.a.createElement(p.n,null,i.a.createElement(p.g,null,i.a.createElement(p.u,null,i.a.createElement(p.e,{lg:"2",xs:"6"},i.a.createElement("img",{src:v.a,className:"full-width",alt:"Stanford NeuroAI Lab Logo"})),i.a.createElement(p.e,{lg:"10",xs:"10"},i.a.createElement("h6",null,"Stanford NeuroAI Lab"," ",i.a.createElement(p.b,{outline:!0,color:"link",size:"1.5em",href:"http://neuroailab.stanford.edu/"},i.a.createElement(g.q,null))),i.a.createElement("p",null,"2016 - Present",i.a.createElement("br",null),"PI: Dan Yamins"),i.a.createElement("p",null,i.a.createElement("em",null,"Modeling structure and development of primate visual cortex")))))),i.a.createElement(p.n,null,i.a.createElement(p.g,null,i.a.createElement(p.u,null,i.a.createElement(p.e,{lg:"2",xs:"6"},i.a.createElement("img",{src:w.a,className:"full-width",alt:"VPNL logo"})),i.a.createElement(p.e,{lg:"10",xs:"10"},i.a.createElement("h6",null,"Stanford Vision and Perception Neuroscience Lab"," ",i.a.createElement(p.b,{outline:!0,color:"link",size:"1.5em",href:"http://vpnl.stanford.edu/"},i.a.createElement(g.q,null))),i.a.createElement("p",null,"2016 - Present",i.a.createElement("br",null),"PI: Kalanit Grill-Spector"),i.a.createElement("p",null,i.a.createElement("em",null,"Characterization of human higher visual cortex via ultra-high-resolution fMRI"))))))),i.a.createElement("br",null),i.a.createElement("h5",null,"Past"),i.a.createElement(p.m,null,i.a.createElement(p.n,null,i.a.createElement(p.g,null,i.a.createElement(p.u,null,i.a.createElement(p.e,{lg:"2",xs:"6"},i.a.createElement("img",{src:U.a,className:"full-width",alt:"IUL lab logo"})),i.a.createElement(p.e,{lg:"10",xs:"10"},i.a.createElement("h6",null,"USC Image Understanding Lab"," ",i.a.createElement(p.b,{outline:!0,color:"link",size:"1.5em",href:"http://geon.usc.edu/"},i.a.createElement(g.q,null))),i.a.createElement("p",null,"2014 - 2016",i.a.createElement("br",null),i.a.createElement("strong",null,"PI: ")," Irving Biederman"),i.a.createElement("p",null,i.a.createElement("em",null,"Interrogating object representations in visual cortex and psychophysical correlates of developmental prosopagnosia")))))),i.a.createElement(p.n,null,i.a.createElement(p.g,null,i.a.createElement(p.u,null,i.a.createElement(p.e,{lg:"2",xs:"6"},i.a.createElement("img",{src:W.a,className:"full-width",alt:"Emotion and Cognition Lab Logo"})),i.a.createElement(p.e,{lg:"10",xs:"10"},i.a.createElement("h6",null,"USC Emotion and Cognition Lab"," ",i.a.createElement(p.b,{outline:!0,color:"link",size:"1.5em",href:"http://gero.usc.edu/labs/matherlab/"},i.a.createElement(g.q,null))),i.a.createElement("p",null,"2013 - 2014",i.a.createElement("br",null),i.a.createElement("strong",null,"PI: ")," Mara Mather"),i.a.createElement("p",null,i.a.createElement("em",null,"Investigating the role of the noradrenergic arousal system in aging and memory"))))))))}},{key:"render_publications",value:function(){return i.a.createElement(n.Fragment,null,i.a.createElement(p.b,{href:"https://scholar.google.com/citations?user=ijttsicAAAAJ&hl=en",color:"secondary"},i.a.createElement(g.m,{size:"1em"})," | My Google Scholar Profile"),i.a.createElement("hr",null),i.a.createElement(Y,{name:"Published",pubs:this.cv.publications.published}),i.a.createElement("br",null),i.a.createElement(Y,{name:"Preprints",pubs:this.cv.publications.preprints}))}},{key:"render_presentations",value:function(){return i.a.createElement(n.Fragment,null,i.a.createElement(Y,{name:"Talks",pubs:this.cv.presentations.talks}),i.a.createElement("br",null),i.a.createElement(Y,{name:"Posters",pubs:this.cv.presentations.posters}))}},{key:"render_skills",value:function(){return i.a.createElement(p.g,null,i.a.createElement(p.u,null,i.a.createElement(p.e,null,"Check out my GitHub to see what I'm up to:",i.a.createElement(p.b,{color:"link"},i.a.createElement(g.l,{size:"2em"})))),i.a.createElement("hr",null),i.a.createElement(p.u,null,i.a.createElement(p.e,{xs:"12",lg:"4"},i.a.createElement("h3",null,"Things I use daily"),i.a.createElement(p.m,null,i.a.createElement(p.n,null,"Python"),i.a.createElement(p.n,null,"MATLAB"),i.a.createElement(p.n,null,"Git / Github"),i.a.createElement(p.n,null,"TensorFlow"),i.a.createElement(p.n,null,"Jupyter"))),i.a.createElement(p.e,{xs:"12",lg:"4"},i.a.createElement("h3",null,"Things I use pretty often"),i.a.createElement(p.m,null,i.a.createElement(p.n,null,"Google TPUv2"),i.a.createElement(p.n,null,"HTML/CSS/React/Redux"),i.a.createElement(p.n,null,"C, C++, Java"),i.a.createElement(p.n,null,"FSL"),i.a.createElement(p.n,null,"FreeSurfer"),i.a.createElement(p.n,null,"Photoshop and Illustrator"))),i.a.createElement(p.e,{xs:"12",lg:"4"},i.a.createElement("h3",null,"Things I've used before"),i.a.createElement(p.m,null,i.a.createElement(p.n,null,"Patch-clamp physiology"),i.a.createElement(p.n,null,"Multi-unit in-vivo recording"),i.a.createElement(p.n,null,"Psychtoolbox, PsychoPy"),i.a.createElement(p.n,null,"Spike sorting (KiloSort)")))))}},{key:"render_awards",value:function(){var e=this;return i.a.createElement(p.g,null,i.a.createElement(p.u,null,i.a.createElement(p.e,null,i.a.createElement(p.m,null,this.cv.awards.map(function(t){return i.a.createElement("span",{key:t.name},e.render_award(t))})))))}},{key:"render_teaching",value:function(){return i.a.createElement(p.g,null,i.a.createElement(p.u,null,i.a.createElement(p.e,null,i.a.createElement(p.m,null,i.a.createElement(p.n,null,i.a.createElement("h6",null,"Teaching Assistant, Introduction to Perception (PSYCH 30)"),i.a.createElement("p",null,"Fall 2017, 2018")),i.a.createElement(p.n,null,i.a.createElement("h6",null,"Teaching Assistant,"," ",i.a.createElement("a",{href:"https://med.stanford.edu/neurogradprogram/prospective_students/bootcamp.html"},"Stanford Intensive Neuroscience Bootcamp")),i.a.createElement("p",null,"Fall 2017")),i.a.createElement(p.n,null,i.a.createElement("h6",null,"Instructor,"," ",i.a.createElement("a",{href:"https://stanfordesp.org/"},"Stanford Splash")),i.a.createElement("p",null,"Fall 2017")),i.a.createElement(p.n,null,i.a.createElement("h6",null,"Instructor,"," ",i.a.createElement("a",{href:"https://med.stanford.edu/neurogradprogram/news_and_events/news/brain-day.html"},"Stanford Brain Day")),i.a.createElement("p",null,"Fall 2017"))))))}},{key:"render_service",value:function(){return i.a.createElement(p.g,null,i.a.createElement(p.u,null,i.a.createElement(p.e,null,i.a.createElement(p.m,null,i.a.createElement(p.n,null,i.a.createElement("h6",null,"Chair, Society for Neuroscience Nanosymposium: Extrastriate Vision"),i.a.createElement("p",null,"2018")),i.a.createElement(p.n,null,i.a.createElement("h6",null,"Co-Leader,"," ",i.a.createElement("a",{href:"http://stanford-cnjc.github.io"},"Computational Neuroscience Journal Club")),i.a.createElement("p",null,"2018")),i.a.createElement(p.n,null,i.a.createElement("h6",null,"Stanford Neurosciences PhD Program Student Program Committee"),i.a.createElement("p",null,"2018")),i.a.createElement(p.n,null,i.a.createElement("h6",null,"Mentor and workshop leader for NSF GRFP Application"),i.a.createElement("p",null,"2017 - 2018")),i.a.createElement(p.n,null,i.a.createElement("h6",null,"Student Speaker Representative, Stanford Neurosciences PhD Program"),i.a.createElement("p",null,"2017 - 2018")),i.a.createElement(p.n,null,i.a.createElement("h6",null,"Mentor, Stanford Biosciences Student Association"),i.a.createElement("p",null,"2017 - 2018")),i.a.createElement(p.n,null,i.a.createElement("h6",null,"Student Representative, USC Undergraduate Neuroscience Program Executive Committee"),i.a.createElement("p",null,"2015 - 2016")),i.a.createElement(p.n,null,i.a.createElement("h6",null,"Mentor to undergraduate lab members"),i.a.createElement("p",null,"2013 - present"),i.a.createElement("p",null,"Jordan Juarez, Isabel Irawan, Emily Meschke, Rafael Maarek, and Catrina Hacker")),i.a.createElement(p.n,null,i.a.createElement("h6",null,"Team captain, USC Cross Country Club"),i.a.createElement("p",null,"2014 -2015"))))))}},{key:"render_scicomm",value:function(){return i.a.createElement(p.g,null,i.a.createElement(p.u,null,i.a.createElement(p.e,null,i.a.createElement(p.m,null,i.a.createElement(p.n,null,i.a.createElement("h6",null,"Back to Basics with Visual Feedbacks"),i.a.createElement("p",null,"Summary of"," ",i.a.createElement("a",{href:"https://www.ncbi.nlm.nih.gov/pubmed/29662217"},"Marques et al., 2018")),i.a.createElement(p.b,{color:"link",href:"http://www.neuwritewest.org/blog/back-to-basics-with-visual-feedbacks"},i.a.createElement(g.q,{size:"1.5em"})," Link to Blog Post")),i.a.createElement(p.n,null,i.a.createElement("h6",null,"Nurturing the Study of Nature"),i.a.createElement("p",null,"Summary of"," ",i.a.createElement("a",{href:"https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0148405"},"Grunspan et al., 2016")),i.a.createElement(p.b,{color:"link",href:"http://www.neuwritewest.org/blog/nurturing-the-study-of-nature"},i.a.createElement(g.q,{size:"1.5em"})," Link to Blog Post")),i.a.createElement(p.n,null,i.a.createElement("h6",null,"Paper a Week"),i.a.createElement("p",null," ","For more, check out my"," ",i.a.createElement(f.a,{to:"/PaperReviews"},"Paper-A-Week")," page, where I review in detail one paper each week!"," "))))))}},{key:"render",value:function(){return i.a.createElement(p.g,null,i.a.createElement(p.u,null,i.a.createElement(p.e,{xs:"8",lg:"4"},i.a.createElement(p.g,null,i.a.createElement(p.u,null,i.a.createElement(p.e,null,i.a.createElement("br",null),i.a.createElement("h1",{className:"display-5"},"Curriculum Vitae"),i.a.createElement("h6",null,"Current through ",this.cv.last_updated),i.a.createElement("p",null,"An interactive version of my CV can be found below. If you'd prefer a PDF version, please click this link instead."),i.a.createElement("p",null,i.a.createElement(p.b,{color:"secondary",href:"files/"+this.cv.pdf.path},i.a.createElement(g.k,{size:"3em"}),this.cv.pdf.path)))))),i.a.createElement(p.e,{xs:"8",lg:"4"},i.a.createElement(p.g,null,i.a.createElement(p.u,null,i.a.createElement(p.e,{xs:"12",lg:"12"},i.a.createElement("img",{src:G.a,alt:"Headshot of Eshed",className:"circle full-width"}),i.a.createElement("p",{className:"center-text anonymous"},"eshed.margalit [at] gmail [dot] com"))))),i.a.createElement(p.e,{xs:"8",lg:"4"},i.a.createElement(p.g,null,i.a.createElement(p.u,null,i.a.createElement(p.e,{xs:"12",lg:"12"},i.a.createElement("hr",null),i.a.createElement("p",null,"I'm a neuroscience graduate student at Stanford with an interest in vision, computational neuroscience, artificial intelligence, and neuroimaging."),i.a.createElement("hr",null))),i.a.createElement(p.u,null,i.a.createElement(p.e,null,this.render_buttons(),i.a.createElement("hr",null)))))),i.a.createElement("hr",null),i.a.createElement(p.u,null,i.a.createElement(p.e,null,i.a.createElement(Q,{default_open:!0,title:"Education",icon:i.a.createElement(g.w,{size:"1em"}),description:"Education history and current affiliations",internal:this.render_education()}))),i.a.createElement("hr",null),i.a.createElement(p.u,null,i.a.createElement(p.e,null,i.a.createElement(Q,{default_open:!0,title:"Research",icon:i.a.createElement(g.f,{size:"1em"}),description:"Labs I've worked in",internal:this.render_research()}))),i.a.createElement("hr",null),i.a.createElement(p.u,null,i.a.createElement(p.e,null,i.a.createElement(Q,{default_open:!0,title:"Publications",icon:i.a.createElement(g.s,{size:"1em"}),description:"Peer-reviewed publications",internal:this.render_publications()}))),i.a.createElement("hr",null),i.a.createElement(p.u,null,i.a.createElement(p.e,null,i.a.createElement(Q,{default_open:!0,title:"Presentations",icon:i.a.createElement(g.x,{size:"1em"}),description:"Conference posters and talks",internal:this.render_presentations()}))),i.a.createElement("hr",null),i.a.createElement(p.u,null,i.a.createElement(p.e,null,i.a.createElement(Q,{default_open:!0,title:"Skills",icon:i.a.createElement(g.p,{size:"1em"}),description:"Methods, programming, and other training",internal:this.render_skills()}))),i.a.createElement("hr",null),i.a.createElement(p.u,null,i.a.createElement(p.e,null,i.a.createElement(Q,{default_open:!0,title:"Awards",icon:i.a.createElement(g.d,{size:"1em"}),description:"Grants, awards, and recognitions",internal:this.render_awards()}))),i.a.createElement("hr",null),i.a.createElement(p.u,null,i.a.createElement(p.e,null,i.a.createElement(Q,{default_open:!0,title:"Teaching",icon:i.a.createElement(g.h,{size:"1em"}),description:"Teaching experience",internal:this.render_teaching()}))),i.a.createElement("hr",null),i.a.createElement(p.u,null,i.a.createElement(p.e,null,i.a.createElement(Q,{default_open:!0,title:"Service",icon:i.a.createElement(g.o,{size:"1em"}),description:"Mentorship and volunteer work",internal:this.render_service()}))),i.a.createElement("hr",null),i.a.createElement(p.u,null,i.a.createElement(p.e,null,i.a.createElement(Q,{default_open:!0,title:"Science Communication",icon:i.a.createElement(g.i,{size:"1em"}),description:"Writing about others' science",internal:this.render_scicomm()}))))}}]),t}(n.Component),$=(a(272),a(218)),ee=function(e){function t(){var e,n;Object(s.a)(this,t);for(var r=arguments.length,o=new Array(r),l=0;l<r;l++)o[l]=arguments[l];return(n=Object(c.a)(this,(e=Object(u.a)(t)).call.apply(e,[this].concat(o)))).render_projects=function(e){return e.map(function(e){var t;t=""===e.url?null:i.a.createElement(p.b,{color:"secondary",href:e.url},i.a.createElement(g.q,null)," "," "," ",e.url_text);var n=a(274)("./".concat(e.img_src));return i.a.createElement("span",{key:e.name},i.a.createElement(p.n,null,i.a.createElement(p.u,null,i.a.createElement(p.e,{lg:"9"},i.a.createElement("h2",null,e.name),i.a.createElement("p",null,e.description),t),i.a.createElement(p.e,{lg:"3"},i.a.createElement("img",{src:n,alt:e.alt,id:e.id,className:"circle full-width"}),i.a.createElement(p.v,{placement:"top",target:e.id},e.alt)))))})},n}return Object(h.a)(t,e),Object(l.a)(t,[{key:"render",value:function(){return i.a.createElement(p.g,null,i.a.createElement(p.u,null,i.a.createElement(p.e,{xs:"8",lg:"8"},i.a.createElement("br",null),i.a.createElement("h1",null,"Projects I've worked on"),i.a.createElement("p",null,"This is an incomplete list of projects that have interested me, please get in touch if you'd like to chat further about any of these things!"),i.a.createElement(p.b,{href:"https://scholar.google.com/citations?user=ijttsicAAAAJ&hl=en",color:"primary"},i.a.createElement(g.m,{size:"1em"})," | My Google Scholar Profile"))),i.a.createElement("br",null),i.a.createElement(p.m,null,this.render_projects($.projects)))}}]),t}(n.Component),te=a(205),ae=(a(118),a(219)),ne=a.n(ae),ie=a(36),re=function(e){function t(e){var n;return Object(s.a)(this,t),(n=Object(c.a)(this,Object(u.a)(t).call(this,e))).load_and_concat_reviews=function(e){return e.map(function(e){return a(119)("./".concat(e))})},n.get_tag_color=function(e){for(var t=0,a=0;a<e.length;a++)t=e.charCodeAt(a)+((t<<5)-t);return"hsl("+t%360+",80%,30%)"},n.render_comma_sep_list=function(e){return e.map(function(t,a){var n;return n=a===e.length-1?1===e.length?i.a.createElement("span",null,t,i.a.createElement("br",null)):i.a.createElement("span",null,"and ",t,i.a.createElement("br",null)):a===e.length-2?i.a.createElement("span",null,t," "):i.a.createElement("span",null,t,","," "),i.a.createElement("span",{key:e.tag+t},n)})},n.handleSearch=function(e){n.setState({query:e,searchbar_value:e})},n.handleSort=function(e){n.setState({sort_mode:e})},n.render_paper_metadata=function(e){var t=e.keywords,a=null;t&&(a=t.map(function(e){return i.a.createElement("span",{key:e},i.a.createElement(p.a,{style:{background:n.get_tag_color(e)},className:"tag-badge",onClick:function(e){e.stopPropagation(),e.preventDefault(),n.handleSearch("".concat(e.target.innerHTML))},href:"#"},e)," ")}));var r=J()(e.review_date,"YYYY-MM-DD").format("MMMM DD, YYYY"),o=J()(e.date,"YYYY-MM").format("MMMM YYYY");return i.a.createElement("div",null,i.a.createElement("div",null,i.a.createElement("h5",null,e.title),n.render_comma_sep_list(e.authors),"Published ",o),i.a.createElement("div",null,a),i.a.createElement("div",null,i.a.createElement("strong",null,"TL;DR:")," ",e.one_sentence),i.a.createElement("div",null,i.a.createElement("em",null,"Read on ",r)))},n.sort_reviews=function(e){return"review date (descending)"===n.state.sort_mode?e.sort(function(e,t){return-J()(e.metadata.review_date).diff(J()(t.metadata.review_date))}):"review date (ascending)"===n.state.sort_mode?e.sort(function(e,t){return J()(e.metadata.review_date).diff(J()(t.metadata.review_date))}):"publication date (descending)"===n.state.sort_mode?e.sort(function(e,t){return-J()(e.metadata.date).diff(J()(t.metadata.date))}):"publication date (ascending)"===n.state.sort_mode?e.sort(function(e,t){return J()(e.metadata.date).diff(J()(t.metadata.date))}):void 0},n.trim_reviews=function(e){if(""===n.state.query)return e;return new ne.a(e,{shouldSort:!1,threshold:.2,location:0,distance:5e3,maxPatternLength:32,minMatchCharLength:4,keys:["metadata.title","metadata.authors","metadata.keywords","metadata.date"]}).search(n.state.query)},n.review_clicked=function(e){var t=e.metadata.date.substring(0,4),a=e.metadata.authors,i=a.length,r=(2===i?a[0].split(",")[0]+" and "+a[1].split(",")[0]:1===i?a[0].split(",")[0]:a[0].split(",")[0]+" et al.")+", "+t;n.setState({active_paper:e,viewing_paper:!0,current_paper_short_title:r})},n.render_papers=function(e){var t=e.map(function(e){return i.a.createElement(p.n,{action:!0,key:e.metadata.title,className:"review-lgi",onClick:function(t){n.review_clicked(e)}},n.render_paper_metadata(e.metadata))});return i.a.createElement(p.m,null,t)},n.render_review=function(e){var t=i.a.createElement("div",null,i.a.createElement("h6",null,"General Summary"),i.a.createElement("ul",null,e.review.summary.map(function(e){return i.a.createElement("li",{key:e},e)}))),a=i.a.createElement("div",null,i.a.createElement("h6",null,"Background"),i.a.createElement("ul",null,e.review.background.map(function(e){return i.a.createElement("li",{key:e},e)}))),r=i.a.createElement("div",null,i.a.createElement("h6",null,"Approach and Methods"),i.a.createElement("ul",null,e.review.approach.map(function(e){return i.a.createElement("li",{key:e},e)}))),o=i.a.createElement("div",null,i.a.createElement("h6",null,"Results"),i.a.createElement("ul",null,e.review.results.map(function(e){return i.a.createElement("li",{key:e},e)}))),s=i.a.createElement("div",null,i.a.createElement("h6",null,"Conclusions"),i.a.createElement("ul",null,e.review.conclusions.map(function(e){return i.a.createElement("li",{key:e},e)}))),l=null;e.review.other.length>0&&(l=i.a.createElement("div",null,i.a.createElement("h6",null,"Other information"),i.a.createElement("ul",null,e.review.other.map(function(e){return i.a.createElement("li",{key:e},e)}))));var c=J()(e.metadata.date,"YYYY-MM").format("MMMM YYYY"),u=null;return e.metadata.doi&&(u=i.a.createElement("a",{href:"http://dx.doi.org/"+e.metadata.doi,target:"_blank",rel:"noopener noreferrer"},"(",e.metadata.doi,")")),i.a.createElement(p.g,null,i.a.createElement(p.u,null,i.a.createElement(p.e,null,i.a.createElement("h4",null,e.metadata.title),n.render_comma_sep_list(e.metadata.authors),n.render_comma_sep_list(e.metadata.institutions),"Published in ",e.metadata.journal," in ",c," ",u,i.a.createElement("hr",null),t,a,r,o,s,l)),i.a.createElement(p.u,null,i.a.createElement(p.e,null,i.a.createElement(p.b,{onClick:function(e){n.setState({viewing_paper:!1}),window.scrollTo(0,n.navRef.current.offsetTop)},color:"primary"},i.a.createElement(g.c,null)," Back to List of Reviews"))))},n.load_and_concat_reviews=n.load_and_concat_reviews.bind(Object(O.a)(n)),n.papers=n.load_and_concat_reviews(ie.default),n.navRef=i.a.createRef(),n.state={query:"",searchbar_value:"",active_paper:null,viewing_paper:!1,current_paper_short_title:"Margalit et al., 2019",sort_mode:"review date (descending)"},n}return Object(h.a)(t,e),Object(l.a)(t,[{key:"render",value:function(){var e=this,t=null;this.state.searchbar_value&&(t=i.a.createElement(p.e,{lg:"2",xs:"2"},i.a.createElement(p.b,{style:{position:"absolute",bottom:"15px"},onClick:function(t){return e.setState({query:"",searchbar_value:""})},color:"danger"},i.a.createElement(g.e,null)," Clear Search")));var a=i.a.createElement(p.g,null,i.a.createElement(p.u,null,i.a.createElement(p.e,{lg:"7",xs:"7"},i.a.createElement(p.h,null,i.a.createElement(p.i,null,i.a.createElement(p.l,{for:"search_input"},"Search by author, year, title, or keyword:"),i.a.createElement(p.j,{type:"text",id:"search_input",onChange:function(t){return e.handleSearch("".concat(t.target.value))},placeholder:"e.g., orthogonal",value:this.state.searchbar_value})))),i.a.createElement(p.e,{lg:"3",xs:"3"},i.a.createElement(p.h,null,i.a.createElement(p.i,null,i.a.createElement(p.l,{for:"sort_input"},"Sort by:"),i.a.createElement(p.j,{type:"select",id:"sort_input",onChange:function(t){return e.handleSort("".concat(t.target.value))}},i.a.createElement("option",null,"review date (descending)"),i.a.createElement("option",null,"review date (ascending) "),i.a.createElement("option",null,"publication date (ascending)"),i.a.createElement("option",null,"publication date (descending)"))))),t),i.a.createElement(p.u,null,i.a.createElement(p.e,null,this.render_papers(this.sort_reviews(this.trim_reviews(this.papers)))))),n=a,r=i.a.createElement(p.o,{tabs:!0},i.a.createElement(p.p,null,i.a.createElement(p.q,{className:"nav-tab",active:!this.state.viewing_paper},"List of Reviews")));return this.state.active_paper&&(n=this.state.viewing_paper?this.render_review(this.state.active_paper):a,r=i.a.createElement(p.o,{tabs:!0},i.a.createElement(p.p,null,i.a.createElement(p.q,{className:"nav-tab",onClick:function(t){e.setState({viewing_paper:!1,query:"",searchbar_value:""})},active:!this.state.viewing_paper},"List of Reviews")),i.a.createElement(p.p,null,i.a.createElement(p.q,{className:"nav-tab",onClick:function(t){e.setState({viewing_paper:!0})},active:this.state.viewing_paper},this.state.current_paper_short_title)))),i.a.createElement("div",{ref:this.navRef},r,i.a.createElement("br",null),n)}}]),t}(n.Component),oe=function(e){function t(e){var n;return Object(s.a)(this,t),(n=Object(c.a)(this,Object(u.a)(t).call(this,e))).load_and_concat_reviews=function(e){return e.map(function(e){return a(119)("./".concat(e))})},n.load_and_concat_reviews=n.load_and_concat_reviews.bind(Object(O.a)(n)),n.papers=n.load_and_concat_reviews(ie.default),n.review_dates=n.papers.map(function(e){return J()(e.metadata.review_date,"YYYY-MM-DD")}),n}return Object(h.a)(t,e),Object(l.a)(t,[{key:"compute_ppw",value:function(){var e=this.review_dates.sort(function(e,t){return e.diff(t)}),t=e[e.length-1].diff(e[0],"days")/7;return Number.parseFloat(e.length/t).toFixed(3)}},{key:"compute_cumulative_number",value:function(){for(var e=this.review_dates.sort(function(e,t){return e.diff(t)}),t=[],a=0,n=0;n<e.length-1;n++){a+=e[n+1].diff(e[n],"days"),t.push(a)}return t}},{key:"render",value:function(){return i.a.createElement(p.g,null,i.a.createElement(p.u,null,i.a.createElement(p.e,{xs:"8",lg:"8"},i.a.createElement("br",null),i.a.createElement("h1",null,"Paper-a-Week"),"In recent years, I've been trying to develop a literature-reading habit that:",i.a.createElement("ul",null,i.a.createElement("li",null,"allows me to jot down notes in a consistent format"),i.a.createElement("li",null,"provides a way to search through notes and paper metadata"),i.a.createElement("li",null,"keeps me accountable to reading on a schedule")),"Enter ",i.a.createElement("strong",null,"Paper-a-Week"),", my attempt to meet those objectives! I've seen"," ",i.a.createElement("a",{href:"https://github.com/shagunsodhani/papers-I-read",target:"_blank",rel:"noopener noreferrer"},"things like this")," ","elsewhere, but the format and content of these entries are meant to serve my own interests."),i.a.createElement(p.e,null,i.a.createElement("br",null),i.a.createElement("h5",null," How am I doing? "),i.a.createElement("p",null," ","Papers / week: ",i.a.createElement("strong",null,this.compute_ppw())," "),i.a.createElement(te.Sparklines,{data:this.compute_cumulative_number(),height:75},i.a.createElement(te.SparklinesCurve,null),i.a.createElement(te.SparklinesSpots,null)),i.a.createElement("p",{className:"float-right"},i.a.createElement("em",null,"Cumulative review count: ",i.a.createElement("strong",null,this.papers.length))))),i.a.createElement("hr",null),i.a.createElement(p.u,null,i.a.createElement(p.e,null,i.a.createElement("h6",null," How this works "),i.a.createElement("ul",null,i.a.createElement("li",null," ","Click through each paper to see my notes about background, methods, results, conclusions, and more"," "),i.a.createElement("li",null," ","The selected paper stays open in a new 'tab', so you can browse the rest of the list while leaving the current review open"," "),i.a.createElement("li",null," ","Each paper has a number of tags, each of which has a unique background color. Click tags to search by that keyword!"),i.a.createElement("li",null," ","The search bar allows you to search by title, author, publication year, or keyword"," ")))),i.a.createElement("br",null),i.a.createElement(p.u,null,i.a.createElement(p.e,null,i.a.createElement(re,null))))}}]),t}(n.Component),se=a(200),le=function(e){function t(){return Object(s.a)(this,t),Object(c.a)(this,Object(u.a)(t).apply(this,arguments))}return Object(h.a)(t,e),Object(l.a)(t,[{key:"componentDidUpdate",value:function(e){this.props.location.pathname!==e.location.pathname&&window.scrollTo(0,0)}},{key:"render",value:function(){return this.props.children}}]),t}(n.Component),ce=Object(se.a)(le),ue=a(201),he=(a(277),a(67)),me=a.n(he),de=a(203),pe=a.n(de),ge=a(15),fe=function(e){function t(){return Object(s.a)(this,t),Object(c.a)(this,Object(u.a)(t).apply(this,arguments))}return Object(h.a)(t,e),Object(l.a)(t,[{key:"getData",value:function(){for(var e=[],t=0;t<30;t++){var a=this.signal_gaussian(t),n={x:t,noise:this.noise_gaussian(t),signal:a};e.push(n)}return e}},{key:"signal_gaussian",value:function(e){var t=1/Math.sqrt(2*Math.PI);return e=(e-this.props.signal_mean)/this.props.signal_sigma,t*Math.exp(-.5*e*e)/this.props.signal_sigma}},{key:"noise_gaussian",value:function(e){var t=1/Math.sqrt(2*Math.PI);return e=(e-this.props.noise_mean)/this.props.noise_sigma,t*Math.exp(-.5*e*e)/this.props.noise_sigma}},{key:"render",value:function(){return i.a.createElement(ge.i,{width:"100%",height:this.props.height},i.a.createElement(ge.b,{data:this.getData()},i.a.createElement(ge.j,{dataKey:"x",ticks:[0,5,10,15,20,25,30]},i.a.createElement(ge.d,{value:"x",offset:0,position:"insideBottom"})),i.a.createElement(ge.k,{domain:[0,.2],label:{value:"P(x)",angle:-90,position:"insideLeft"}}),i.a.createElement(ge.a,{name:"Signal Absent",type:"monotone",dataKey:"noise",stroke:this.props.noise_color,strokeWidth:2,fill:this.props.noise_color}),i.a.createElement(ge.h,{x:this.props.noise_mean,stroke:this.props.noise_color,strokeOpacity:.2}),i.a.createElement(ge.a,{name:"Signal Present",type:"monotone",dataKey:"signal",stroke:this.props.signal_color,strokeWidth:2,fill:this.props.signal_color}),i.a.createElement(ge.h,{x:this.props.signal_mean,stroke:this.props.signal_color,strokeOpacity:.2}),i.a.createElement(ge.h,{x:this.props.criterion,stroke:"black",strokeWidth:2,strokeOpacity:.5,label:{value:"Criterion",angle:-90,position:"insideTop",offset:30}}),i.a.createElement(ge.e,{verticalAlign:"top",height:36})))}}]),t}(n.Component),ye=function(e){function t(){return Object(s.a)(this,t),Object(c.a)(this,Object(u.a)(t).apply(this,arguments))}return Object(h.a)(t,e),Object(l.a)(t,[{key:"render",value:function(){return i.a.createElement(ge.g,{width:this.props.size,height:this.props.size,data:this.props.roc_data},i.a.createElement(ge.c,{strokeDasharray:"3 3"}),i.a.createElement(ge.j,{domain:[0,1],type:"number",dataKey:"false_positives"},i.a.createElement(ge.d,{value:"False Positives",offset:-5,position:"insideBottom"})),i.a.createElement(ge.k,{label:{value:"Hits",angle:-90,position:"insideLeft"}}),i.a.createElement(ge.h,{y:this.props.hit_rate,stroke:this.props.signal_color,strokeWidth:2,strokeOpacity:.4}),i.a.createElement(ge.h,{x:this.props.fp_rate,stroke:this.props.noise_color,strokeWidth:2,strokeOpacity:.4}),i.a.createElement(ge.f,{type:"monotone",dataKey:"hits",dot:!1,stroke:"#d47006",strokeWidth:4}),i.a.createElement(ge.f,{type:"monotone",dataKey:"false_positives",dot:!1,stroke:"#bbb",strokeWidth:3}))}}]),t}(n.Component);function ve(e){var t=Math.round(100*e)/100;return t>1?t=1:t<0&&(t=0),t}function be(e,t,n){return a(199)(t,Math.pow(n,2)).cdf(e)}function we(e,t){return a(199)(0,Math.pow(t,2)).ppf(e)}function Ee(e,t,a,n){for(var i=[],r=0;r<30;r+=.1){var o=1-be(r,e,t),s={false_positives:1-be(r,a,n),hits:o};i.push(s)}return i.push({false_positives:0,hits:0}),i.push({false_positives:1,hits:1}),i.sort(function(e,t){return e.false_positives-t.false_positives}),i}a(458),a(460);var ke=function(e){function t(e){var a;return Object(s.a)(this,t),(a=Object(c.a)(this,Object(u.a)(t).call(this,e))).handleOnChangeSignalMu=function(e){a.setState({signal_mean:e}),a.updateMetrics()},a.handleOnChangeSignalSigma=function(e){a.state.sigma_lock?a.setState({signal_sigma:e,noise_sigma:e}):a.setState({signal_sigma:e}),a.updateMetrics()},a.handleOnChangeNoiseMu=function(e){a.setState({noise_mean:e}),a.updateMetrics()},a.handleOnChangeNoiseSigma=function(e){a.state.sigma_lock?a.setState({noise_sigma:e,signal_sigma:e}):a.setState({noise_sigma:e}),a.updateMetrics()},a.handleOnChangeCriterion=function(e){a.setState({criterion:e}),a.updateMetrics()},a.toggleCheckbox=function(e){a.setState({sigma_lock:e.target.checked}),e.target.checked&&a.setState({signal_sigma:a.state.noise_sigma}),a.updateMetrics()},a.updateHitsInput=function(e){var t=e.target,n=t.value,i=t.min,r=t.max;n=Math.max(Number(i),Math.min(Number(r),Number(n)));var o=a.state.criterion+we(n,a.state.signal_sigma);o=Math.round(100*o)/100,a.setState({hits:n,signal_mean:o,dprime:(o-a.state.noise_mean)/Math.sqrt(.5*(Math.pow(a.state.signal_sigma,2)+Math.pow(a.state.noise_sigma,2))),misses:1-n})},a.updateMissesInput=function(e){var t=e.target,n=t.value,i=t.min,r=t.max,o=1-(n=Math.max(Number(i),Math.min(Number(r),Number(n)))),s=a.state.criterion+we(o,a.state.signal_sigma);s=Math.round(100*s)/100,a.setState({misses:n,signal_mean:s,dprime:(s-a.state.noise_mean)/Math.sqrt(.5*(Math.pow(a.state.signal_sigma,2)+Math.pow(a.state.noise_sigma,2))),hits:o})},a.updateFPsInput=function(e){var t=e.target,n=t.value,i=t.min,r=t.max;n=Math.max(Number(i),Math.min(Number(r),Number(n)));var o=a.state.criterion+we(n,a.state.noise_sigma);o=Math.round(100*o)/100,a.setState({fp:n,noise_mean:o,dprime:(a.state.signal_mean-o)/Math.sqrt(.5*(Math.pow(a.state.signal_sigma,2)+Math.pow(a.state.noise_sigma,2))),cr:1-n})},a.updateCRsInput=function(e){var t=e.target,n=t.value,i=t.min,r=t.max,o=1-(n=Math.max(Number(i),Math.min(Number(r),Number(n)))),s=a.state.criterion+we(o,a.state.noise_sigma);s=Math.round(100*s)/100,a.setState({fp:o,noise_mean:s,dprime:(a.state.signal_mean-s)/Math.sqrt(.5*(Math.pow(a.state.signal_sigma,2)+Math.pow(a.state.noise_sigma,2))),cr:n})},a.state={signal_mean:15,signal_sigma:4,noise_mean:10,noise_sigma:4,criterion:12,sigma_lock:!0,isEditingHits:!1,isEditingFP:!1,isEditingMisses:!1,isEditingCR:!1},a.signal_color="#0d5e08",a.noise_color="#960f0f",a.updateMetrics=a.updateMetrics.bind(Object(O.a)(a)),a}return Object(h.a)(t,e),Object(l.a)(t,[{key:"componentDidMount",value:function(){this.updateMetrics()}},{key:"updateMetrics",value:function(){this.setState(function(e,t){return{dprime:(e.signal_mean-e.noise_mean)/Math.sqrt(.5*(Math.pow(e.signal_sigma,2)+Math.pow(e.noise_sigma,2))),hits:ve(1-be(e.criterion,e.signal_mean,e.signal_sigma)),misses:ve(be(e.criterion,e.signal_mean,e.signal_sigma)),fp:ve(1-be(e.criterion,e.noise_mean,e.noise_sigma)),cr:ve(be(e.criterion,e.noise_mean,e.noise_sigma))}})}},{key:"renderSliders",value:function(){return i.a.createElement(p.g,null,i.a.createElement(p.u,null,i.a.createElement(p.e,{lg:"4",xs:"4"},i.a.createElement("div",null,"Signal Present Mean:",i.a.createElement("br",null)," ",i.a.createElement("strong",null,this.state.signal_mean)),i.a.createElement(pe.a,{min:1,max:25,value:this.state.signal_mean,orientation:"horizontal",onChange:this.handleOnChangeSignalMu})),i.a.createElement(p.e,{lg:"4",xs:"4"},i.a.createElement("div",null,"Signal Absent Mean:",i.a.createElement("br",null)," ",i.a.createElement("strong",null,this.state.noise_mean)),i.a.createElement(pe.a,{min:1,max:25,value:this.state.noise_mean,orientation:"horizontal",onChange:this.handleOnChangeNoiseMu})),i.a.createElement(p.e,{lg:"4",xs:"4"},i.a.createElement("div",null,"Criterion:",i.a.createElement("br",null)," ",i.a.createElement("strong",null,this.state.criterion)),i.a.createElement(pe.a,{min:0,max:30,value:this.state.criterion,orientation:"horizontal",onChange:this.handleOnChangeCriterion}))),i.a.createElement(p.u,null,i.a.createElement(p.e,{lg:"4",xs:"4"},i.a.createElement("div",null,"Signal Present Std:",i.a.createElement("br",null)," ",i.a.createElement("strong",null,this.state.signal_sigma)),i.a.createElement(pe.a,{min:1,max:15,value:this.state.signal_sigma,orientation:"horizontal",onChange:this.handleOnChangeSignalSigma})),i.a.createElement(p.e,{lg:"4",xs:"4"},i.a.createElement("div",null,"Signal Absent Std:",i.a.createElement("br",null)," ",i.a.createElement("strong",null,this.state.noise_sigma)),i.a.createElement(pe.a,{min:1,max:15,value:this.state.noise_sigma,orientation:"horizontal",onChange:this.handleOnChangeNoiseSigma})),i.a.createElement(p.e,{lg:"4",xs:"4"},i.a.createElement(p.h,null,i.a.createElement(p.i,{check:!0},i.a.createElement(p.l,{check:!0},i.a.createElement(p.j,{defaultChecked:this.state.sigma_lock,onClick:this.toggleCheckbox,type:"checkbox"})," ","Assume equal standard deviations?"))))))}},{key:"toggleEditingHits",value:function(){this.setState({isEditingHits:!this.state.isEditingHits})}},{key:"toggleEditingMisses",value:function(){this.setState({isEditingMisses:!this.state.isEditingMisses})}},{key:"toggleEditingFPs",value:function(){this.setState({isEditingFP:!this.state.isEditingFP})}},{key:"toggleEditingCRs",value:function(){this.setState({isEditingCR:!this.state.isEditingCR})}},{key:"renderFields",value:function(){var e,t,a,n,r=(Math.round(100*this.state.dprime)/100).toFixed(2),o=(Math.round(100*this.state.hits)/100).toFixed(2),s=(Math.round(100*this.state.misses)/100).toFixed(2),l=(Math.round(100*this.state.fp)/100).toFixed(2),c=(Math.round(100*this.state.cr)/100).toFixed(2);return e=this.state.isEditingHits?i.a.createElement(p.j,{type:"number",name:"hits",id:"hitsInput",value:this.state.hits,onChange:this.updateHitsInput,onBlur:this.toggleEditingHits.bind(this),min:0,max:1,step:.1}):i.a.createElement(p.j,{type:"text",name:"hits",id:"hitsInput",value:o,onFocus:this.toggleEditingHits.bind(this),readOnly:!0}),t=this.state.isEditingMisses?i.a.createElement(p.j,{type:"number",name:"misses",id:"missesInput",value:this.state.misses,onChange:this.updateMissesInput,onBlur:this.toggleEditingMisses.bind(this),min:0,max:1,step:.1}):i.a.createElement(p.j,{type:"text",name:"misses",id:"missesInput",value:s,onFocus:this.toggleEditingMisses.bind(this),readOnly:!0}),a=this.state.isEditingFP?i.a.createElement(p.j,{type:"number",name:"FPs",id:"FPsInput",value:this.state.fp,onChange:this.updateFPsInput,onBlur:this.toggleEditingFPs.bind(this),min:0,max:1,step:.1}):i.a.createElement(p.j,{type:"text",name:"FPs",id:"FPsInput",value:l,onFocus:this.toggleEditingFPs.bind(this),readOnly:!0}),n=this.state.isEditingCR?i.a.createElement(p.j,{type:"number",name:"CRs",id:"CRsInput",value:this.state.cr,onChange:this.updateCRsInput,onBlur:this.toggleEditingCRs.bind(this),min:0,max:1,step:.1}):i.a.createElement(p.j,{type:"text",name:"CRs",id:"CRsInput",value:c,onFocus:this.toggleEditingCRs.bind(this),readOnly:!0}),i.a.createElement(p.g,null,i.a.createElement(p.u,null,i.a.createElement(p.e,null,i.a.createElement("h5",null,"d':"," ",i.a.createElement("strong",null,r)))),i.a.createElement("hr",null),i.a.createElement(p.u,null,i.a.createElement(p.e,null,i.a.createElement("h5",null,"Values at Given Criterion"))),i.a.createElement(p.u,null,i.a.createElement(p.e,{lg:"6",xs:"6"},i.a.createElement(p.i,null,i.a.createElement(p.l,{for:"hitsInput"},"Hits:"),e)),i.a.createElement(p.e,{lg:"6",xs:"6"},i.a.createElement(p.i,null,i.a.createElement(p.l,{for:"missesInput"},"Misses:"),t))),i.a.createElement(p.u,null,i.a.createElement(p.e,{lg:"6",xs:"6"},i.a.createElement(p.i,null,i.a.createElement(p.l,{for:"FPsInput"},"False Positives:"),a)),i.a.createElement(p.e,{lg:"6",xs:"6"},i.a.createElement(p.i,null,i.a.createElement(p.l,{for:"CRsInput"},"Correct Rejections:"),n))))}},{key:"render",value:function(){return i.a.createElement(p.g,null,i.a.createElement(p.u,null,i.a.createElement(p.e,{lg:"8",xs:"12"},i.a.createElement("h4",{className:"center"},"Normal Distributions"),i.a.createElement("br",null),i.a.createElement(fe,{height:250,signal_mean:this.state.signal_mean,signal_sigma:this.state.signal_sigma,signal_color:this.signal_color,noise_mean:this.state.noise_mean,noise_sigma:this.state.noise_sigma,noise_color:this.noise_color,criterion:this.state.criterion})),i.a.createElement(p.e,{lg:"4",xs:"12"},i.a.createElement("h4",{className:"center"},"ROC Curve"),i.a.createElement("br",null),i.a.createElement(ye,{size:250,roc_data:Ee(this.state.signal_mean,this.state.signal_sigma,this.state.noise_mean,this.state.noise_sigma),hit_rate:this.state.hits,fp_rate:this.state.fp,signal_color:this.state.signal_color,noise_color:this.state.noise_color}))),i.a.createElement("hr",null),i.a.createElement(p.u,null,i.a.createElement(p.e,{lg:"8",md:"12",xs:"12"},this.renderSliders()),i.a.createElement(p.e,{lg:"4",md:"12",xs:"12"},this.renderFields())))}}]),t}(n.Component),_e=(a(462),function(e){function t(){return Object(s.a)(this,t),Object(c.a)(this,Object(u.a)(t).apply(this,arguments))}return Object(h.a)(t,e),Object(l.a)(t,[{key:"renderDprimer",value:function(){var e=i.a.createElement(ue.InlineMath,{math:"\\mu_{SignalPresent}"}),t=i.a.createElement(ue.InlineMath,{math:"\\mu_{SignalAbsent}"}),a=i.a.createElement(ue.InlineMath,null,"\\sigma"),n=i.a.createElement(ue.InlineMath,null,"\\infty");return i.a.createElement(p.g,null,i.a.createElement(p.u,null,i.a.createElement(p.e,null,i.a.createElement("h3",null,"A d' Primer"),i.a.createElement("h5",null," The Basics "),"d', also called the sensitivity index, is the primary statistic used in Signal Detection Theory. At its core, d' is a measure of how far apart two distributions are. If two distributions are perfectly overlapping, d'=0 . If two distributions are 1 standard deviation apart, d'=1.",i.a.createElement("br",null),"The equation is:",i.a.createElement(ue.BlockMath,{math:"d' = \\frac{\\mu_{SignalPresent} - \\mu_{SignalAbsent}}{\\sigma}"}),"where ",e," is the mean of the distribution when there is a true signal present, ",t," is the mean of the distribution when there is no signal present, and ",a," is the standard deviation of the distributions. Note that this formula assumes that the standard deviation (",i.a.createElement(ue.InlineMath,null,"\\sigma"),") is equal for both distributions. See below for discussion of the case where this assumption is not met.",i.a.createElement("br",null),i.a.createElement("br",null),i.a.createElement("h5",null,"Hits, Misses, False Positives, and Correct Rejections"),'Given the SignalPresent and SignalAbsent distributions, setting a "criterion" tells you the hit rate, miss rate, false positive rate, and correct rejection rate. In plain English, the hit rate is the proportion of the time that a signal is reported when there actually is a signal, and the false positive rate is the proportion of the time that a signal is reported when there is no signal present.',i.a.createElement("br",null),"Remember, the miss rate is just one minus the hit rate, and the correct rejection rate is just one minus the false positive rate, so knowing the hit rate gives you the miss rate, and knowing the false positive rate gives you the correct rejection rate.",i.a.createElement("br",null),i.a.createElement(p.u,null,i.a.createElement(p.e,{lg:{size:3,offset:4},xs:{size:6,offset:3}},i.a.createElement("img",{src:me.a,alt:"outcome table",className:"full-width"}))),i.a.createElement("br",null),i.a.createElement("br",null),i.a.createElement("h5",null," What's the Criterion? "),'The criterion indicates the amount of evidence above which a signal will be reported. Thus, the hit rate is the proportion of responses from the SignalPresent distribution above the criterion, and the miss rate is the propoertion of resopnses from the SignalPresent distribution below the criterion. The false positive rate is the proportion of responses from the SignalAbsent distribution above the criterion, and the correct rejection rate is the propoertion of responses from the SignalAbsent distribution below the criterion. A "conservative" criterion is relatively high, such that both hits and false positives will be low. A "liberal" criterion is relatively low, such that both hits and false positives will be high.',i.a.createElement("br",null),i.a.createElement("br",null),i.a.createElement("h5",null," ROC Curves "),"Receiver Operating Characteristic (ROC) Curves provide a visual representation of discriminability for a sensor. The x-axis is the false positive rate and the y-axis is the hit rate. An ROC curve is composed of all of the false positive rates and hit rates corresponding to all possible criteria one could choose from 0 to"," ",n,". Note that d' is constant along this curve, because different points along the curve only differ in criterion, not in sensor sensitivity. Increasing d' moves the curve up and to the left, whereas decreasing d' brings the curve toward the",i.a.createElement(ue.InlineMath,null,"y=x")," unity line. The"," ",i.a.createElement(ue.InlineMath,null,"y=x")," unity line represents a d' of 0, since hits and false positives are exactly equal along that line, and thus, the Signal Present and Signal Absent distributions must be perfectly overlapping.",i.a.createElement("br",null),i.a.createElement("br",null),"Often, the area under the ROC curve (AUC) is used to quantify how sensitive a sensor is. AUC ranges from 0 to 1, with 1 being a perfect sensor corresponding to a d' of ",n,". How does the AUC numerically relate to the d' value?",i.a.createElement(ue.BlockMath,{math:"d' = \\sqrt{2}Z(AUC)"})," where Z() is the inverse CDF of the Gaussian distribution.",i.a.createElement("br",null),i.a.createElement("br",null),i.a.createElement("h5",null," Advanced Topics "),i.a.createElement("h6",null," Unequal Standard Deviations "),"What if the standard deviations of the two distributions are not equal? No problem, just replace the denominator with the square root of the average variance!",i.a.createElement(ue.BlockMath,{math:"d' = \\frac{\\mu_{SignalPresent} - \\mu_{SignalAbsent}}{\\sqrt{\\frac{1}{2}(\\sigma_{SignaPresent}^2 + \\sigma_{SignalAbsent}^2)}}"}),i.a.createElement("br",null),i.a.createElement("h6",null," Computing d' from the hit rate and false alarm rate "),"If you have the hit rate and false alarm rate and can assume the two distributions are approximately normal, you can also compute d' as:",i.a.createElement(ue.BlockMath,{math:"d' = Z(HitRate) - Z(FalsePositiveRate)"}),"where Z() is the inverse CDF of the Gaussian distribution.",i.a.createElement("br",null),i.a.createElement("hr",null),i.a.createElement("h5",null," References and Future Reading "),i.a.createElement("a",{href:"http://www.cns.nyu.edu/~david/handouts/sdt/sdt.html",target:"_blank",rel:"noopener noreferrer"},i.a.createElement(p.b,{color:"link"},"David Heeger's Signal Detection Theory Webpage ",i.a.createElement(g.q,null))),i.a.createElement("a",{href:"https://en.wikipedia.org/wiki/Sensitivity_index",target:"_blank",rel:"noopener noreferrer"},i.a.createElement(p.b,{color:"link"},"Wikipedia Page for Sensitivity ",i.a.createElement(g.q,null))))))}},{key:"render",value:function(){return i.a.createElement(p.g,null,i.a.createElement(p.u,null,i.a.createElement(p.e,{xs:"12",lg:"12"},i.a.createElement("br",null),i.a.createElement("h1",null,"d' Calculator"),i.a.createElement("h5",null,"An interactive tool for learning about signal detection theory"),i.a.createElement("p",null,"This webpage was designed to help students in PSYCH 30, a large undergradate class on Perception at Stanford University, understand the basics of Signal Detection Theory and the sensitivity index, d'. Play with the calculator below and scroll down to read more about signal detection and how to compute d'. If you have any issues or find bugs, please let me know via email!"),i.a.createElement(ke,null),i.a.createElement("hr",null),this.renderDprimer())))}}]),t}(n.Component)),je=a(229),xe=a.n(je),Se=(a(464),function(e){function t(e){var a;return Object(s.a)(this,t),a=Object(c.a)(this,Object(u.a)(t).call(this,e)),new xe.a(".copy-src"),a}return Object(h.a)(t,e),Object(l.a)(t,[{key:"render",value:function(){return i.a.createElement(m.a,null,i.a.createElement(ce,null,i.a.createElement("div",{className:"App"},i.a.createElement(d.a,{path:"/",component:F}),i.a.createElement(d.a,{exact:!0,path:"/",component:M}),i.a.createElement(d.a,{exact:!0,path:"/CV",component:Z}),i.a.createElement(d.a,{exact:!0,path:"/Research",component:ee}),i.a.createElement(d.a,{exact:!0,path:"/PaperReviews",component:oe}),i.a.createElement(d.a,{exact:!0,path:"/DPCalc",component:_e}),i.a.createElement(d.a,{path:"/",component:N}))))}}]),t}(n.Component));Boolean("localhost"===window.location.hostname||"[::1]"===window.location.hostname||window.location.hostname.match(/^127(?:\.(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)){3}$/));a(466);o.a.render(i.a.createElement(Se,null),document.getElementById("root")),"serviceWorker"in navigator&&navigator.serviceWorker.ready.then(function(e){e.unregister()})},61:function(e,t,a){e.exports=a.p+"static/media/clouds_rest.837e031f.png"},64:function(e,t,a){e.exports=a.p+"static/media/vtc_fade.99b1e008.gif"},65:function(e,t,a){e.exports=a.p+"static/media/SU_Seal_Red.70ef3ddf.png"},66:function(e,t,a){e.exports=a.p+"static/media/USC_Seal.f3cde9dd.png"},67:function(e,t,a){e.exports=a.p+"static/media/dp_table.e20c8eec.png"},68:function(e,t,a){e.exports=a.p+"static/media/emcog.421510d6.jpg"},69:function(e,t){e.exports="data:image/png;base64,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"},70:function(e,t,a){e.exports=a.p+"static/media/headshot-sq.479e4fef.png"}},[[253,2,1]]]);
//# sourceMappingURL=main.8cda2a97.chunk.js.map